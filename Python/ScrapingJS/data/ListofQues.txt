Question: 1
client wants to read a file from HDFS. How does the data get from the DataNodes to the client?

A.The NameNode reads the blocks from the DataNodes, and caches them. Then, the application reads the blocks from the NameNode.

B.The application reads the blocks directly from the DataNodes.

C.The blocks are sent to a single DataNode, then the application reads the blocks from that Data Node.

--------------------------------------------------------------------------------------------------
Question: 2
Which of the following statements arc accurate in describing features of Hadoop rack awareness?(Choose 2)

A.HDFS is rack aware but MapReduce daemons are not.

B.Rack location is considered in the HDFS block placement policy.

C.Hadoop gives preference to intra-rack data transfer In order to conserve bandwidth.

D.Even for small clusters on a single rack, configuring rack awareness will improve performance.

E.Configuration of rack awareness is accomplished using a configuration file. You cannot use a rack topology script.

--------------------------------------------------------------------------------------------------
Question: 3
Which of the following statements is the most accurate about the choice of operating systems torun on a Hadoop cluster?

A.Linux and Solaris/OpenSolaris are preferable to Windows. Solaris running on SPARC hardware is the preferred Hadoop slave node configuration.

B.Linux is preferable to Windows and Solaris/OpenSolaris. Some Linux distributions are intended for cluster environments more than others.

C.The choice of operating system isn’t very important for a Hadoop cluster.

D.Linux is preferable to Windows and Solaris/OpenSolaris, but the choice of Linux distribution is unimportant when planning a Hadoop cluster.

--------------------------------------------------------------------------------------------------
Question: 4
For a MapReduce job, what’s the relationship between tasks and task attempts?

A.There are always exactly as many task attempts as there are tasks.

B.There are always at least as many task attempts as there are tasks.

C.There are always at most as many task attempts as there are tasks.

--------------------------------------------------------------------------------------------------
Question: 5
You have a cluster running with the Fair in Scheduler enabled. There are currently no jobs runningon the cluster, and you submit a job A, so that only job A is running on the cluster. A while later,

you submit job B, Now job A and job B are running on the cluster at the same time.Which of the following describes how the Fair Scheduler operates? (Choose 2)

A.When job B gets submitted, it will get assigned tasks, while job A continues to run with fewer tasks.

B.When job A gets submitted, it doesn’t consume all the task slots.

C.When job A gets submitted, it consumes all the task slots.

D.When job B gets submitted, job A has to finish first, before job B can get scheduled.

--------------------------------------------------------------------------------------------------
Question: 6
Hadoop provider web interface can be used for all of the following EXCEPT: (choose 1)

A.Keeping track of the number of files and directories stored in HDFS.

B.Keeping track of jobs running on the cluster.

C.Browsing files in HDFS.

D.Keeping track of tasks running on each individual slave node.

E.Keeping track of processor and memory utilization on each individual slave node.

--------------------------------------------------------------------------------------------------
Question: 7
Compare the hardware requirements of the NameNode with that of the DataNodes in a Hadoopcluster:

A.The NameNode requires more memory and no disk drives.

B.The NameNode requires more memory but less disk capacity.

C.The NameNode and DataNodes should have the same hardware configuration.

D.The NameNode requires less memory and fewer number of disk drives than the DataNodes.

E.The NameNode requires more memory and requires greater disk capacity than the DataNodes.

--------------------------------------------------------------------------------------------------
Question: 8
What information is stored on disk on the NameNode? (Choose 4)

A.File permissions of the files in HDFS.

B.An edit log of changes that have been made since the last backup of the NameNode.

C.A catalog of DataNodes and the blocks that are stored on them.

D.Names of the files in HDFS.

E.The directory structure of the files in HDFS.

F.An edit log of changes that have been made since the last snapshot compaction by the Secondary NameNode.

G.The status of the heartbeats of each DataNode.

--------------------------------------------------------------------------------------------------
Question: 9
What do you need to do when adding new slave nodes to a cluster?

A.Halt and resubmit any running MapReduce jobs to guarantee correctness.

B.Restart the NameNode daemon.

C.Add a new entry to /etc/slavenodes on the NameNode host.

D.Increase the value of dfs.number.of.nodes in hdfs-site.xml.

E.add the new node’s DNS name to the conf/slaves file on the master node.

--------------------------------------------------------------------------------------------------
Question: 10
You have installed a cluster running HDFS and MapReduce version 2 (MRv2) on YARN.You have no afs.hosts entry()ies in your hdfs-alte.xml configuration file. You configure anew worker node by setting fs.default.name in its configuration files to point to theNameNode on your cluster, and you start the DataNode daemon on that worker node. Whatdo you have to do on the cluster to allow the worker node to join, and start storing HDFSblocks?

A. Without creating a dfs.hosts file or making any entries, run the command hadoop dfsadmin –refreshHadoop on the NameNode

B. Create a dfs.hosts file on the NameNode, add the worker node’s name to it, then issue the command hadoop dfsadmin –refreshNodes on the NameNode

C. Restart the NameNode

D. Nothing; the worker node will automatically join the cluster when the DataNode daemon is started.

--------------------------------------------------------------------------------------------------
Question: 11
Given: You want to clean up this list by removing jobs where the state is KILLED. Whatcommand you enter?

<a href="http://cdn.aiotestking.com/wp-content/uploads/cca-505-v1/1.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/cca-505-v1/1.jpg"/></a> 

A. Yarn application –kill application_1374638600275_0109

B. Yarn rmadmin –refreshQueue

C. Yarn application –refreshJobHistory

D. Yarn rmadmin –kill application_1374638600275_0109

--------------------------------------------------------------------------------------------------
Question: 12
Assuming a cluster running HDFS, MapReduce version 2 (MRv2) on YARN with all settingsat their default, what do you need to do when adding a new slave node to a cluster?

A. Add a new entry to /etc/nodes on the NameNode host.

B. Restart the NameNode daemon.

C. Increase the value of dfs.number.of.needs in hdfs-site.xml

D. Restart the NameNode and ResourceManager deamons and resubmit any running jobs

E. Nothing, other than ensuring that DNS (or /etc/hosts files on all machines) contains am entry for the new node.

Explanation:



--------------------------------------------------------------------------------------------------
Question: 13
You have a 20 node Hadoop cluster, with 18 slave nodes and 2 master nodes runningHDFS High Availability (HA). You want to minimize the chance of data loss in you cluster.What should you do?

A. Configure the cluster’s disk drives with an appropriate fault tolerant RAID level

B. Run the ResourceManager on a different master from the NameNode in the order to load share HDFS metadata processing

C. Run a Secondary NameNode on a different master from the NameNode in order to load provide automatic recovery from a NameNode failure

D. Set an HDFS replication factor that provides data redundancy, protecting against failure

E. Add another master node to increase the number of nodes running the JournalNode which increases the number of machines available to HA to create a quorum

--------------------------------------------------------------------------------------------------
Question: 14
You decide to create a cluster which runs HDFS in High Availability mode with automaticfailover, using Quorum-based Storage. What is the purpose of ZooKeeper in such aconfiguration?

A. It manages the Edits file, which is a log changes to the HDFS filesystem.

B. It monitors an NFS mount point and reports if the mount point disappears

C. It both keeps track of which NameNode is Active at any given time, and manages the Edits file, which is a log of changes to the HDFS filesystem 

D. It only keeps track of which NameNode is Active at any given time

E. Clients connect to ZoneKeeper to determine which NameNode is Active

--------------------------------------------------------------------------------------------------
Question: 15
During the execution of a MapReduce v2 (MRv2) job on YARN, where does the Mapperplace the intermediate data each Map task?

A. The Mapper stores the intermediate data on the underlying filesystem of the local disk in the directories yarn.nodemanager.local-dirs

B. The Mapper transfers the intermediate data immediately to the Reducers as it generated by the Map task

C. The Mapper stores the intermediate data on the mode running the job’s ApplicationMaster so that is available to YARN’s ShuffleService before the data is presented to the Reducer

D. The Mapper stores the intermediate data in HDFS on the node where the MAP tasks ran in the HDFS /usercache/&amp;[user]sppcache/application_&amp;(appid) directory for the user who ran the job

E. YARN holds the intermediate data in the NodeManager’s memory (a container) until it is transferred to the Reducers

Explanation:



--------------------------------------------------------------------------------------------------
Question: 16
Which Yarn daemon or service monitors a Container’s per-application resource usage (e.g,memory, CPU)?

A. NodeManager

B. ApplicationMaster

C. ApplicationManagerService

D. ResourceManager

--------------------------------------------------------------------------------------------------
Question: 17
You are planning a Hadoop cluster and considering implementing 10 Gigabit Ethernet asthe network fabric. Which workloads benefit the most from a faster network fabric?

A. When your workload generates a large amount of output data, significantly larger than amount of intermediate data

B. When your workload generates a large amount of intermediate data, on the order of the input data itself

C. When workload consumers a large amount of input data, relative to the entire capacity of HDFS

D. When your workload consists of processor-intensive tasks

Explanation:



--------------------------------------------------------------------------------------------------
Question: 18
For each YARN Job, the Hadoop framework generates task log files. Where are Hadoop’sfiles stored?

A. Cached In the YARN container running the task, then copied into HDFS on fob completion

B. Cached by the NodeManager managing the job containers, then written to a log directory on the NameNode

C. In HDFS, In the directory of the user who generates the job

D. On the local disk of the slave node running the task

--------------------------------------------------------------------------------------------------
Question: 19
You are the hadoop fs –put command to add a file “sales.txt” to HDFS. This file is smallenough that it fits into a single block, which is replicated to three nodes in your cluster (witha replication factor of 3). One of the nodes holding this file (a single block) fails. How will thecluster handle the replication of this file in this situation/

A. This file will be immediately re-replicated and all other HDFS operations on the cluster will halt until the cluster’s replication values are restored

B. The file will remain under-replicated until the administrator brings that nodes back online 

C. The file will be re-replicated automatically after the NameNode determines it is under replicated based on the block reports it receives from the DataNodes

D. The cluster will re-replicate the file the next time the system administrator reboots the NameNode daemon (as long as the file’s replication doesn’t fall two)

--------------------------------------------------------------------------------------------------
Question: 20
You are configuring your cluster to run HDFS and MapReduce v2 (MRv2) on YARN. Whichdaemons need to be installed on your clusters master nodes?

A. DataNode

B. HMaster

C. TaskTracker

D. NameNode

E. ResourceManager

F. JobTracker

--------------------------------------------------------------------------------------------------
Question: 21
Assume you have a file named foo.txt in your local directory. You issue the following threecommands: Hadoop fs –mkdir input Hadoop fs –put foo.txt input/foo.txt Hadoop fs –putfoo.txt input What happens when you issue that third command?

A. You get a error message telling you that foo.txt already exists. The file is not written to HDFS

B. The write silently fails

C. You get a warning that foo.txt is being overwritten

D. You get an error message telling you that foo.txt already exists, and asking you if you would like to overwrite

E. The file is uploaded and stored as a plain named input

F. The write succeeds, overwriting foo.txt in HDFS with no warning

G. You get an error message telling you that input is not a directory

--------------------------------------------------------------------------------------------------
Question: 22
You have a Hadoop cluster running HDFS, and a gateway machine external to the clusterfrom which clients submit jobs. What do you need to do in order to run on the cluster andsubmit jobs from the command line of the gateway machine?

A. Install the impslad daemon, statestored daemon, and catalogd daemon on each machine in the cluster and on the gateway node

B. Install the impalad daemon on each machine in the cluster, the statestored daemon and catalogd daemon on one machine in the cluster, and the impala shell on your gateway machine

C. Install the impalad daemon and the impala shell on your gateway machine, and the statestored daemon and catalog daemon on one of the nodes in the cluster 

 </div>]
--------------------------------------------------------------------------------------------------
Question: 23
You have converted your Hadoop cluster from a MapReduce 1 (MRv1) architecture toaMapReduce 2 (MRv2) on YARN architecture. Your developers are accustomed tospecifying map and reduce tasks (resource allocation) tasks when they run jobs. Adeveloper wants to know how specify to reduce tasks when a specific job runs. Whichmethod should you tell that developer to implement?

A. In YARN, resource allocation is a function of virtual cores specified by the ApplicationMaster making requests to the NodeManager where a reduce task is handled by a single container (and this a single virtual core). Thus, the developer needs to specify the number of virtual cores to the NodeManager by executing –p yarn.nodemanager.cpuvcores= 2

B. MapReduce version 2 (MRv2) on YARN abstracts resource allocation away from the idea of “tasks” into memory and virtual cores, thus eliminating the need for a developer to specify the number of reduce tasks, and indeed preventing the developer from specifying the number of reduce tasks.

C. In YARN, resource allocation is a function of megabytes of memory in multiple of 1024mb. Thus, they should specify the amount of memory resource they need by executing –D mapreduce.reduce.memory-mp-2040

D. In YARN, the ApplicationMaster is responsible for requesting the resources required for a 

specific job. Thus, executing –p yarn.applicationmaster.reduce.tasks-2 will specify that the ApplicationMaster launch two task containers on the worker nodes.

E. Developers specify reduce tasks in the exact same way for both MapReduce version 1 (MRv1) and MapReduce version 2 (MRv2) on YARN. Thus, executing –p mapreduce.job.reduce-2 will specify 2 reduce tasks.

--------------------------------------------------------------------------------------------------
Question: 24
You are upgrading a Hadoop cluster from HDFS and MapReduce version 1 (MRv1) to onerunning HDFS and MapReduce version 2 (MRv2) on YARN. You want to set and enforce ablock of 128MB for all new files written to the cluster after the upgrade. What should youdo?

A. Set dfs.block.size to 134217728 on all the worker nodes, on all client machines, and on the NameNode, and set the parameter to final.

B. Set dfs.block.size to 134217728 on all the worker nodes and client machines, and set the parameter to final. You do need to set this value on the NameNode.

C. Set dfs.block.size to 128M on all the worker nodes and client machines, and set the parameter to final. You do need to set this value on the NameNode.

D. You cannot enforce this, since client code can always override this value.

E. Set dfs.block.size to 128M on all the worker nodes, on all client machines, and on the NameNode, and set the parameter to final.

Explanation:



--------------------------------------------------------------------------------------------------
Question: 25
Which two are Features of Hadoop’s rack topology?

A. Configuration of rack awareness is accomplished using a configuration file. You cannot use a rack topology script.

B. Even for small clusters on a single rack, configuring rack awareness will improve performance.

C. Rack location is considered in the HDFS block placement policy

D. HDFS is rack aware but MapReduce daemons are not

E. Hadoop gives preference to Intra rack data transfer in order to conserve bandwidth

--------------------------------------------------------------------------------------------------
Question: 26
Which YARN daemon or service negotiates map and reduce Containers from theScheduler, tracking their status and monitoring for progress?

A. ApplicationMaster

B. NodeManager

C. ResourceManager

D. ApplicationManager

Explanation:



--------------------------------------------------------------------------------------------------
Question: 27
You are configuring a cluster running HDFS, MapReduce version 2 (MRv2) on YARNrunning Linux. How must you format the underlying filesystem of each DataNode?

A. They must not formatted – – HDFS will format the filesystem automatically

B. They may be formatted in any Linux filesystem

C. They must be formatted as HDFS

D. They must be formatted as either ext3 or ext4

--------------------------------------------------------------------------------------------------
Question: 28
Your cluster has the following characteristics: • A rack aware topology is configured and on •Replication is not set to 3 • Cluster block size is set to 64 MB Which describes the file readprocess when a client application connects into the cluster and requests a 50MB file?

A. The client queries the NameNode which retrieves the block from the nearest DataNode to the client and then passes that block back to the client.

B. The client queries the NameNode for the locations of the block, and reads from a random location in the list it retrieves to eliminate network I/O leads by balancing which nodes it retrieves data from at any given time. 

C. The client queries the NameNode for the locations of the block, and reads all three copies. The first copy to complete transfer to the client is the one the client reads as part of Hadoop’s speculative execution framework.

D. The client queries the NameNode for the locations of the block, and reads from the first location in the list it receives.

--------------------------------------------------------------------------------------------------
Question: 29
On a cluster running CDH 5.0 or above, you use the hadoop fs –put command to write a300MB file into a previously empty directory using an HDFS block of 64MB. Just after thiscommand has finished writing 200MB of this file, what would another use see when theylook in the directory?

A. They will see the file with a ._COPYING_ extension on its name. if they view the file, they will see contents of the file up to the last completed block (as each 64MB block is written, that block becomes available)

B. The directory will appear to be empty until the entire file write is completed on the cluster

C. They will see the file with a ._COPYING_extension on its name. If they attempt to view the file, they will get a ConcurrentFileAccessException until the entire file write is completed on the cluster.

D. They will see the file with its original name. if they attempt to view the file, they will get a ConcurrentFileAccessException until the entire file write is completed on the cluster

Explanation:



--------------------------------------------------------------------------------------------------
Question: 30
Your cluster’s mapped-site.xml includes the following parameters&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;4096&lt;value/&gt;&lt;name&gt;mapreduce.reduce.memory,mb&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; And your cluster’syarn-site.xml includes the following parameters&lt;name&gt;yarn.nodemanager/vmen-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; What is themaximum amount of virtual memory allocated for each map before YARN will kill itsContainer?

A. 17.2 GB

B. 24.6 GB

C. 8.2 GB

D. 4 GB

--------------------------------------------------------------------------------------------------
Question: 31
You suspect that your NameNode is incorrectly configured, and is swapping memory todisk. Which Linux commands help you to identify whether swapping is occurring?

A. free

B. df

C. memcat 

 </div>]
--------------------------------------------------------------------------------------------------
Question: 32
Your cluster is configured with HDFS and MapReduce version 2 (MRv2) on YARN. What isthe result when you execute: hadoop jar samplejar.jar MyClass on a client machine?

A. SampleJar.Jar is serialized into an XML file which is submitted to the ApplicationMaster

B. SampleJar.Jar is sent directly to the ResourceManager

C. SampleJar.Jar is placed in a temporary directly in HDFS

D. SampleJar.jar is sent to the ApplicationMaster which allocation a container for Sample.jar

--------------------------------------------------------------------------------------------------
Question: 33
Which three basic configuration parameters must you set to migrate your cluster fromMapReduce1 (MRv1) to MapReduce v2 (MRv2)?

A. Configure the NodeManager hostname and enable services on YARN by setting the 

following property in yarn-site.xml: &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt; &lt;value&gt;your_nodeManager_hostname&lt;/value&gt;</div>]
--------------------------------------------------------------------------------------------------
Question: 34
You are running a Hadoop cluster with MapReduce version 2 (MRv2) on YARN. Youconsistently see that MapReduce map tasks on your cluster are running slowly because ofexcessive garbage collection of JVM, how do you increase JVM heap property to 3GB tooptimize performance?

A. Yarn.application.child.java.opts-Xax3072m 

B. Yarn.application.child.java.opts=-3072m

C. Mapreduce.map.java.opts=-Xmx3072m

D. Mapreduce.map.java.opts=-Xms3072m

--------------------------------------------------------------------------------------------------
Question: 35
You want to understand more about how users browse you public website. For example,you want to know which pages they visit prior to placing an order. You have a server farm of200 web servers hosting your website. Which is the most efficient process to gather theseweb server logs into your Hadoop cluster for analysis?

A. Sample the web server logs web servers and copy them into HDFS using curl

B. Channel these clickstream into Hadoop using Hadoop Streaming

C. Write a MApReduce job with the web servers from mappers and the Hadoop cluster nodes reducers

D. Ingest the server web logs into HDFS using Flume

E. Import all users clicks from your OLTP databases into Hadoop using Sqoop

--------------------------------------------------------------------------------------------------
Question: 36
Your cluster implements HDFS High Availability (HA). Your two NameNodes are namednn01 and nn02. What occurs when you execute the command: hdfs haadmin –failover nn01nn02

A. nn02 becomes the standby NameNode and nn01 becomes the active NameNode

B. nn02 is fenced, and nn01 becomes the active NameNode

C. nn01 becomes the standby NamNode and nn02 becomes the active NAmeNode

D. nn01 is fenced, and nn02 becomes the active NameNode

--------------------------------------------------------------------------------------------------
Question: 37
Your Hadoop cluster is configured with HDFS and MapReduce version 2 (MRv2) on YARN.Can you configure a worker node to run a NodeManager daemon but not a DataNodedaemon and still have a function cluster?

A. Yes. The daemon will receive data from the NameNode to run Map tasks

B. Yes. The daemon will get data from another (non-local) DataNode to run Map tasks

C. Yes. The daemon will receive Reduce tasks only

Explanation:



--------------------------------------------------------------------------------------------------
Question: 38
Which YARN process runs as “controller O” of a submitted job and is responsible forresource requests?

A. NodeManager

B. ApplicationManager

C. JobTracker

D. ResourceManager

E. ApplicationMaster

F. JobHistoryServer

--------------------------------------------------------------------------------------------------
Question: 39
You have a cluster running with the Fair Scheduler enabled. There are currently no jobsrunning on the cluster, and you submit a job A, so that only job A is running on the cluster. Awhile later, you submit Job B. now job A and Job B are running on the cluster at the sametime. How will the Fair Scheduler handle these two jobs?

A. When job A gets submitted, it consumes all the tasks slots.

B. When job A gets submitted, it doesn’t consume all the task slots

C. When job B gets submitted, Job A has to finish first, before job B can scheduled

D. When job B gets submitted, it will get assigned tasks, while Job A continue to run with fewer tasks.

Explanation:



--------------------------------------------------------------------------------------------------
Question: 40
You observe that the number of spilled records from Map tasks far exceeds the number ofmap output records. Your child heap size is 1GB and your io.sort.mb value is set to 100 MB.How would you tune your io.sort.mb value to achieve maximum memory to disk I/O ratio?

A. Decrease the io.sort.mb value to 0

B. Increase the io.sort.mb to 1GB

C. For 1GB child heap size an io.sort.mb of 128 MB will always maximize memory to disk I/O

D. Tune the io.sort.mb value until you observe that the number of spilled records equals (or is as close to equals) the number of map output records

--------------------------------------------------------------------------------------------------
Question: 41
You want a node to only swap Hadoop daemon data from RAM to disk when absolutelynecessary. What should you do?

A. Set vm.swappiness to o in /etc/sysctl.conf

B. Delete the /dev/vmswap file on the node

C. Set the ram.swap parameter to o in core-site.xml 

D. Delete the /swapfile file on the node

E. Delete the /etc/swap file on the node

--------------------------------------------------------------------------------------------------
Question: 42
Which is the default scheduler in YARN?

A. Fair Scheduler

B. FIFO Scheduler

C. Capacity Scheduler

D. YARN doesn’t configure a default scheduler. You must first assign a appropriate scheduler class in yarn-site.xml

--------------------------------------------------------------------------------------------------
Question: 43
Each node in your Hadoop cluster, running YARN, has 64 GB memory and 24 cores. Youryarn-site.xml has the following configuration: &lt;property&gt;&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;32768&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;

&lt;value&gt;23&lt;/value&gt; &lt;/property&gt; You want YARN to launch no more than 16 containers pernode. What should you do?

A. No action is needed: YARN’s dynamic resource allocation automatically optimizes the node memory and cores

B. Modify yarn-site.xml with the following property: &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;16&lt;/value&gt;

C. Modify yarn-site.xml with the following property: &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt;

D. Modify yarn-site.xml with the following property: &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;

--------------------------------------------------------------------------------------------------
Question: 44
Your Hadoop cluster contains nodes in three racks. You have NOT configured the dfs.hostsproperty in the NameNode’s configuration file. What results?

A. No new nodes can be added to the cluster until you specify them in the dfs.hosts file

B. The NameNode will update the dfs.hosts property to include machine running DataNode daemon on the next NameNode reboot or with the command dfsadmin -refreshNodes

C. Any machine running the DataNode daemon can immediately join the cluster

D. Presented with a blank dfs.hosts property, the NameNode will permit DatNode specified in mapred.hosts to join the cluster

Explanation:



--------------------------------------------------------------------------------------------------
Question: 45
On a cluster running MapReduce v2 (MRv2) on YARN, a MapReduce job is given adirectory of 10 plain text as its input directory. Each file is made up of 3 HDFS blocks. Howmany Mappers will run?

A. We cannot say; the number of Mappers is determined by the RsourceManager

B. We cannot say; the number of Mappers is determined by the ApplicationManager

C. We cannot say; the number of Mappers is determined by the developer

D. 30

E. 3

F. 10

--------------------------------------------------------------------------------------------------
Question: 46
You are working on a project where you need to chain together MapReduce, Pig jobs. Youalso needs the ability to use forks, decision, and path joins. Which ecosystem project shouldyou use to perform these actions?

A. HUE

B. Oozie

C. Zookeeper 

D. HBase

E. Sqoop

--------------------------------------------------------------------------------------------------
Question: 47
What processes must you do if you are running a Hadoop cluster with a single NameNodeand six DataNodes, and you want to change a configuration parameter so that it affects allsix DataNodes.

A. You must modify the configuration files on the NameNode only. DataNodes read their configuration from the master nodes.

B. You don’t need to restart any daemon, as they will pick up changes automatically

C. You must restart all six DatNode daemon to apply the changes to the cluste

D. You must modify the configuration file on each of the six DataNode machines.

E. You must restart the NameNode daemon to apply the changes to the cluster

--------------------------------------------------------------------------------------------------
Question: 48
Identify two features/issues that YARN is designed to address:

A. Standardize on a single MapReduce API

B. Single point of failure in the NameNode

C. Reduce complexity of the MapReduce APIs

D. Resource pressures on the JobTracker

E. Ability to run frameworks other than MapReduce, such as MPI

F. HDFS latency

--------------------------------------------------------------------------------------------------
Question: 49
slave node in your cluster has four 2TB hard drives installed (4 x 2TB). The DataNode isconfigured to store HDFS blocks on the disks. You set the value of thedfs.datanode.du.reserved parameter to 100GB. How does this alter HDFS block storage?

A. All hard drives may be used to store HDFS blocks as long as atleast 100 GB in total is available on the node

B. 100 GB on each hard drive may not be used to store HDFS blocks

C. 25 GB on each hard drive may not be used to store HDFS blocks

D. A maximum of 100 GB on each hard drive may be used to store HDFS blocks

Explanation:



--------------------------------------------------------------------------------------------------
Question: 50
When is the earliest point at which the reduce method of a given Reducer can be called?

A.As soon as at least one mapper has finished processing its input split.

B.As soon as a mapper has emitted at least one record.

C.Not until all mappers have finished processing all records.

D.It depends on the InputFormat used for the job.

--------------------------------------------------------------------------------------------------
Question: 51
Which describes how a client reads a file from HDFS?

A.The client queries the NameNode for the block location(s). The NameNode returns the block location(s) to the client. The client reads the data directory off the DataNode(s).

B.The client queries all DataNodes in parallel. The DataNode that contains the requested data responds directly to the client. The client reads the data directly off the DataNode.

C.The client contacts the NameNode for the block location(s). The NameNode then queries the DataNodes for block locations. The DataNodes respond to the NameNode, and the NameNode redirects the client to the DataNode that holds the requested data block(s). The client then reads the data directly off the DataNode.

D.The client contacts the NameNode for the block location(s). The NameNode contacts the DataNode that holds the requested data block. Data is transferred from the DataNode to the NameNode, and then from the NameNode to the client.

--------------------------------------------------------------------------------------------------
Question: 52
You are developing a combiner that takes as input Text keys, IntWritable values, and emits Textkeys, IntWritable values. Which interface should your class implement?

A.Combiner &lt;Text, IntWritable, Text, IntWritable&gt; 

B.Mapper &lt;Text, IntWritable, Text, IntWritable&gt; 

C.Reducer &lt;Text, Text, IntWritable, IntWritable&gt; 

D.Reducer &lt;Text, IntWritable, Text, IntWritable&gt; 

E.Combiner &lt;Text, Text, IntWritable, IntWritable&gt;

--------------------------------------------------------------------------------------------------
Question: 53
Indentify the utility that allows you to create and run MapReduce jobs with any executable or scriptas the mapper and/or the reducer?

A.Oozie

B.Sqoop

C.Flume

D.Hadoop Streaming

E.mapred

--------------------------------------------------------------------------------------------------
Question: 54
How are keys and values presented and passed to the reducers during a standard sort and shufflephase of MapReduce?

A.Keys are presented to reducer in sorted order; values for a given key are not sorted.

B.Keys are presented to reducer in sorted order; values for a given key are sorted in ascending order.

C.Keys are presented to a reducer in random order; values for a given key are not sorted.

D.Keys are presented to a reducer in random order; values for a given key are sorted in ascending order.

Explanation:Reducer has 3 primary phases:1. ShuffleThe Reducer copies the sorted output from each Mapper using HTTP across the network.2. SortThe framework merge sorts Reducer inputs by keys (since different Mappers may have output thesame key).The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they aremerged.

SecondarySortTo achieve a secondary sort on the values returned by the value iterator, the application shouldextend the key with the secondary key and define a grouping comparator. The keys will be sortedusing the entire key, but will be grouped using the grouping comparator to decide which keys andvalues are sent in the same call to reduce.3. ReduceIn this phase the reduce(Object, Iterable, Context) method is called for each &lt;key, (collection ofvalues)&gt; in the sorted inputs.The output of the reduce task is typically written to a RecordWriter viaTaskInputOutputContext.write(Object, Object).The output of the Reducer is not re-sorted.Reference: org.apache.hadoop.mapreduce, ClassReducer&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt;

--------------------------------------------------------------------------------------------------
Question: 55
Assuming default settings, which best describes the order of data provided to a reducer’s reducemethod:

A.The keys given to a reducer aren’t in a predictable order, but the values associated with those keys always are.

B.Both the keys and values passed to a reducer always appear in sorted order.

C.Neither keys nor values are in any predictable order.

D.The keys given to a reducer are in sorted order but the values associated with each key are in no predictable order

Explanation:Reducer has 3 primary phases:1. ShuffleThe Reducer copies the sorted output from each Mapper using HTTP across the network.2. SortThe framework merge sorts Reducer inputs by keys (since different Mappers may have output thesame key).

The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they aremerged.SecondarySortTo achieve a secondary sort on the values returned by the value iterator, the application shouldextend the key with the secondary key and define a grouping comparator. The keys will be sortedusing the entire key, but will be grouped using the grouping comparator to decide which keys andvalues are sent in the same call to reduce.3. ReduceIn this phase the reduce(Object, Iterable, Context) method is called for each &lt;key, (collection ofvalues)&gt; in the sorted inputs.The output of the reduce task is typically written to a RecordWriter viaTaskInputOutputContext.write(Object, Object).The output of the Reducer is not re-sorted.Reference: org.apache.hadoop.mapreduce, ClassReducer&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt;

--------------------------------------------------------------------------------------------------
Question: 56
You wrote a map function that throws a runtime exception when it encounters a control characterin input data. The input supplied to your mapper contains twelve such characters totals, spreadacross five file splits. The first four file splits each have two control characters and the last split hasfour control characters.Indentify the number of failed task attempts you can expect when you run the job withmapred.max.map.attempts set to 4:

A.You will have forty-eight failed task attempts

B.You will have seventeen failed task attempts

C.You will have five failed task attempts

D.You will have twelve failed task attempts

E.You will have twenty failed task attempts 

Explanation:There will be four failed task attempts for each of the five file splits.

Note: When the jobtracker is notified of a task attempt that has failed (by the tasktracker’s heartbeat tall),it will reschedule execution of the task. The jobtracker will try to avoid rescheduling the task on atasktracker where it has previously tailed. Furthermore, if a task fails four times (or more), it willnot be retried further. This value is configurable: the maximum number of attempts to run a task iscontrolled by the mapred.map.max.attempts property for map tasks andmapred.reduce.max.attempts for reduce tasks. By default, if any task fails four times (or whateverthe maximum number of attempts is configured to), the whole job fails.

--------------------------------------------------------------------------------------------------
Question: 57
You want to populate an associative array in order to perform a map-side join. You’ve decided toput this information in a text file, place that file into the DistributedCache and read it in yourMapper before any records are processed.Indentify which method in the Mapper you should use to implement code for reading the file andpopulating the associative array?

A.combine 

B.map

C.init

D.configure 

Explanation:See 3) below.Here is an illustrative example on how to use the DistributedCache:// Setting up the cache for the application1. Copy the requisite files to the FileSystem:$ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar$ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar$ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz$ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz

2. Setup the application’s JobConf:JobConf job = new JobConf();DistributedCache.addCacheFile(new URI(“/myapp/lookup.dat#lookup.dat”), job);DistributedCache.addCacheArchive(new URI(“/myapp/map.zip”, job);DistributedCache.addFileToClassPath(new Path(“/myapp/mylib.jar”), job);DistributedCache.addCacheArchive(new URI(“/myapp/mytar.tar”, job);DistributedCache.addCacheArchive(new URI(“/myapp/mytgz.tgz”, job);DistributedCache.addCacheArchive(new URI(“/myapp/mytargz.tar.gz”, job);3. Use the cached files in the Mapperor Reducer:public static class MapClass extends MapReduceBase implements Mapper&lt;K, V, K, V&gt; {private Path[] localArchives;private Path[] localFiles;public void configure(JobConf job) {// Get the cached archives/fileslocalArchives = DistributedCache.getLocalCacheArchives(job);localFiles = DistributedCache.getLocalCacheFiles(job);}public void map(K key, V value, OutputCollector&lt;K, V&gt; output, Reporter reporter) throws IOException {// Use data from the cached archives/files here// …// …output.collect(k, v);}}Reference: org.apache.hadoop.filecache , Class DistributedCache

--------------------------------------------------------------------------------------------------
Question: 58
You’ve written a MapReduce job that will process 500 million input records and generated 500million key-value pairs. The data is not uniformly distributed. Your MapReduce job will create asignificant amount of intermediate data that it needs to transfer between mappers and reduceswhich is a potential bottleneck. A custom implementation of which interface is most likely to reducethe amount of intermediate data transferred across the network?

A.Partitioner

B.OutputFormat

C.WritableComparable

D.Writable

E.InputFormat

F.Combiner

--------------------------------------------------------------------------------------------------
Question: 59
Can you use MapReduce to perform a relational join on two large tables sharing a key? Assumethat the two tables are formatted as comma-separated files in HDFS.

A.Yes.

B.Yes, but only if one of the tables fits into memory

C.Yes, so long as both tables fit into memory.

D.No, MapReduce cannot perform relational operations.

E.No, but it can be done with either Pig or Hive.

Explanation:Note: * Join Algorithms in MapReduceA) Reduce-side joinB) Map-side joinC) In-memory join/ Striped Striped variant variant

/ Memcached variant* Which join to use?/ In-memory join &gt; map-side join &gt; reduce-side join/ Limitations of each?In-memory join: memoryMap-side join: sort order and partitioningReduce-side join: general purpose

--------------------------------------------------------------------------------------------------
Question: 60
You have just executed a MapReduce job. Where is intermediate data written to after beingemitted from the Mapper’s map method?

A.Intermediate data in streamed across the network from Mapper to the Reduce and is never written to disk.

B.Into in-memory buffers on the TaskTracker node running the Mapper that spill over and are written into HDFS.

C.Into in-memory buffers that spill over to the local file system of the TaskTracker node running the Mapper.

D.Into in-memory buffers that spill over to the local file system (outside HDFS) of the TaskTracker node running the Reducer

E.Into in-memory buffers on the TaskTracker node running the Reducer that spill over and are written into HDFS.

--------------------------------------------------------------------------------------------------
Question: 61
You want to understand more about how users browse your public website, such as which pages

they visit prior to placing an order. You have a farm of 200 web servers hosting your website. Howwill you gather this data for your analysis?

A.Ingest the server web logs into HDFS using Flume.

B.Write a MapReduce job, with the web servers for mappers, and the Hadoop cluster nodes for reduces.

C.Import all users’ clicks from your OLTP databases into Hadoop, using Sqoop.

D.Channel these clickstreams inot Hadoop using Hadoop Streaming.

E.Sample the weblogs from the web servers, copying them into Hadoop using curl.

Explanation:Hadoop MapReduce for Parsing WeblogsHere are the steps for parsing a log file using Hadoop MapReduce:Load log files into the HDFS location using this Hadoop command:hadoop fs -put &lt;local file path of weblogs&gt; &lt;hadoop HDFS location&gt;The Opencsv2.3.jar framework is used for parsing log records.Below is the Mapper program for parsing the log file from the HDFS location.public static class ParseMapper extends Mapper&lt;Object, Text, NullWritable,Text &gt;{private Text word = new Text();public void map(Object key, Text value, Context context) throws IOException, InterruptedException {CSVParser parse = new CSVParser(‘ ‘,’\”‘);String sp[]=parse.parseLine(value.toString());int spSize=sp.length;StringBuffer rec= new StringBuffer();for(int i=0;i&lt;spSize;i++){rec.append(sp[i]);if(i!=(spSize-1))rec.append(“,”);}word.set(rec.toString());context.write(NullWritable.get(), word);}

}The command below is the Hadoop-based log parse execution. TheMapReduce program isattached in this article. You can add extra parsing methods in the class. Be sure to create a newJAR with any change and move it to the Hadoop distributed job tracker system.hadoop jar &lt;path of logparse jar&gt; &lt;hadoop HDFS logfile path&gt; &lt;output path of parsed log file&gt;The output file is stored in the HDFS location, and the output file name starts with “part-“.

--------------------------------------------------------------------------------------------------
Question: 62
MapReduce v2 (MRv2/YARN) is designed to address which two issues?

A.Single point of failure in the NameNode.

B.Resource pressure on the JobTracker.

C.HDFS latency.

D.Ability to run frameworks other than MapReduce, such as MPI.

E.Reduce complexity of the MapReduce APIs.

F.Standardize on a single MapReduce API.

Explanation:YARN (Yet Another Resource Negotiator), as an aspect of Hadoop, has two majorkinds of benefits:* (D) The ability to use programming frameworks other than MapReduce./ MPI (Message Passing Interface) was mentioned as a paradigmatic example of a MapReducealternative* Scalability, no matter what programming framework you use.Note: * The fundamental idea of MRv2 is to split up the two major functionalities of the JobTracker,resource management and job scheduling/monitoring, into separate daemons. The idea is to havea global ResourceManager (RM) and per-application ApplicationMaster (AM). An application iseither a single job in the classical sense of Map-Reduce jobs or a DAG of jobs.* (B) The central goal of YARN is to clearly separate two things that are unfortunately smushedtogether in current Hadoop, specifically in (mainly) JobTracker:/ Monitoring the status of the cluster with respect to which nodes have which resources available.Under YARN, this will be global./ Managing the parallelization execution of any specific job. Under YARN, this will be done

separately for each job.The current Hadoop MapReduce system is fairly scalable — Yahoo runs 5000 Hadoop jobs, trulyconcurrently, on a single cluster, for a total 1.5 – 2 millions jobs/cluster/month. Still, YARN willremove scalability bottlenecksReference: Apache Hadoop YARN – Concepts &amp; Applications

--------------------------------------------------------------------------------------------------
Question: 63
You need to run the same job many times with minor variations. Rather than hardcoding all jobconfiguration options in your drive code, you’ve decided to have your Driver subclassorg.apache.hadoop.conf.Configured and implement the org.apache.hadoop.util.Tool interface.Indentify which invocation correctly passes.mapred.job.name with a value of Example to Hadoop?

A.hadoop “mapred.job.name=Example” MyDriver input output

B.hadoop MyDriver mapred.job.name=Example input output

C.hadoop MyDrive –D mapred.job.name=Example input output

D.hadoop setproperty mapred.job.name=Example MyDriver input output

E.hadoop setproperty (“mapred.job.name=Example”) MyDriver input output

--------------------------------------------------------------------------------------------------
Question: 64
You are developing a MapReduce job for sales reporting. The mapper will process input keysrepresenting the year (IntWritable) and input values representing product indentifies (Text).Indentify what determines the data types used by the Mapper for a given job.

A.The key and value types specified in the JobConf.setMapInputKeyClass and JobConf.setMapInputValuesClass methods

B.The data types specified in HADOOP_MAP_DATATYPES environment variable

C.The mapper-specification.xml file submitted with the job determine the mapper’s input key and value types.

D.The InputFormat used by the job determines the mapper’s input key and value types.

--------------------------------------------------------------------------------------------------
Question: 65
Identify the MapReduce v2 (MRv2 / YARN) daemon responsible for launching applicationcontainers and monitoring application resource usage?

A.ResourceManager

B.NodeManager

C.ApplicationMaster

D.ApplicationMasterService

E.TaskTracker

F.JobTracker

--------------------------------------------------------------------------------------------------
Question: 66
Which best describes how TextInputFormat processes input files and line breaks?

A.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReader of the split that contains the beginning of the broken line.

B.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReaders of both splits containing the broken line.

C.The input file is split exactly at the line breaks, so each RecordReader will read a series of complete lines.

D.Input file splits may cross line breaks. A line that crosses file splits is ignored.

E.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReader of the split that contains the end of the broken line.

--------------------------------------------------------------------------------------------------
Question: 67
For each input key-value pair, mappers can emit:

A.As many intermediate key-value pairs as designed. There are no restrictions on the types of those key-value pairs (i.e., they can be heterogeneous).

B.As many intermediate key-value pairs as designed, but they cannot be of the same type as the input key-value pair.

C.One intermediate key-value pair, of a different type.

D.One intermediate key-value pair, but of the same type.

E.As many intermediate key-value pairs as designed, as long as all the keys have the same types and all the values have the same type.

--------------------------------------------------------------------------------------------------
Question: 68
You have the following key-value pairs as output from your Map task:(the, 1)(fox, 1)(faster, 1)(than, 1)(the, 1)(dog, 1)How many keys will be passed to the Reducer’s reduce method?

A.Six 

B.Five

C.Four

D.Two

E.One

F.Three 

--------------------------------------------------------------------------------------------------
Question: 69
You have user profile records in your OLPT database, that you want to join with web logs youhave already ingested into the Hadoop file system. How will you obtain these user records?

A.HDFS command

B.Pig LOAD command

C.Sqoop import

D.Hive LOAD DATA command

E.Ingest with Flume agents

F.Ingest with Hadoop Streaming

Explanation:Apache Hadoop and Pig provide excellent tools for extracting and analyzing datafrom very large Web logs.We use Pig scripts for sifting through the data and to extract useful information from the Web logs.We load the log file into Pig using the LOAD command.raw_logs = LOAD ‘apacheLog.log’ USING TextLoader AS (line:chararray);Note 1:Data Flow and Components* Content will be created by multiple Web servers and logged in local hard discs. This content willthen be pushed to HDFS using FLUME framework. FLUME has agents running on Web servers;these are machines that collect data intermediately using collectors and finally push that data toHDFS.* Pig Scripts are scheduled to run using a job scheduler (could be cron or any sophisticated batchjob solution). These scripts actually analyze the logs on various dimensions and extract theresults. Results from Pig are by default inserted into HDFS, but we can use storageimplementation for other repositories also such as HBase, MongoDB, etc. We have also tried thesolution with HBase (please see the implementation section). Pig Scripts can either push this datato HDFS and then MR jobs will be required to read and push this data into HBase, or Pig scriptscan push this data into HBase directly. In this article, we use scripts to push data onto HDFS, aswe are showcasing the Pig framework applicability for log analysis at large scale.* The database HBase will have the data processed by Pig scripts ready for reporting and furtherslicing and dicing.* The data-access Web service is a REST-based service that eases the access and integrationswith data clients. The client can be in any language to access REST-based API. These clientscould be BI- or UI-based clients.

Note 2:The Log Analysis Software Stack* Hadoop is an open source framework that allows users to process very large data in parallel. It’sbased on the framework that supports Google search engine. The Hadoop core is mainly dividedinto two modules:1. HDFS is the Hadoop Distributed File System. It allows you to store large amounts of data usingmultiple commodity servers connected in a cluster.2. Map-Reduce (MR) is a framework for parallel processing of large data sets. The defaultimplementation is bonded with HDFS.* The database can be a NoSQL database such as HBase. The advantage of a NoSQL databaseis that it provides scalability for the reporting module as well, as we can keep historical processeddata for reporting purposes. HBase is an open source columnar DB or NoSQL DB, which usesHDFS. It can also use MR jobs to process data. It gives real-time, random read/write access tovery large data sets — HBase can save very large tables having million of rows. It’s a distributeddatabase and can also keep multiple versions of a single row.* The Pig framework is an open source platform for analyzing large data sets and is implementedas a layered language over the Hadoop Map-Reduce framework. It is built to ease the work ofdevelopers who write code in the Map-Reduce format, since code in Map-Reduce format needs tobe written in Java. In contrast, Pig enables users to write code in a scripting language.* Flume is a distributed, reliable and available service for collecting, aggregating and moving alarge amount of log data (src flume-wiki). It was built to push large logs into Hadoop-HDFS forfurther processing. It’s a data flow solution, where there is an originator and destination for eachnode and is divided into Agent and Collector tiers for collecting logs and pushing them todestination storage.Reference: Hadoop and Pig for Large-Scale Web Log Analysis

--------------------------------------------------------------------------------------------------
Question: 70
What is the disadvantage of using multiple reducers with the default HashPartitioner anddistributing your workload across you cluster?

A.You will not be able to compress the intermediate data.

B.You will longer be able to take advantage of a Combiner.

C.By using multiple reducers with the default HashPartitioner, output files may not be in globally sorted order.

D.There are no concerns with this approach. It is always advisable to use multiple reduces.

--------------------------------------------------------------------------------------------------
Question: 71
Given a directory of files with the following structure: line number, tab character, string:Example:1abialkjfjkaoasdfjksdlkjhqweroij2kadfjhuwqounahagtnbvaswslmnbfgy3kjfteiomndscxeqalkzhtopedkfsikjYou want to send each line as one record to your Mapper. Which InputFormat should you use tocomplete the line: conf.setInputFormat (____.class) ; ?

A.SequenceFileAsTextInputFormat

B.SequenceFileInputFormat

C.KeyValueFileInputFormat

D.BDBInputFormat

Explanation:Note:The output format for your first MR job should be SequenceFileOutputFormat – this will store theKey/Values output from the reducer in a binary format, that can then be read back in, in yoursecond MR job using SequenceFileInputFormat.

Reference: How to parse CustomWritable from text in Hadoophttp://stackoverflow.com/questions/9721754/how-to-parse-customwritable-from-text-in-hadoop(see answer 1 and then see the comment #1 for it)

--------------------------------------------------------------------------------------------------
Question: 72
You need to perform statistical analysis in your MapReduce job and would like to call methods inthe Apache Commons Math library, which is distributed as a 1.3 megabyte Java archive (JAR) file.Which is the best way to make this library available to your MapReducer job at runtime?

A.Have your system administrator copy the JAR to all nodes in the cluster and set its location in the HADOOP_CLASSPATH environment variable before you submit your job.

B.Have your system administrator place the JAR file on a Web server accessible to all cluster nodes and then set the HTTP_JAR_URL environment variable to its location.

C.When submitting the job on the command line, specify the –libjars option followed by the JAR file path. 

D.Package your code and the Apache Commands Math library into a zip file named JobJar.zip

--------------------------------------------------------------------------------------------------
Question: 73
The Hadoop framework provides a mechanism for coping with machine issues such as faultyconfiguration or impending hardware failure. MapReduce detects that one or a number ofmachines are performing poorly and starts more copies of a map or reduce task. All the tasks runsimultaneously and the task finish first are used. This is called:

A.Combine

B.IdentityMapper

C.IdentityReducer

D.Default Partitioner

E.Speculative Execution

--------------------------------------------------------------------------------------------------
Question: 74
For each intermediate key, each reducer task can emit:

A.As many final key-value pairs as desired. There are no restrictions on the types of those keyvalue pairs (i.e., they can be heterogeneous).

B.As many final key-value pairs as desired, but they must have the same type as the intermediate key-value pairs.

C.As many final key-value pairs as desired, as long as all the keys have the same type and all the values have the same type.

D.One final key-value pair per value associated with the key; no restrictions on the type.

E.One final key-value pair per key; no restrictions on the type.

--------------------------------------------------------------------------------------------------
Question: 75
What data does a Reducer reduce method process?

A.All the data in a single input file.

B.All data produced by a single mapper.

C.All data for a given key, regardless of which mapper(s) produced it.

D.All data for a given value, regardless of which mapper(s) produced it.

--------------------------------------------------------------------------------------------------
Question: 76
All keys used for intermediate output from mappers must: 

A.Implement a splittable compression algorithm.

B.Be a subclass of FileInputFormat.

C.Implement WritableComparable.

D.Override isSplitable.

E.Implement a comparator for speedy sorting. 

--------------------------------------------------------------------------------------------------
Question: 77
On a cluster running MapReduce v1 (MRv1), a TaskTracker heartbeats into the JobTracker onyour cluster, and alerts the JobTracker it has an open map task slot.What determines how the JobTracker assigns each map task to a TaskTracker?

A.The amount of RAM installed on the TaskTracker node.

B.The amount of free disk space on the TaskTracker node.

C.The number and speed of CPU cores on the TaskTracker node.

D.The average system load on the TaskTracker node over the past fifteen (15) minutes.

E.The location of the InsputSplit to be processed in relation to the location of the node.

Explanation:The TaskTrackers send out heartbeat messages to the JobTracker, usually everyfew minutes, to reassure the JobTracker that it is still alive. These message also inform theJobTracker of the number of available slots, so the JobTracker can stay up to date with where inthe cluster work can be delegated. When the JobTracker tries to find somewhere to schedule a

task within the MapReduce operations, it first looks for an empty slot on the same server thathosts the DataNode containing the data, and if not, it looks for an empty slot on a machine in thesame rack.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, HowJobTracker schedules a task?

--------------------------------------------------------------------------------------------------
Question: 78
What is a SequenceFile?

A.A SequenceFile contains a binary encoding of an arbitrary number of homogeneous Writable objects

B.A SequenceFile contains a binary encoding of an arbitrary number of heterogeneous Writable objects

C.A SequenceFile contains a binary encoding of an arbitrary number of WritableComparable objects, in sorted order.

D.A SequenceFile contains a binary encoding of an arbitrary number key-value pairs. Each key must be the same type. Each value must be the same type. 

--------------------------------------------------------------------------------------------------
Question: 79
client application creates an HDFS file named foo.txt with a replication factor of 3. Identify whichbest describes the file access rules in HDFS if the file has a single block that is stored on datanodes A, B and C?

A.The file will be marked as corrupted if data node B fails during the creation of the file.

B.Each data node locks the local file to prohibit concurrent readers and writers of the file.

C.Each data node stores a copy of the file in the local file system with the same name as the HDFS file.

D.The file can be accessed if at least one of the data nodes storing the file is available.

--------------------------------------------------------------------------------------------------
Question: 80
In a MapReduce job, you want each of your input files processed by a single map task. How doyou configure a MapReduce job so that a single map task processes each input file regardless ofhow many blocks the input file occupies?

A.Increase the parameter that controls minimum split size in the job configuration.

B.Write a custom MapRunner that iterates over all key-value pairs in the entire file.

C.Set the number of mappers equal to the number of input files you want to process.

D.Write a custom FileInputFormat and override the method isSplitable to always return false.

Explanation:FileInputFormat is the base class for all file-based InputFormats. This provides ageneric implementation of getSplits(JobContext). Subclasses of FileInputFormat can also override

the isSplitable(JobContext, Path) method to ensure input-files are not split-up and are processedas a whole by Mappers.Reference: org.apache.hadoop.mapreduce.lib.input, Class FileInputFormat&lt;K,V&gt;

--------------------------------------------------------------------------------------------------
Question: 81
Which process describes the lifecycle of a Mapper?

A.The JobTracker calls the TaskTracker’s configure () method, then its map () method and finally its close () method.

B.The TaskTracker spawns a new Mapper to process all records in a single input split.

C.The TaskTracker spawns a new Mapper to process each key-value pair.

D.The JobTracker spawns a new Mapper to process all records in a single file.

Explanation:For each map instance that runs, the TaskTracker creates a new instance of yourmapper.Note:* The Mapper is responsible for processing Key/Value pairs obtained from the InputFormat. Themapper may perform a number of Extraction and Transformation functions on the Key/Value pairbefore ultimately outputting none, one or many Key/Value pairs of the same, or different Key/Valuetype.* With the new Hadoop API, mappers extend the org.apache.hadoop.mapreduce.Mapper class.This class defines an ‘Identity’ map function by default – every input Key/Value pair obtained fromthe InputFormat is written out.Examining the run() method, we can see the lifecycle of the mapper:/*** Expert users can override this method for more complete control over the* execution of the Mapper.* @param context* @throws IOException*/public void run(Context context) throws IOException, InterruptedException {setup(context);while (context.nextKeyValue()) {map(context.getCurrentKey(), context.getCurrentValue(), context);}

cleanup(context);}setup(Context) – Perform any setup for the mapper. The default implementation is a no-op method.map(Key, Value, Context) – Perform a map operation in the given Key / Value pair. The defaultimplementation calls Context.write(Key, Value)cleanup(Context) – Perform any cleanup for the mapper. The default implementation is a no-opmethod.Reference: Hadoop/MapReduce/Mapper

--------------------------------------------------------------------------------------------------
Question: 82
Determine which best describes when the reduce method is first called in a MapReduce job?

A.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The programmer can configure in the job what percentage of the intermediate data should arrive before the reduce method begins.

B.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The reduce method is called only after all intermediate data has been copied and sorted.

C.Reduce methods and map methods all start at the beginning of a job, in order to provide optimal performance for map-only or reduce-only jobs.

D.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The reduce method is called as soon as the intermediate key-value pairs start to arrive.

Explanation:* In a MapReduce job reducers do not start executing the reduce method until the allMap jobs have completed. Reducers start copying intermediate key-value pairs from the mappersas soon as they are available. The programmer defined reduce method is called only after all themappers have finished.* Reducers start copying intermediate key-value pairs from the mappers as soon as they areavailable. The progress calculation also takes in account the processing of data transfer which isdone by reduce process, therefore the reduce progress starts showing up as soon as anyintermediate key-value pair for a mapper is available to be transferred to reducer. Though thereducer progress is updated still the programmer defined reduce method is called only after all themappers have finished.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , When is the

reducers are started in a MapReduce job?

--------------------------------------------------------------------------------------------------
Question: 83
To process input key-value pairs, your mapper needs to lead a 512 MB data file in memory. Whatis the best way to accomplish this?

A.Serialize the data file, insert in it the JobConf object, and read the data into memory in the configure method of the mapper.

B.Place the data file in the DistributedCache and read the data into memory in the map method of the mapper.

C.Place the data file in the DataCache and read the data into memory in the configure method of the mapper.

D.Place the data file in the DistributedCache and read the data into memory in the configure method of the mapper.

Explanation:Hadoop has a distributed cache mechanism to make available file locally that maybe needed by Map/Reduce jobsUse CaseLets understand our Use Case a bit more in details so that we can follow-up the code snippets.We have a Key-Value file that we need to use in our Map jobs. For simplicity, lets say we need toreplace all keywords that we encounter during parsing, with some other value.So what we need isA key-values files (Lets use a Properties files)The Mapper code that uses the codeWrite the Mapper code that uses itview sourceprint?01.public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {02.03.Properties cache;04.05.@Override06.protected void setup(Context context) throws IOException, InterruptedException {

07.super.setup(context);08.Path[] localCacheFiles = DistributedCache.getLocalCacheFiles(context.getConfiguration());09.10.if(localCacheFiles != null) {11.// expecting only single file here12.for (int i = 0; i &lt; localCacheFiles.length; i++) {13.Path localCacheFile = localCacheFiles[i];14.cache = new Properties();15.cache.load(new FileReader(localCacheFile.toString()));16.}17.} else {18.// do your error handling here19.}20.21.}22.23.@Override24.public void map(LongWritable key, Text value, Context context) throws IOException,InterruptedException {25.// use the cache here26.// if value contains some attribute, cache.get(&lt;value&gt;)

27.// do some action or replace with something else28.}29.30.}Note:* Distribute application-specific large, read-only files efficiently.DistributedCache is a facility provided by the Map-Reduce framework to cache files (text, archives,jars etc.) needed by applications.Applications specify the files, via urls (hdfs:// or http://) to be cached via the JobConf. TheDistributedCache assumes that the files specified via hdfs:// urls are already present on theFileSystem at the path specified by the url.Reference: Using Hadoop Distributed Cache

--------------------------------------------------------------------------------------------------
Question: 84
You have written a Mapper which invokes the following five calls to the OutputColletor.collectmethod:output.collect (new Text (“Apple”), new Text (“Red”) ) ;output.collect (new Text (“Banana”), new Text (“Yellow”) ) ;output.collect (new Text (“Apple”), new Text (“Yellow”) ) ;output.collect (new Text (“Cherry”), new Text (“Red”) ) ;output.collect (new Text (“Apple”), new Text (“Green”) ) ;How many times will the Reducer’s reduce method be invoked?

A.6

B.3

C.1

D.0

E.5

--------------------------------------------------------------------------------------------------
Question: 85
In a MapReduce job, the reducer receives all values associated with same key. Which statementbest describes the ordering of these values?

A.The values are in sorted order.

B.The values are arbitrarily ordered, and the ordering may vary from run to run of the same MapReduce job.

C.The values are arbitrary ordered, but multiple runs of the same MapReduce job will always have the same ordering.

D.Since the values come from mapper outputs, the reducers will receive contiguous sections of sorted values.

Explanation:Note: * Input to the Reducer is the sorted output of the mappers. * The framework calls the application’s Reduce function once for each unique key in the sorted

order.* Example:For the given sample input the first map emits:&lt; Hello, 1&gt; &lt; World, 1&gt; &lt; Bye, 1&gt; &lt; World, 1&gt; The second map emits:&lt; Hello, 1&gt; &lt; Hadoop, 1&gt; &lt; Goodbye, 1&gt; &lt; Hadoop, 1&gt;

--------------------------------------------------------------------------------------------------
Question: 86
You need to create a job that does frequency analysis on input data. You will do this by writing aMapper that uses TextInputFormat and splits each value (a line of text from an input file) intoindividual characters. For each one of these characters, you will emit the character as a key andan InputWritable as the value. As this will produce proportionally more intermediate data than inputdata, which two resources should you expect to be bottlenecks?

A.Processor and network I/O

B.Disk I/O and network I/O

C.Processor and RAM

D.Processor and disk I/O

--------------------------------------------------------------------------------------------------
Question: 87
Your client application submits a MapReduce job to your Hadoop cluster. Identify the Hadoopdaemon on which the Hadoop framework will look for an available slot schedule a MapReduceoperation.

A.TaskTracker

B.NameNode

C.DataNode

D.JobTracker

E.Secondary NameNode

Explanation:JobTracker is the daemon service for submitting and tracking MapReduce jobs inHadoop. There is only One Job Tracker process run on any hadoop cluster. Job Tracker runs onits own JVM process. In a typical production cluster its run on a separate machine. Each slavenode is configured with job tracker node location. The JobTracker is single point of failure for theHadoop MapReduce service. If it goes down, all running jobs are halted. JobTracker in Hadoopperforms following actions(from Hadoop Wiki:)

Client applications submit jobs to the Job tracker.The JobTracker talks to the NameNode to determine the location of the dataThe JobTracker locates TaskTracker nodes with available slots at or near the dataThe JobTracker submits the work to the chosen TaskTracker nodes.The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, theyare deemed to have failed and the work is scheduled on a different TaskTracker.A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to dothen: it may resubmit the job elsewhere, it may mark that specific record as something to avoid,and it may may even blacklist the TaskTracker as unreliable.When the work is completed, the JobTracker updates its status.Client applications can poll the JobTracker for information.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What is aJobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?

--------------------------------------------------------------------------------------------------
Question: 88
You want to count the number of occurrences for each unique word in the supplied input data.You’ve decided to implement this by having your mapper tokenize each word and emit a literalvalue 1, and then have your reducer increment a counter for each literal 1 it receives. Aftersuccessful implementing this, it occurs to you that you could optimize this by specifying acombiner. Will you be able to reuse your existing Reduces as your combiner in this case and whyor why not?

A.Yes, because the sum operation is both associative and commutative and the input and output types to the reduce method match.

B.No, because the sum operation in the reducer is incompatible with the operation of a Combiner.

C.No, because the Reducer and Combiner are separate interfaces.

D.No, because the Combiner is incompatible with a mapper which doesn’t use the same data type for both the key and value.

E.Yes, because Java is a polymorphic object-oriented language and thus reducer code can be reused as a combiner.

--------------------------------------------------------------------------------------------------
Question: 89
Which project gives you a distributed, Scalable, data store that allows you random, realtimeread/write access to hundreds of terabytes of data?

A.HBase

B.Hue

C.Pig

D.Hive

E.Oozie

F.Flume

G.Sqoop

Explanation:Use Apache HBase when you need random, realtime read/write access to your BigData.Note: This project’s goal is the hosting of very large tables — billions of rows X millions of columns— atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned,column-oriented store modeled after Google’s Bigtable: A Distributed Storage System forStructured Data by Chang et al. Just as Bigtable leverages the distributed data storage providedby the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoopand HDFS.Features:

Linear and modular scalability.Strictly consistent reads and writes.Automatic and configurable sharding of tablesAutomatic failover support between RegionServers.Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.Easy to use Java API for client access.Block cache and Bloom Filters for real-time queries.Query predicate push down via server side FiltersThrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary dataencoding optionsExtensible jruby-based (JIRB) shellSupport for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMXReference: http://hbase.apache.org/ (when would I use HBase? First sentence)

--------------------------------------------------------------------------------------------------
Question: 90
You use the hadoop fs –put command to write a 300 MB file using and HDFS block size of 64 MB.Just after this command has finished writing 200 MB of this file, what would another user seewhen trying to access this life?

A.They would see Hadoop throw an ConcurrentFileAccessException when they try to access this file.

B.They would see the current state of the file, up to the last bit written by the command.

C.They would see the current of the file through the last completed block.

D.They would see no content until the whole file written and closed.

--------------------------------------------------------------------------------------------------
Question: 91
Identify the tool best suited to import a portion of a relational database every day as files intoHDFS, and generate Java classes to interact with that imported data?

A.Oozie

B.Flume

C.Pig

D.Hue

E.Hive

F.Sqoop

G.fuse-dfs

--------------------------------------------------------------------------------------------------
Question: 92
You have a directory named jobdata in HDFS that contains four files: _first.txt, second.txt, .third.txt

and #data.txt. How many files will be processed by the FileInputFormat.setInputPaths () commandwhen it’s given a path object representing this directory?

A.Four, all files will be processed

B.Three, the pound sign is an invalid character for HDFS file names

C.Two, file names with a leading period or underscore are ignored

D.None, the directory cannot be named jobdata

E.One, no special characters can prefix the name of an input file 

--------------------------------------------------------------------------------------------------
Question: 93
You write MapReduce job to process 100 files in HDFS. Your MapReduce algorithm usesTextInputFormat: the mapper applies a regular expression over input values and emits key-valuespairs with the key consisting of the matching text, and the value containing the filename and byteoffset. Determine the difference between setting the number of reduces to one and settings thenumber of reducers to zero.

A.There is no difference in output between the two settings.

B.With zero reducers, no reducer runs and the job throws an exception. With one reducer, instances of matching patterns are stored in a single file on HDFS.

C.With zero reducers, all instances of matching patterns are gathered together in one file on HDFS. With one reducer, instances of matching patterns are stored in multiple files on HDFS.

D.With zero reducers, instances of matching patterns are stored in multiple files on HDFS. With one reducer, all instances of matching patterns are gathered together in one file on HDFS.

Explanation:* It is legal to set the number of reduce-tasks to zero if no reduction is desired.In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set bysetOutputPath(Path). The framework does not sort the map-outputs before writing them out to theFileSystem.* Often, you may want to process input data using a map function only. To do this, simply setmapreduce.job.reduces to zero. The MapReduce framework will not create any reducer tasks.Rather, the outputs of the mapper tasks will be the final output of the job.Note:

ReduceIn this phase the reduce(WritableComparable, Iterator, OutputCollector, Reporter) method iscalled for each &lt;key, (list of values)&gt; pair in the grouped inputs.The output of the reduce task is typically written to the FileSystem viaOutputCollector.collect(WritableComparable, Writable).Applications can use the Reporter to report progress, set application-level status messages andupdate Counters, or just indicate that they are alive.The output of the Reducer is not sorted.

--------------------------------------------------------------------------------------------------
Question: 94
combiner reduces:

A.The number of values across different keys in the iterator supplied to a single reduce method call.

B.The amount of intermediate data that must be transferred between the mapper and reducer.

C.The number of input files a mapper must process.

D.The number of output files a reducer must produce.

--------------------------------------------------------------------------------------------------
Question: 95
In a MapReduce job with 500 map tasks, how many map task attempts will there be?

A.It depends on the number of reduces in the job.

B.Between 500 and 1000.

C.At most 500.

D.At least 500.

E.Exactly 500.

--------------------------------------------------------------------------------------------------
Question: 96
MapReduce v2 (MRv2/YARN) splits which major functions of the JobTracker into separatedaemons? Select two.

A.Heath states checks (heartbeats)

B.Resource management

C.Job scheduling/monitoring

D.Job coordination between the ResourceManager and NodeManager

E.Launching tasks 

F.Managing file system metadata

G.MapReduce metric reporting

H.Managing tasks

Explanation:The fundamental idea of MRv2 is to split up the two major functionalities of theJobTracker, resource management and job scheduling/monitoring, into separate daemons. Theidea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM). Anapplication is either a single job in the classical sense of Map-Reduce jobs or a DAG of jobs.Note: The central goal of YARN is to clearly separate two things that are unfortunately smushed togetherin current Hadoop, specifically in (mainly) JobTracker:/ Monitoring the status of the cluster with respect to which nodes have which resources available.Under YARN, this will be global./ Managing the parallelization execution of any specific job. Under YARN, this will be doneseparately for each job.

Reference: Apache Hadoop YARN – Concepts &amp; Applications

--------------------------------------------------------------------------------------------------
Question: 97
What types of algorithms are difficult to express in MapReduce v1 (MRv1)?

A.Algorithms that require applying the same mathematical function to large numbers of individual binary records.

B.Relational operations on large amounts of structured and semi-structured data.

C.Algorithms that require global, sharing states.

D.Large-scale graph algorithms that require one-step link traversal.

E.Text analysis algorithms on large collections of unstructured text (e.g, Web crawls).

Explanation:See 3) below.Limitations of Mapreduce – where not to use MapreduceWhile very powerful and applicable to a wide variety of problems, MapReduce is not the answer toevery problem. Here are some problems I found where MapReudce is not suited and some papersthat address the limitations of MapReuce.1. Computation depends on previously computed valuesIf the computation of a value depends on previously computed values, then MapReduce cannot beused. One good example is the Fibonacci series where each value is summation of the previoustwo values. i.e., f(k+2) = f(k+1) + f(k). Also, if the data set is small enough to be computed on asingle machine, then it is better to do it as a single reduce(map(data)) operation rather than goingthrough the entire map reduce process. 2. Full-text indexing or ad hoc searchingThe index generated in the Map step is one dimensional, and the Reduce step must not generatea large amount of data or there will be a serious performance degradation. For example,CouchDB’s MapReduce may not be a good fit for full-text indexing or ad hoc searching. This is aproblem better suited for a tool such as Lucene.3. Algorithms depend on shared global stateSolutions to many interesting problems in text processing do not require global synchronization.As a result, they can be expressed naturally in MapReduce, since map and reduce tasks runindependently and in isolation. However, there are many examples of algorithms that depend

crucially on the existence of shared global state during processing, making them difficult toimplement in MapReduce (since the single opportunity for global synchronization in MapReduce isthe barrier between the map and reduce phases of processing)Reference: Limitations of Mapreduce – where not to use Mapreduce

--------------------------------------------------------------------------------------------------
Question: 98
In the reducer, the MapReduce API provides you with an iterator over Writable values. What doescalling the next () method return? 

A.It returns a reference to a different Writable object time.

B.It returns a reference to a Writable object from an object pool.

C.It returns a reference to the same Writable object each time, but populated with different data.

D.It returns a reference to a Writable object. The API leaves unspecified whether this is a reused object or a new object.

E.It returns a reference to the same Writable object if the next value is the same as the previous value, or a new Writable object otherwise.

--------------------------------------------------------------------------------------------------
Question: 99
Table metadata in Hive is:

A.Stored as metadata on the NameNode.

B.Stored along with the data in HDFS.

C.Stored in the Metastore.

D.Stored in ZooKeeper.

Explanation:By default, hive use an embedded Derby database to store metadata information.The metastore is the “glue” between Hive and HDFS. It tells Hive where your data files live inHDFS, what type of data they contain, what tables they belong to, etc.

The Metastore is an application that runs on an RDBMS and uses an open source ORM layercalled DataNucleus, to convert object representations into a relational schema and vice versa.They chose this approach as opposed to storing this information in hdfs as they need theMetastore to be very low latency. The DataNucleus layer allows them to plugin many differentRDBMS technologies.Note: * By default, Hive stores metadata in an embedded Apache Derby database, and otherclient/server databases like MySQL can optionally be used.* features of Hive include:Metadata storage in an RDBMS, significantly reducing the time to perform semantic checks duringquery execution.Reference: Store Hive Metadata into RDBMS

--------------------------------------------------------------------------------------------------
Question: 100
Analyze each scenario below and indentify which best describes the behavior of the defaultpartitioner?

A.The default partitioner assigns key-values pairs to reduces based on an internal random number generator.

B.The default partitioner implements a round-robin strategy, shuffling the key-value pairs to each reducer in turn. This ensures an event partition of the key space. 

C.The default partitioner computers the hash of the key. Hash values between specific ranges are associated with different buckets, and each bucket is assigned to a specific reducer.

D.The default partitioner computers the hash of the key and divides that valule modulo the number of reducers. The result determines the reducer assigned to process the key-value pair.

E.The default partitioner computers the hash of the value and takes the mod of that value with the number of reducers. The result determines the reducer assigned to process the key-value pair.

Explanation:The default partitioner computes a hash value for the key and assigns the partitionbased on this result.The default Partitioner implementation is called HashPartitioner. It uses the hashCode() method ofthe key objects modulo the number of partitions total to determine which partition to send a given(key, value) pair to.In Hadoop, the default partitioner is HashPartitioner, which hashes a record’s key to determine

which partition (and thus which reducer) the record belongs in.The number of partition is thenequal to the number of reduce tasks for the job.Reference: Getting Started With (Customized) Partitioning

--------------------------------------------------------------------------------------------------
Question: 101
You need to move a file titled “weblogs” into HDFS. When you try to copy the file, you can’t. Youknow you have ample space on your DataNodes. Which action should you take to relieve thissituation and store more files in HDFS?

A.Increase the block size on all current files in HDFS.

B.Increase the block size on your remaining files.

C.Decrease the block size on your remaining files.

D.Increase the amount of memory for the NameNode.

E.Increase the number of disks (or size) for the NameNode.

F.Decrease the block size on all current files in HDFS.

--------------------------------------------------------------------------------------------------
Question: 102
In a large MapReduce job with m mappers and n reducers, how many distinct copy operations willthere be in the sort/shuffle phase?

A.mXn (i.e., m multiplied by n)

B.n

C.m

D.m+n (i.e., m plus n)

E.mn (i.e., m to the power of n)

--------------------------------------------------------------------------------------------------
Question: 103
Workflows expressed in Oozie can contain:

A.Sequences of MapReduce and Pig. These sequences can be combined with other actions including forks, decision points, and path joins.

B.Sequences of MapReduce job only; on Pig on Hive tasks or jobs. These MapReduce sequences can be combined with forks and path joins.

C.Sequences of MapReduce and Pig jobs. These are limited to linear sequences of actions with exception handlers but no forks.

D.Iterntive repetition of MapReduce jobs until a desired answer or state is reached.

--------------------------------------------------------------------------------------------------
Question: 104
Which best describes what the map method accepts and emits?

A.It accepts a single key-value pair as input and emits a single key and list of corresponding values as output.

B.It accepts a single key-value pairs as input and can emit only one key-value pair as output.

C.It accepts a list key-value pairs as input and can emit only one key-value pair as output.

D.It accepts a single key-value pairs as input and can emit any number of key-value pair as output, including zero.

--------------------------------------------------------------------------------------------------
Question: 105
When can a reduce class also serve as a combiner without affecting the output of a MapReduceprogram?

A.When the types of the reduce operation’s input key and input value match the types of the reducer’s output key and output value and when the reduce operation is both communicative and associative.

B.When the signature of the reduce method matches the signature of the combine method.

C.Always. Code can be reused in Java since it is a polymorphic object-oriented programming language.

D.Always. The point of a combiner is to serve as a mini-reducer directly after the map phase to increase performance.

E.Never. Combiners and reducers must be implemented separately because they serve different purposes. 

--------------------------------------------------------------------------------------------------
Question: 106
You want to perform analysis on a large collection of images. You want to store this data in HDFSand process it with MapReduce but you also want to give your data analysts and data scientiststhe ability to process the data directly from HDFS with an interpreted high-level programminglanguage like Python. Which format should you use to store this data in HDFS?

A.SequenceFiles

B.Avro

C.JSON

D.HTML

E.XML

F.CSV

--------------------------------------------------------------------------------------------------
Question: 107
You want to run Hadoop jobs on your development workstation for testing before you submit themto your production cluster. Which mode of operation in Hadoop allows you to most closely simulate

a production cluster while using a single machine?

A.Run all the nodes in your production cluster as virtual machines on your development workstation.

B.Run the hadoop command with the –jt local and the –fs file:///options.

C.Run the DataNode, TaskTracker, NameNode and JobTracker daemons on a single machine.

D.Run simldooop, the Apache open-source software for simulating Hadoop clusters.

--------------------------------------------------------------------------------------------------
Question: 108
Your cluster’s HDFS block size in 64MB. You have directory containing 100 plain text files, each ofwhich is 100MB in size. The InputFormat for your job is TextInputFormat. Determine how manyMappers will run?

A.64

B.100

C.200

D.640

Explanation:Each file would be split into two as the block size (64 MB) is less than the file size(100 MB), so 200 mappers would be running.Note:If you’re not compressing the files then hadoop will process your large files (say 10G), with anumber of mappers related to the block size of the file.

Say your block size is 64M, then you will have ~160 mappers processing this 10G file (160*64 ~=10G). Depending on how CPU intensive your mapper logic is, this might be an acceptable blockssize, but if you find that your mappers are executing in sub minute times, then you might want toincrease the work done by each mapper (by increasing the block size to 128, 256, 512m – theactual size depends on how you intend to process the data).Reference: http://stackoverflow.com/questions/11014493/hadoop-mapreduce-appropriate-inputfiles-size (first answer, second paragraph)New Questions

--------------------------------------------------------------------------------------------------
Question: 109
Which of the following best describes the workings of TextInputFormat?

A.Input file splits may cross line breaks. A line that crosses tile splits is ignored.

B.The input file is split exactly at the line breaks, so each Record Reader will read a series of complete lines.

C.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReaders of both splits containing the broken line.

D.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReader of the split that contains the end of the broken line.

E.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReader of the split that contains the beginning of the broken line.

--------------------------------------------------------------------------------------------------
Question: 110
Which of the following statements most accurately describes the relationship between MapReduceand Pig?

A.Pig provides additional capabilities that allow certain types of data manipulation not possible with MapReduce.

B.Pig provides no additional capabilities to MapReduce. Pig programs are executed as MapReduce jobs via the Pig interpreter.

C.Pig programs rely on MapReduce but are extensible, allowing developers to do special-purpose processing not provided by MapReduce.

D.Pig provides the additional capability of allowing you to control the flow of multiple MapReduce jobs.

--------------------------------------------------------------------------------------------------
Question: 111
You need to import a portion of a relational database every day as files to HDFS, and generateJava classes to Interact with your imported data. Which of the following tools should you use toaccomplish this?

A.Pig

B.Hue

C.Hive

D.Flume

E.Sqoop

F.Oozie

G.fuse-dfs

Explanation:Sqoop (“SQL-to-Hadoop”) is a straightforward command-line tool with the following

capabilities:Imports individual tables or entire databases to files in HDFSGenerates Java classes to allow you to interact with your imported dataProvides the ability to import from SQL databases straight into your Hive data warehouseNote: Data Movement Between Hadoop and Relational DatabasesData can be moved between Hadoop and a relational database as a bulk data transfer, orrelational tables can be accessed from within a MapReduce map function.Note: * Cloudera’s Distribution for Hadoop provides a bulk data transfer tool (i.e., Sqoop) that importsindividual tables or entire databases into HDFS files. The tool also generates Java classes thatsupport interaction with the imported data. Sqoop supports all relational databases over JDBC,and Quest Software provides a connector (i.e., OraOop) that has been optimized for access todata residing in Oracle databases.Reference: http://log.medcl.net/item/2011/08/hadoop-and-mapreduce-big-data-analytics-gartner/(Data Movement between hadoop and relational databases, second paragraph)

--------------------------------------------------------------------------------------------------
Question: 112
You have an employee who is a Date Analyst and is very comfortable with SQL. He would like torun ad-hoc analysis on data in your HDFS duster. Which of the following is a data warehousingsoftware built on top of Apache Hadoop that defines a simple SQL-like query language well-suitedfor this kind of user?

A.Pig

B.Hue

C.Hive

D.Sqoop

E.Oozie

F.Flume

G.Hadoop Streaming

Explanation:Hive defines a simple SQL-like query language, called QL, that enables usersfamiliar with SQL to query the data. At the same time, this language also allows programmers whoare familiar with the MapReduce framework to be able to plug in their custom mappers and

reducers to perform more sophisticated analysis that may not be supported by the built-incapabilities of the language. QL can also be extended with custom scalar functions (UDF’s),aggregations (UDAF’s), and table functions (UDTF’s).Reference: https://cwiki.apache.org/Hive/ (Apache Hive, first sentence and second paragraph)

--------------------------------------------------------------------------------------------------
Question: 113
What is the preferred way to pass a small number of configuration parameters to a mapper orreducer?

A.As key-value pairs in the jobconf object.

B.As a custom input key-value pair passed to each mapper or reducer.

C.Using a plain text file via the Distributedcache, which each mapper or reducer reads.

D.Through a static variable in the MapReduce driver class (i.e., the class that submits the MapReduce job).

--------------------------------------------------------------------------------------------------
Question: 114
Given a Mapper, Reducer, and Driver class packaged into a jar, which is the correct way ofsubmitting the job to the cluster?

A.jar MyJar.jar

B.jar MyJar.jar MyDriverClass inputdir outputdir

C.hadoop jar MyJar.jar MyDriverClass inputdir outputdir

D.hadoop jar class MyJar.jar MyDriverClass inputdir outputdir

--------------------------------------------------------------------------------------------------
Question: 115
What is the difference between a failed task attempt and a killed task attempt?

A.A failed task attempt is a task attempt that threw an unhandled exception. A killed task attempt is one that was terminated by the JobTracker.

B.A failed task attempt is a task attempt that did not generate any key value pairs. A killed task attempt is a task attempt that threw an exception, and thus killed by the execution framework.

C.A failed task attempt is a task attempt that completed, but with an unexpected status value. A killed task attempt is a duplicate copy of a task attempt that was started as part of speculative execution.

D.A failed task attempt is a task attempt that threw a RuntimeException (i.e., the task fails). A killed task attempt is a task attempt that threw any other type of exception (e.g., IOException); the execution framework catches these exceptions and reports them as killed.

--------------------------------------------------------------------------------------------------
Question: 116
Custom programmer-defined counters in MapReduce are:

A.Lightweight devices for bookkeeping within MapReduce programs.

B.Lightweight devices for ensuring the correctness of a MapReduce program. Mappers Increment counters, and reducers decrement counters. If at the end of the program the counters read zero, then you are sure that the job completed correctly.

C.Lightweight devices for synchronization within MapReduce programs. You can use counters to coordinate execution between a mapper and a reducer.

--------------------------------------------------------------------------------------------------
Question: 117
MapReduce is well-suited for all of the following applications EXCEPT? (Choose one):

A.Text mining on a large collections of unstructured documents.

B.Analysis of large amounts of Web logs (queries, clicks, etc.).

C.Online transaction processing (OLTP) for an e-commerce Website.

D.Graph mining on a large social network (e.g., Facebook friends network).

--------------------------------------------------------------------------------------------------
Question: 118
Your Custer’s HOFS block size is 64MB. You have a directory containing 100 plain text files, eachof which Is 100MB in size. The InputFormat for your job is TextInputFormat. How many Mapperswill run?

A.64

B.100

C.200

D.640

--------------------------------------------------------------------------------------------------
Question: 119
Does the MapReduce programming model provide a way for reducers to communicate with eachother?

A.Yes, all reducers can communicate with each other by passing information through the jobconf object.

B.Yes, reducers can communicate with each other by dispatching intermediate key value pairs that get shuffled to another reduce

C.Yes, reducers running on the same machine can communicate with each other through shared memory, but not reducers on different machines.

D.No, each reducer runs independently and in isolation.

--------------------------------------------------------------------------------------------------
Question: 120
Which of the following best describes the map method input and output?

A.It accepts a single key-value pair as input and can emit only one key-value pair as output.

B.It accepts a list of key-value pairs as input hut run emit only one key value pair as output.

C.It accepts a single key-value pair as input and emits a single key and list of corresponding values as output

D.It accepts a single key-value pair as input and can emit any number of key-value pairs as output, including zero.

--------------------------------------------------------------------------------------------------
Question: 121
Your client application submits a MapReduce job to your Hadoop cluster. The Hadoop framework

looks for an available slot to schedule the MapReduce operations on which of the followingHadoop computing daemons?

A.DataNode

B.NameNode

C.JobTracker

D.TaskTracker

E.Secondary NameNode

--------------------------------------------------------------------------------------------------
Question: 122
Which MapReduce daemon runs on each slave node and participates in job execution?

A.TaskTracker

B.JobTracker

C.NameNode

D.Secondary NameNode

--------------------------------------------------------------------------------------------------
Question: 123
What is the standard configuration of slave nodes in a Hadoop cluster?

A.Each slave node runs a JobTracker and a DataNode daemon.

B.Each slave node runs a TaskTracker and a DataNode daemon.

C.Each slave node either runs a TaskTracker or a DataNode daemon, but not both.

D.Each slave node runs a DataNode daemon, but only a fraction of the slave nodes run TaskTrackers.

E.Each slave node runs a TaskTracker, but only a fraction of the slave nodes run DataNode daemons.

--------------------------------------------------------------------------------------------------
Question: 124
Which happens if the NameNode crashes?

A.HDFS becomes unavailable until the NameNode is restored.

B.The Secondary NameNode seamlessly takes over and there is no service interruption.

C.HDFS becomes unavailable to new MapReduce jobs, but running jobs will continue until completion.

D.HDFS becomes temporarily unavailable until an administrator starts redirecting client requests to the Secondary NameNode.

--------------------------------------------------------------------------------------------------
Question: 125
You are running a job that will process a single InputSplit on a cluster which has no other jobscurrently running. Each node has an equal number of open Map slots. On which node will Hadoopfirst attempt to run the Map task?

A.The node with the most memory

B.The node with the lowest system load

C.The node on which this InputSplit is stored

D.The node with the most free local disk space

--------------------------------------------------------------------------------------------------
Question: 126
How does the NameNode detect that a DataNode has failed?

A.The NameNode does not need to know that a DataNode has failed.

B.When the NameNode fails to receive periodic heartbeats from the DataNode, it considers the DataNode as failed.

C.The NameNode periodically pings the datanode. If the DataNode does not respond, the NameNode considers the DataNode as failed.

D.When HDFS starts up, the NameNode tries to communicate with the DataNode and considers the DataNode as failed if it does not respond.

--------------------------------------------------------------------------------------------------
Question: 127
The NameNode uses RAM for the following purpose:

A.To store the contents of files in HDFS.

B.To store filenames, list of blocks and other meta information.

C.To store the edits log that keeps track of changes in HDFS.

D.To manage distributed read and write locks on files in HDFS.

Explanation:The NameNode is the centerpiece of an HDFS file system. It keeps the directorytree of all files in the file system, and tracks where across the cluster the file data is kept. It doesnot store the data of these files itself. There is only One NameNode process run on any hadoopcluster. NameNode runs on its own JVM process. In a typical production cluster its run on aseparate machine. The NameNode is a Single Point of Failure for the HDFS Cluster. When theNameNode goes down, the file system goes offline. Client applications talk to the NameNodewhenever they wish to locate a file, or when they want to add/copy/move/delete a file. The

NameNode responds the successful requests by returning a list of relevant DataNode serverswhere the data livesReference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What is aNameNode? How many instances of NameNode run on a Hadoop Cluster?

--------------------------------------------------------------------------------------------------
Question: 128
What is a Writable?

A.Writable is an interface that all keys and values in MapReduce must implement. Classes implementing this interface must implement methods for serializing and deserializing themselves.

B.Writable is an abstract class that all keys and values in MapReduce must extend. Classes extending this abstract base class must implement methods for serializing and deserializing themselves

C.Writable is an interface that all keys, but not values, in MapReduce must implement. Classes implementing this interface must implement methods for serializing and deserializing themselves.

D.Writable is an abstract class that all keys, but not values, in MapReduce must extend. Classes extending this abstract base class must implement methods for serializing and deserializing themselves.

--------------------------------------------------------------------------------------------------
Question: 129
During the standard sort and shuffle phase of MapReduce, keys and values are passed toreducers. Which of the following is true?

A.Keys are presented to a reducer in sorted order; values for a given key are not sorted.

B.Keys are presented to a reducer in soiled order; values for a given key are sorted in ascending order.

C.Keys are presented to a reducer in random order; values for a given key are not sorted.

D.Keys are presented to a reducer in random order; values for a given key are sorted in ascending order.

--------------------------------------------------------------------------------------------------
Question: 130
What is the behavior of the default partitioner?

A.The default partitioner assigns key value pairs to reducers based on an internal random number generator.

B.The default partitioner implements a round robin strategy, shuffling the key value pairs to each reducer in turn. This ensures an even partition of the key space.

C.The default partitioner computes the hash of the key. Hash values between specific ranges are associated with different buckets, and each bucket is assigned to a specific reducer.

D.The default partitioner computes the hash of the key and divides that value modulo the number of reducers. The result determines the reducer assigned to process the key-value pair.

E.The default partitioner computes the hash of the value and takes the mod of that value with the number of reducers. The result determines the reducer assigned to process the key value pair.

--------------------------------------------------------------------------------------------------
Question: 131
Which statement best describes the data path of intermediate key-value pairs (i.e., output of themappers)?

A.Intermediate key-value pairs are written to HDFS. Reducers read the intermediate data from HDFS.

B.Intermediate key-value pairs are written to HDFS. Reducers copy the intermediate data to the local disks of the machines running the reduce tasks.

C.Intermediate key-value pairs are written to the local disks of the machines running the map tasks, and then copied to the machine running the reduce tasks.

D.Intermediate key-value pairs are written to the local disks of the machines running the map tasks, and are then copied to HDFS. Reducers read the intermediate data from HDFS.

--------------------------------------------------------------------------------------------------
Question: 132
If you run the word count MapReduce program with m mappers and r reducers, how many outputfiles will you get at the end of the job? And how many key-value pairs will there be in each file?Assume k is the number of unique words in the input files.

A.There will be r files, each with exactly k/r key-value pairs.

B.There will be r files, each with approximately k/m key-value pairs.

C.There will be r files, each with approximately k/r key-value pairs.

D.There will be m files, each with exactly k/m key value pairs.

E.There will be m files, each with approximately k/m key-value pairs.

--------------------------------------------------------------------------------------------------
Question: 133
You have a large dataset of key-value pairs, where the keys are strings, and the values areintegers. For each unique key, you want to identify the largest integer. In writing a MapReduceprogram to accomplish this, can you take advantage of a combiner?

A.No, a combiner would not be useful in this case.

B.Yes.

C.Yes, but the number of unique keys must be known in advance.

D.Yes, as long as all the keys fit into memory on each node.

E.Yes, as long as all the integer values that share the same key fit into memory on each node.

--------------------------------------------------------------------------------------------------
Question: 134
What happens in a MapReduce job when you set the number of reducers to zero?

A.No reducer executes, but the mappers generate no output.

B.No reducer executes, and the output of each mapper is written to a separate file in HDFS.

C.No reducer executes, but the outputs of all the mappers are gathered together and written to a single file in HDFS.

D.Setting the number of reducers to zero is invalid, and an exception is thrown.

--------------------------------------------------------------------------------------------------
Question: 135
Combiners Increase the efficiency of a MapReduce program because:

A.They provide a mechanism for different mappers to communicate with each Other, thereby reducing synchronization overhead.

B.They provide an optimization and reduce the total number of computations that are needed to execute an algorithm by a factor of n, where is the number of reducer.

C.They aggregate intermediate map output locally on each individual machine and therefore reduce the amount of data that needs to be shuffled across the network to the reducers.

D.They aggregate intermediate map output horn a small number of nearby (i.e., rack-local) machines and therefore reduce the amount of data that needs to be shuffled across the network to the reducers.

Explanation:Combiners are used to increase the efficiency of a MapReduce program. They areused to aggregate intermediate map output locally on individual mapper outputs. Combiners canhelp you reduce the amount of data that needs to be transferred across to the reducers. You canuse your reducer code as a combiner if the operation performed is commutative and associative.The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also,if required it may execute it more then 1 times. Therefore your MapReduce jobs should not

depend on the combiners execution.http://www.fromdev.com/2010/12/interview-questions-hadoop-mapreduce.html (question no. 12)

--------------------------------------------------------------------------------------------------
Question: 136
In a large MapReduce job with m mappers and r reducers, how many distinct copy operations willthere be in the sort/shuffle phase?

A.m

B.r

C.m+r (i.e., m plus r)

D.mxr (i.e., m multiplied by r)

E.mr (i.e., m to the power of r)

--------------------------------------------------------------------------------------------------
Question: 137
What happens in a MapReduce job when you set the number of reducers to one?

A.A single reducer gathers and processes all the output from all the mappers. The output is written in as many separate files as there are mappers.

B.A single reducer gathers and processes all the output from all the mappers. The output is written to a single file in HDFS.

C.Setting the number of reducers to one creates a processing bottleneck, and since the number of reducers as specified by the programmer is used as a reference value only, the MapReduce runtime provides a default setting for the number of reducers.

D.Setting the number of reducers to one is invalid, and an exception is thrown.

--------------------------------------------------------------------------------------------------
Question: 138
In the standard word count MapReduce algorithm, why might using a combiner reduce the overallJob running time?

A.Because combiners perform local aggregation of word counts, thereby allowing the mappers to process input data faster.

B.Because combiners perform local aggregation of word counts, thereby reducing the number of mappers that need to run.

C.Because combiners perform local aggregation of word counts, and then transfer that data to reducers without writing the intermediate data to disk.

D.Because combiners perform local aggregation of word counts, thereby reducing the number of key-value pairs that need to be snuff let across the network to the reducers.

--------------------------------------------------------------------------------------------------
Question: 139
Which two of the following are valid statements? (Choose two)

A.HDFS is optimized for storing a large number of files smaller than the HDFS block size.

B.HDFS has the Characteristic of supporting a “write once, read many” data access model.

C.HDFS is a distributed file system that replaces ext3 or ext4 on Linux nodes in a Hadoop cluster.

D.HDFS is a distributed file system that runs on top of native OS filesystems and is well suited to storage of very large data sets.

Explanation:B: HDFS is designed to support very large files. Applications that are compatiblewith HDFS are those that deal with large data sets. These applications write their data only once

but they read it one or more times and require these reads to be satisfied at streaming speeds.HDFS supports write-once-read-many semantics on files.D: * Hadoop Distributed File System: A distributed file system that provides high-throughput accessto application data.* DFS is designed to support very large files. Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers

--------------------------------------------------------------------------------------------------
Question: 140
Your cluster’s mapred-start.xml includes the following parameters&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;&lt;value&gt;4096&lt;/value&gt;&lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;&lt;value&gt;8192&lt;/value&gt;And any cluster’s yarn-site.xml includes the following parameters&lt;name&gt;yarn.nodemanager.vmen-pmen-ration&lt;/name&gt;&lt;value&gt;2.1&lt;/value&gt;What is the maximum amount of virtual memory allocated for each map task before YARN will killits Container?

A.4 GB

B.17.2 GB

C.8.9GB

D.8.2 GB

E.24.6 GB

--------------------------------------------------------------------------------------------------
Question: 141
Assuming you’re not running HDFS Federation, what is the maximum number of NameNodedaemons you should run on your cluster in order to avoid a “split-brain” scenario with yourNameNode when running HDFS High Availability (HA) using Quorum-based storage?

A.Two active NameNodes and two Standby NameNodes

B.One active NameNode and one Standby NameNode

C.Two active NameNodes and on Standby NameNode

D.Unlimited. HDFS High Availability (HA) is designed to overcome limitations on the number of NameNodes you can deploy

--------------------------------------------------------------------------------------------------
Question: 142
Table schemas in Hive are:

A.Stored as metadata on the NameNode

B.Stored along with the data in HDFS

C.Stored in the Metadata

D.Stored in ZooKeeper

--------------------------------------------------------------------------------------------------
Question: 143
For each YARN job, the Hadoop framework generates task log file. Where are Hadoop task logfiles stored?

A.Cached by the NodeManager managing the job containers, then written to a log directory on the NameNode

B.Cached in the YARN container running the task, then copied into HDFS on job completion

C.In HDFS, in the directory of the user who generates the job

D.On the local disk of the slave mode running the task

--------------------------------------------------------------------------------------------------
Question: 144
You have a cluster running with the fair Scheduler enabled. There are currently no jobs running onthe cluster, and you submit a job A, so that only job A is running on the cluster. A while later, yousubmit Job B. now Job A and Job B are running on the cluster at the same time. How will the FairScheduler handle these two jobs?

A.When Job B gets submitted, it will get assigned tasks, while job A continues to run with fewer tasks.

B.When Job B gets submitted, Job A has to finish first, before job B can gets scheduled.

C.When Job A gets submitted, it doesn’t consumes all the task slots.

D.When Job A gets submitted, it consumes all the task slots.

--------------------------------------------------------------------------------------------------
Question: 145
Each node in your Hadoop cluster, running YARN, has 64GB memory and 24 cores. Youryarn.site.xml has the following configuration:&lt;property&gt;&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;&lt;value&gt;32768&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;&lt;value&gt;12&lt;/value&gt;&lt;/property&gt;You want YARN to launch no more than 16 containers per node. What should you do?

A.Modify yarn-site.xml with the following property: &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt;

B.Modify yarn-sites.xml with the following property: &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;

C.Modify yarn-site.xml with the following property: &lt;name&gt;yarn.nodemanager.resource.cpu-vccores&lt;/name&gt;

D.No action is needed: YARN’s dynamic resource allocation automatically optimizes the node memory and cores

--------------------------------------------------------------------------------------------------
Question: 146
You want to node to only swap Hadoop daemon data from RAM to disk when absolutelynecessary. What should you do?

A.Delete the /dev/vmswap file on the node

B.Delete the /etc/swap file on the node

C.Set the ram.swap parameter to 0 in core-site.xml

D.Set vm.swapfile file on the node

E.Delete the /swapfile file on the node

--------------------------------------------------------------------------------------------------
Question: 147
You are configuring your cluster to run HDFS and MapReducer v2 (MRv2) on YARN. Which twodaemons needs to be installed on your cluster’s master nodes?

A.HMaster

B.ResourceManager

C.TaskManager

D.JobTracker

E.NameNode

F.DataNode

--------------------------------------------------------------------------------------------------
Question: 148
You observed that the number of spilled records from Map tasks far exceeds the number of mapoutput records. Your child heap size is 1GB and your io.sort.mb value is set to 1000MB. Howwould you tune your io.sort.mb value to achieve maximum memory to disk I/O ratio?

A.For a 1GB child heap size an io.sort.mb of 128 MB will always maximize memory to disk I/O

B.Increase the io.sort.mb to 1GB

C.Decrease the io.sort.mb value to 0

D.Tune the io.sort.mb value until you observe that the number of spilled records equals (or is as close to equals) the number of map output records.

--------------------------------------------------------------------------------------------------
Question: 149
You are running a Hadoop cluster with a NameNode on host mynamenode, a secondaryNameNode on host mysecondarynamenode and several DataNodes.Which best describes how you determine when the last checkpoint happened?

A.Execute hdfs namenode –report on the command line and look at the Last Checkpoint information

B.Execute hdfs dfsadmin –saveNamespace on the command line which returns to you the last checkpoint value in fstime file

C.Connect to the web UI of the Secondary NameNode (http://mysecondary:50090/) and look at the “Last Checkpoint” information

D.Connect to the web UI of the NameNode (http://mynamenode:50070) and look at the “Last Checkpoint” information

--------------------------------------------------------------------------------------------------
Question: 150
On a cluster running MapReduce v2 (MRv2) on YARN, a MapReduce job is given a directory of 10plain text files as its input directory. Each file is made up of 3 HDFS blocks. How many Mapperswill run?

A.We cannot say; the number of Mappers is determined by the ResourceManager

B.We cannot say; the number of Mappers is determined by the developer

C.30

D.3

E.10

F.We cannot say; the number of mappers is determined by the ApplicationMaster

--------------------------------------------------------------------------------------------------
Question: 151
You’re upgrading a Hadoop cluster from HDFS and MapReduce version 1 (MRv1) to one runningHDFS and MapReduce version 2 (MRv2) on YARN. You want to set and enforce version 1 (MRv1)to one running HDFS and MapReduce version 2 (MRv2) on YARN. You want to set and enforce ablock size of 128MB for all new files written to the cluster after upgrade. What should you do?

A.You cannot enforce this, since client code can always override this value

B.Set dfs.block.size to 128M on all the worker nodes, on all client machines, and on the NameNode, and set the parameter to final

C.Set dfs.block.size to 128 M on all the worker nodes and client machines, and set the parameter to final. You do not need to set this value on the NameNode

D.Set dfs.block.size to 134217728 on all the worker nodes, on all client machines, and on the NameNode, and set the parameter to final

E.Set dfs.block.size to 134217728 on all the worker nodes and client machines, and set the parameter to final. You do not need to set this value on the NameNode

--------------------------------------------------------------------------------------------------
Question: 152
Your cluster has the following characteristics:A rack aware topology is configured and onReplication is set to 3Cluster block size is set to 64MBWhich describes the file read process when a client application connects into the cluster andrequests a 50MB file?

A.The client queries the NameNode for the locations of the block, and reads all three copies. The first copy to complete transfer to the client is the one the client reads as part of hadoop’s speculative execution framework.

B.The client queries the NameNode for the locations of the block, and reads from the first location in the list it receives.

C.The client queries the NameNode for the locations of the block, and reads from a random location in the list it receives to eliminate network I/O loads by balancing which nodes it retrieves data from any given time.

D.The client queries the NameNode which retrieves the block from the nearest DataNode to the client then passes that block back to the client.

--------------------------------------------------------------------------------------------------
Question: 153
Your Hadoop cluster is configuring with HDFS and MapReduce version 2 (MRv2) on YARN. Canyou configure a worker node to run a NodeManager daemon but not a DataNode daemon and stillhave a functional cluster?

A.Yes. The daemon will receive data from the NameNode to run Map tasks

B.Yes. The daemon will get data from another (non-local) DataNode to run Map tasks

C.Yes. The daemon will receive Map tasks only

D.Yes. The daemon will receive Reducer tasks only

--------------------------------------------------------------------------------------------------
Question: 154
You have A 20 node Hadoop cluster, with 18 slave nodes and 2 master nodes running HDFS High

Availability (HA). You want to minimize the chance of data loss in your cluster. What should youdo?

A.Add another master node to increase the number of nodes running the JournalNode which increases the number of machines available to HA to create a quorum

B.Set an HDFS replication factor that provides data redundancy, protecting against node failure

C.Run a Secondary NameNode on a different master from the NameNode in order to provide automatic recovery from a NameNode failure.

D.Run the ResourceManager on a different master from the NameNode in order to load-share HDFS metadata processing

E.Configure the cluster’s disk drives with an appropriate fault tolerant RAID level

--------------------------------------------------------------------------------------------------
Question: 155
You are running  Hadoop cluster with all monitoring facilities properly configured.Which scenario will go undeselected?

A.HDFS is almost full

B.The NameNode goes down

C.A DataNode is disconnected from the cluster

D.Map or reduce tasks that are stuck in an infinite loop

E.MapReduce jobs are causing excessive memory swaps

--------------------------------------------------------------------------------------------------
Question: 156
You decide to create a cluster which runs HDFS in High Availability mode with automatic failover,using Quorum Storage. What is the purpose of ZooKeeper in such a configuration?

A.It only keeps track of which NameNode is Active at any given time

B.It monitors an NFS mount point and reports if the mount point disappears

C.It both keeps track of which NameNode is Active at any given time, and manages the Edits file. Which is a log of changes to the HDFS filesystem

D.If only manages the Edits file, which is log of changes to the HDFS filesystem

E.Clients connect to ZooKeeper to determine which NameNode is Active

--------------------------------------------------------------------------------------------------
Question: 157
Choose three reasons why should you run the HDFS balancer periodically?

A.To ensure that there is capacity in HDFS for additional data

B.To ensure that all blocks in the cluster are 128MB in size

C.To help HDFS deliver consistent performance under heavy loads

D.To ensure that there is consistent disk utilization across the DataNodes

E.To improve data locality MapReduce

--------------------------------------------------------------------------------------------------
Question: 158
Your cluster implements HDFS High Availability (HA). Your two NameNodes are named nn01 andnn02. What occurs when you execute the command: hdfs haadmin –failover nn01 nn02?

A.nn02 is fenced, and nn01 becomes the active NameNode

B.nn01 is fenced, and nn02 becomes the active NameNode

C.nn01 becomes the standby NameNode and nn02 becomes the active NameNode

D.nn02 becomes the standby NameNode and nn01 becomes the active NameNode

Explanation:

failover – initiate a failover between two NameNodesThis subcommand causes a failover from the first provided NameNode to the second. If the firstNameNode is in the Standby state, this command simply transitions the second to the Active statewithout error. If the first NameNode is in the Active state, an attempt will be made to gracefully

transition it to the Standby state. If this fails, the fencing methods (as configured bydfs.ha.fencing.methods) will be attempted in order until one of the methods succeeds. Only afterthis process will the second NameNode be transitioned to the Active state. If no fencing methodsucceeds, the second NameNode will not be transitioned to the Active state, and an error will bereturned.

--------------------------------------------------------------------------------------------------
Question: 159
You have a Hadoop cluster HDFS, and a gateway machine external to the cluster from whichclients submit jobs. What do you need to do in order to run Impala on the cluster and submit jobsfrom the command line of the gateway machine?

A.Install the impalad daemon statestored daemon, and daemon on each machine in the cluster, and the impala shell on your gateway machine

B.Install the impalad daemon, the statestored daemon, the catalogd daemon, and the impala shell on your gateway machine

C.Install the impalad daemon and the impala shell on your gateway machine, and the statestored daemon and catalogd daemon on one of the nodes in the cluster

D.Install the impalad daemon on each machine in the cluster, the statestored daemon and catalogd daemon on one machine in the cluster, and the impala shell on your gateway machine

E.Install the impalad daemon, statestored daemon, and catalogd daemon on each machine in the cluster and on the gateway node

--------------------------------------------------------------------------------------------------
Question: 160
You are planning a Hadoop cluster and considering implementing 10 Gigabit Ethernet as thenetwork fabric. Which workloads benefit the most from faster network fabric?

A.When your workload generates a large amount of output data, significantly larger than the amount of intermediate data

B.When your workload consumes a large amount of input data, relative to the entire capacity if HDFS

C.When your workload consists of processor-intensive tasks

D.When your workload generates a large amount of intermediate data, on the order of the input data itself

--------------------------------------------------------------------------------------------------
Question: 161
Your cluster is running MapReduce version 2 (MRv2) on YARN. Your ResourceManager isconfigured to use the FairScheduler. Now you want to configure your scheduler such that a newuser on the cluster can submit jobs into their own queue application submission. Whichconfiguration should you set?

A.You can specify new queue name when user submits a job and new queue can be created dynamically if the property yarn.scheduler.fair.allow-undecleared-pools = true

B.Yarn.scheduler.fair.user.fair-as-default-queue = false and yarn.scheduler.fair.allowundecleared-pools = true

C.You can specify new queue name when user submits a job and new queue can be created dynamically if yarn .schedule.fair.user-as-default-queue = false

D.You can specify new queue name per application in allocations.xml file and have new jobs automatically assigned to the application queue

--------------------------------------------------------------------------------------------------
Question: 162
slave node in your cluster has 4 TB hard drives installed (4 x 2TB). The DataNode is configuredto store HDFS blocks on all disks. You set the value of the dfs.datanode.du.reserved parameter to100 GB. How does this alter HDFS block storage?

A.25GB on each hard drive may not be used to store HDFS blocks

B.100GB on each hard drive may not be used to store HDFS blocks

C.All hard drives may be used to store HDFS blocks as long as at least 100 GB in total is available on the node

D.A maximum if 100 GB on each hard drive may be used to store HDFS blocks

--------------------------------------------------------------------------------------------------
Question: 163
What two processes must you do if you are running a Hadoop cluster with a single NameNodeand six DataNodes, and you want to change a configuration parameter so that it affects all sixDataNodes.

A.You must modify the configuration files on the NameNode only. DataNodes read their configuration from the master nodes

B.You must modify the configuration files on each of the six SataNodes machines

C.You don’t need to restart any daemon, as they will pick up changes automatically

D.You must restart the NameNode daemon to apply the changes to the cluster

E.You must restart all six DatNode daemon to apply the changes to the cluster

--------------------------------------------------------------------------------------------------
Question: 164
You have installed a cluster HDFS and MapReduce version 2 (MRv2) on YARN. You have nodfs.hosts entry(ies) in your hdfs-site.xml configuration file. You configure a new worker node bysetting fs.default.name in its configuration files to point to the NameNode on your cluster, and youstart the DataNode daemon on that worker node. What do you have to do on the cluster to allowthe worker node to join, and start sorting HDFS blocks?

A.Without creating a dfs.hosts file or making any entries, run the commands hadoop.dfsadminrefreshModes on the NameNode

B.Restart the NameNode

C.Creating a dfs.hosts file on the NameNode, add the worker Node’s name to it, then issue the command hadoop dfsadmin –refresh Nodes = on the Namenode

D.Nothing; the worker node will automatically join the cluster when NameNode daemon is started

--------------------------------------------------------------------------------------------------
Question: 165
You use the hadoop fs –put command to add a file “sales.txt” to HDFS. This file is small enoughthat it fits into a single block, which is replicated to three nodes in your cluster (with a replicationfactor of 3). One of the nodes holding this file (a single block) fails. How will the cluster handle thereplication of file in this situation?

A.The file will remain under-replicated until the administrator brings that node back online

B.The cluster will re-replicate the file the next time the system administrator reboots the NameNode daemon (as long as the file’s replication factor doesn’t fall below)

C.This will be immediately re-replicated and all other HDFS operations on the cluster will halt until the cluster’s replication values are resorted

D.The file will be re-replicated automatically after the NameNode determines it is under-replicated based on the block reports it receives from the NameNodes

--------------------------------------------------------------------------------------------------
Question: 166
Given:<a href="http://cdn.aiotestking.com/wp-content/uploads/cca-500/2.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/cca-500/2.jpg"/></a>

You want to clean up this list by removing jobs where the State is KILLED. What command you enter?

A.Yarn application –refreshJobHistory

B.Yarn application –kill application_1374638600275_0109

C.Yarn rmadmin –refreshQueue

D.Yarn rmadmin –kill application_1374638600275_0109

--------------------------------------------------------------------------------------------------
Question: 167
Assume you have a file named foo.txt in your local directory. You issue the following threecommands:Hadoop fs –mkdir inputHadoop fs –put foo.txt input/foo.txtHadoop fs –put foo.txt inputWhat happens when you issue the third command?

A.The write succeeds, overwriting foo.txt in HDFS with no warning

B.The file is uploaded and stored as a plain file named input

C.You get a warning that foo.txt is being overwritten

D.You get an error message telling you that foo.txt already exists, and asking you if you would like to overwrite it.

E.You get a error message telling you that foo.txt already exists. The file is not written to HDFS

F.You get an error message telling you that input is not a directory

G.The write silently fails

--------------------------------------------------------------------------------------------------
Question: 168
You are configuring a server running HDFS, MapReduce version 2 (MRv2) on YARN runningLinux. How must you format underlying file system of each DataNode?

A.They must be formatted as HDFS

B.They must be formatted as either ext3 or ext4

C.They may be formatted in any Linux file system

D.They must not be formatted – – HDFS will format the file system automatically

--------------------------------------------------------------------------------------------------
Question: 169
You are migrating a cluster from MApReduce version 1 (MRv1) to MapReduce version 2 (MRv2)on YARN. You want to maintain your MRv1 TaskTracker slot capacities when you migrate. Whatshould you do/

A.Configure yarn.applicationmaster.resource.memory-mb and yarn.applicationmaster.resource.cpu-vcores so that ApplicationMaster container allocations match the capacity you require.

B.You don’t need to configure or balance these properties in YARN as YARN dynamically balances resource management capabilities on your cluster

C.Configure mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum ub yarn-site.xml to match your cluster’s capacity set by the yarn-scheduler.minimum-allocation

D.Configure yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpuvcores to match the capacity you require under YARN for each NodeManager

--------------------------------------------------------------------------------------------------
Question: 170
You are working on a project where you need to chain together MapReduce, Pig jobs. You alsoneed the ability to use forks, decision points, and path joins. Which ecosystem project should you

use to perform these actions?

A.Oozie

B.ZooKeeper

C.HBase

D.Sqoop

E.HUE

--------------------------------------------------------------------------------------------------
Question: 171
Which process instantiates user code, and executes map and reduce tasks on a cluster runningMapReduce v2 (MRv2) on YARN?

A.NodeManager

B.ApplicationMaster

C.TaskTracker

D.JobTracker

E.NameNode

F.DataNode

G.ResourceManager

--------------------------------------------------------------------------------------------------
Question: 172
Cluster Summary:45 files and directories, 12 blocks = 57 total. Heap size is 15.31 MB/193.38MB(7%)<a href="http://cdn.aiotestking.com/wp-content/uploads/cca-500/1.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/cca-500/1.jpg"/></a>

Refer to the above screenshot.You configure a Hadoop cluster with seven DataNodes and on of your monitoring UIs displays thedetails shown in the exhibit.What does the this tell you?

A.The DataNode JVM on one host is not active

B.Because your under-replicated blocks count matches the Live Nodes, one node is dead, and your DFS Used % equals 0%, you can’t be certain that your cluster has all the data you’ve written it.

C.Your cluster has lost all HDFS data which had bocks stored on the dead DatNode

D.The HDFS cluster is in safe mode

--------------------------------------------------------------------------------------------------
Question: 173
Which two features does Kerberos security add to a Hadoop cluster?

A.User authentication on all remote procedure calls (RPCs)

B.Encryption for data during transfer between the Mappers and Reducers

C.Encryption for data on disk (“at rest”)

D.Authentication for user access to the cluster against a central server

E.Root access to the cluster for users hdfs and mapred but non-root access for clients

--------------------------------------------------------------------------------------------------
Question: 174
Assuming a cluster running HDFS, MapReduce version 2 (MRv2) on YARN with all settings attheir default, what do you need to do when adding a new slave node to cluster?

A.Nothing, other than ensuring that the DNS (or/etc/hosts files on all machines) contains any entry for the new node.

B.Restart the NameNode and ResourceManager daemons and resubmit any running jobs.

C.Add a new entry to /etc/nodes on the NameNode host.

D.Restart the NameNode of dfs.number.of.nodes in hdfs-site.xml

--------------------------------------------------------------------------------------------------
Question: 175
Which YARN daemon or service negotiations map and reduce Containers from the Scheduler,tracking their status and monitoring progress?

A.NodeManager

B.ApplicationMaster

C.ApplicationManager

D.ResourceManager

--------------------------------------------------------------------------------------------------
Question: 176
During the execution of a MapReduce v2 (MRv2) job on YARN, where does the Mapper place theintermediate data of each Map Task?

A.The Mapper stores the intermediate data on the node running the Job’s ApplicationMaster so that it is available to YARN ShuffleService before the data is presented to the Reducer

B.The Mapper stores the intermediate data in HDFS on the node where the Map tasks ran in the HDFS /usercache/&amp;(user)/apache/application_&amp;(appid) directory for the user who ran the job

C.The Mapper transfers the intermediate data immediately to the reducers as it is generated by the Map Task

D.YARN holds the intermediate data in the NodeManager’s memory (a container) until it is transferred to the Reducer

E.The Mapper stores the intermediate data on the underlying filesystem of the local disk in the directories yarn.nodemanager.locak-DIFS

--------------------------------------------------------------------------------------------------
Question: 177
You suspect that your NameNode is incorrectly configured, and is swapping memory to disk.Which Linux commands help you to identify whether swapping is occurring?

A.free

B.df

C.memcat

D.top

E.jps

F.vmstat

G.swapinfo

--------------------------------------------------------------------------------------------------
Question: 178
On a cluster running CDH 5.0 or above, you use the hadoop fs –put command to write a 300MBfile into a previously empty directory using an HDFS block size of 64 MB. Just after this commandhas finished writing 200 MB of this file, what would another use see when they look in directory?

A.The directory will appear to be empty until the entire file write is completed on the cluster

B.They will see the file with a ._COPYING_ extension on its name. If they view the file, they will see contents of the file up to the last completed block (as each 64MB block is written, that block becomes available)

C.They will see the file with a ._COPYING_ extension on its name. If they attempt to view the file, they will get a ConcurrentFileAccessException until the entire file write is completed on the cluster

D.They will see the file with its original name. If they attempt to view the file, they will get a ConcurrentFileAccessException until the entire file write is completed on the cluster

--------------------------------------------------------------------------------------------------
Question: 179
Which command does Hadoop offer to discover missing or corrupt HDFS data?

A.Hdfs fs –du

B.Hdfs fsck

C.Dskchk

D.The map-only checksum

E.Hadoop does not provide any tools to discover missing or corrupt data; there is not need because three replicas are kept for each data block

--------------------------------------------------------------------------------------------------
Question: 180
What does CDH packaging do on install to facilitate Kerberos security setup?

A.Automatically configures permissions for log files at &amp;MAPRED_LOG_DIR/userlogs

B.Creates users for hdfs and mapreduce to facilitate role assignment

C.Creates directories for temp, hdfs, and mapreduce with the correct permissions

D.Creates a set of pre-configured Kerberos keytab files and their permissions

E.Creates and configures your kdc with default cluster values

--------------------------------------------------------------------------------------------------
Question: 181
You want to understand more about how users browse your public website. For example, youwant to know which pages they visit prior to placing an order. You have a server farm of 200 webservers hosting your website. Which is the most efficient process to gather these web serveracross logs into your Hadoop cluster analysis?

A.Sample the web server logs web servers and copy them into HDFS using curl

B.Ingest the server web logs into HDFS using Flume

C.Channel these clickstreams into Hadoop using Hadoop Streaming

D.Import all user clicks from your OLTP databases into Hadoop using Sqoop

E.Write a MapReeeduce job with the web servers for mappers and the Hadoop cluster nodes for reducers

--------------------------------------------------------------------------------------------------
Question: 182
Which three basic configuration parameters must you set to migrate your cluster from MapReduce1 (MRv1) to MapReduce V2 (MRv2)?

A.Configure the NodeManager to enable MapReduce services on YARN by setting the following property in yarn-site.xml: &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt; &lt;value&gt;your_nodeManager_shuffle&lt;/value&gt;

B.Configure the NodeManager hostname and enable node services on YARN by setting the following property in yarn-site.xml: &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt; &lt;value&gt;your_nodeManager_hostname&lt;/value&gt;

C.Configure a default scheduler to run on YARN by setting the following property in mapredsite.xml: &lt;name&gt;mapreduce.jobtracker.taskScheduler&lt;/name&gt; &lt;Value&gt;org.apache.hadoop.mapred.JobQueueTaskScheduler&lt;/value&gt;

D.Configure the number of map tasks per jon YARN by setting the following property in mapred: &lt;name&gt;mapreduce.job.maps&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;

E.Configure the ResourceManager hostname and enable node services on YARN by setting the following property in yarn-site.xml: &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;your_resourceManager_hostname&lt;/value&gt;

F.Configure MapReduce as a Framework running on YARN by setting the following property in mapred-site.xml: &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;

--------------------------------------------------------------------------------------------------
Question: 183
You need to analyze 60,000,000 images stored in JPEG format, each of which is approximately 25KB. Because you Hadoop cluster isn’t optimized for storing and processing many small files, youdecide to do the following actions:1. Group the individual images into a set of larger files2. Use the set of larger files as input for a MapReduce job that processes them directly with pythonusing Hadoop streaming.Which data serialization system gives the flexibility to do this?

A.CSV

B.XML

C.HTML

D.Avro

E.SequenceFiles

F.JSON

--------------------------------------------------------------------------------------------------
Question: 184
Identify two features/issues that YARN is designated to address:

A.Standardize on a single MapReduce API

B.Single point of failure in the NameNode

C.Reduce complexity of the MapReduce APIs

D.Resource pressure on the JobTracker

E.Ability to run framework other than MapReduce, such as MPI

F.HDFS latency

--------------------------------------------------------------------------------------------------
Question: 185
Which YARN daemon or service monitors a Controller’s per-application resource using (e.g.,memory CPU)?

A.ApplicationMaster

B.NodeManager

C.ApplicationManagerService

D.ResourceManager

--------------------------------------------------------------------------------------------------
Question: 186
Which is the default scheduler in YARN?

A.YARN doesn’t configure a default scheduler, you must first assign an appropriate scheduler class in yarn-site.xml

B.Capacity Scheduler

C.Fair Scheduler

D.FIFO Scheduler

--------------------------------------------------------------------------------------------------
Question: 187
Which YARN process run as “container 0” of a submitted job and is responsible for resourceqrequests?

A.ApplicationManager

B.JobTracker

C.ApplicationMaster

D.JobHistoryServer

E.ResoureManager

F.NodeManager

--------------------------------------------------------------------------------------------------
Question: 188
Which scheduler would you deploy to ensure that your cluster allows short jobs to finish within areasonable time without starting long-running jobs?

A.Complexity Fair Scheduler (CFS)

B.Capacity Scheduler

C.Fair Scheduler

D.FIFO Scheduler

--------------------------------------------------------------------------------------------------
Question: 189
Your cluster is configured with HDFS and MapReduce version 2 (MRv2) on YARN. What is theresult when you execute: hadoop jar SampleJar MyClass on a client machine?

A.SampleJar.Jar is sent to the ApplicationMaster which allocates a container for SampleJar.Jar

B.Sample.jar is placed in a temporary directory in HDFS

C.SampleJar.jar is sent directly to the ResourceManager

D.SampleJar.jar is serialized into an XML file which is submitted to the ApplicatoionMaster

--------------------------------------------------------------------------------------------------
Question: 190
You have one primary HMaster and one standby. Your primary HMaster Falls fails and your clientapplication needs to make a metadata change. Which of the following is the effect on your clientapplication?

A.The client will query ZooKeeper to find the location of the new HMaster and complete the metadata change.

B.The client will make the metadata change regardless of the slate of the HMaster.

C.The new HMaster will notify the client and complete the metadata change.

D.The client application will fail with a runtime error.

--------------------------------------------------------------------------------------------------
Question: 191
You have an “Employees” table in HBase. The Row Keys are the employees’ IDs. You would liketo retrieve all employees who have an employee ID between ‘user_100’ and ‘user_110’. The shellcommand you would use to complete this is:

A.scan ‘Employees’, {STARTROW =&gt; ‘user_100’, STOPROW =&gt; ‘user_111’}

B.get ‘Employees’, {STARTROW =&gt; ‘user_100’, STOPROW =&gt; ‘user_110’}

C.scan ‘Employees’, {STARTROW =&gt; ‘user_100’, SLIMIT =&gt; 10}

D.scan ‘Employees’, {STARTROW =&gt; ‘user_100’, STOPROW =&gt; ‘user_110’}

--------------------------------------------------------------------------------------------------
Question: 192
The cells in a given row have versions that range from 1000 to 2000. You execute a deletespecifying the value 3000 for the version. What is the outcome?

A.The delete fails with an error.

B.Only cells equal to the Specified version are deleted.

C.The entire row is deleted.

D.Nothing in the row is deleted.

--------------------------------------------------------------------------------------------------
Question: 193
You have an average key-value pair size of 100 bytes. Your primary access is random needs onthe table. Which of the following actions will speed up random reading performance on yourcluster?

A.Turn off WAL on puts

B.Increase the number of versions kept

C.Decrease the block size

D.Increase the block size

--------------------------------------------------------------------------------------------------
Question: 194
You have a table with the following rowkeys:r1, r2, r3, r10, r15, r20, r25, r30, r35In which order will these rows be retrieved from a scan?

A.r35, r30, r3, r25, r20, r2, r15, r10, r1

B.r1, r2, r3, r10, r15, r20, r25, r30, r35

C.r1, r10, r15, r2, r20, r25, r3, r30, r35

D.r35, r30, r25, r20, r15, r10, r3, r2, r1

--------------------------------------------------------------------------------------------------
Question: 195
Data is written to the HLog in which of the following orders?

A.In order of writes

B.In order of writes, separated by region

C.Ascending first by region and second by row key

D.Descending first by region and second by row key

Explanation:



--------------------------------------------------------------------------------------------------
Question: 196
You have two standbys and one primary HMaster. Your primary HMaster fails. Which of theremaining HMasters becomes the new primary?

A.Whichever HMaster first responds to ZooKeeper

B.Whichever HMaster ZooKeeper randomly selects

C.Whichever HMaster creates the znode first

D.Whichever HMaster has the lower IP address

--------------------------------------------------------------------------------------------------
Question: 197
Under default settings, which feature of HBase ensures that data won’t be lost in the event of aRegionServer failure?

A.All HBase activity is written to the WAL, which is stored in HDFS

B.All operations are logged on the HMaster.

C.HBase is ACID compliant, which guarantees that it is Durable.

D.Data is stored on the local filesystem of the RegionServer.

Explanation: HBase data updates are stored in a place in memory called memstore for fast write.In the event of a region server failure, the contents of the memstore are lost because they havenot been saved to disk yet. To prevent data loss in such a scenario, the updates are persisted in aWAL file before they are stored in the memstore. In the event of a region server failure, the lostcontents in the memstore can be regenerated by replaying the updates (also called edits) from theWAL file.Reference: HBase Log Splitting

http://tm.durusau.net/?p=27674(See ‘From the post’ second paragraph)

--------------------------------------------------------------------------------------------------
Question: 198
You need to create a “WebLogs” table in HBase. The table will consist of a single Column Familycalled “Errors” and two column qualifiers, “IP” and “URL”. The shell command you should use tocreate the table is:

A.create ‘WebLogs’, {NAME =&gt; ‘Errors:IP’, NAME =&gt;’Errors:URL’}

B.create ‘WebLogs’, ‘Errors’ {NAME =&gt; ‘IP’, NAME =&gt; ‘URL’}

C.create ‘WebLogs’, ‘Errors:IP’, ‘Errors:URL’

D.create ‘WebLogs’, ‘Errors’

Explanation: Columns in Apache HBase are grouped into column families. All column membersof a column family have the same prefix. For example, the columns courses:history andcourses:math are both members of the courses column family. The colon character (:) delimits thecolumn family from the column qualifier . The column family prefix must be composed of printablecharacters. The qualifying tail, the column family qualifier, can be made of any arbitrary bytes.Column families must be declared up front at schema definition time whereas columns do notneed to be defined at schema time but can be conjured on the fly while the table is up an running.

Physically, all column family members are stored together on the filesystem. Because tunings andstorage specifications are done at the column family level, it is advised that all column familymembers have the same general access pattern and size characteristics.

--------------------------------------------------------------------------------------------------
Question: 199
Given that following is your entire dataset:<a href="http://cdn.aiotestking.com/wp-content/uploads/ccb-400/1.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ccb-400/1.jpg"/></a>

How many sets of physical files will be read during a scan of the entire dataset immediatelyfollowing a major compaction?

A.Two

B.One

C.Three

D.Four

Explanation: There are two columns families (Managers and Skills) so there will be two files.Note:

* Physically, all column family members are stored together on the filesystem. Because tuningsand storage specifications are done at the column family level, it is advised that all column familymembers have the same general access pattern and size characteristics.* HBase currently does not do well with anything above two or three column families so keep thenumber of column families in your schema low. Currently, flushing and compactions are done on aper Region basis so if one column family is carrying the bulk of the data bringing on flushes, theadjacent families will also be flushed though the amount of data they carry is small. When manycolumn families the flushing and compaction interaction can make for a bunch of needless i/oloading (To be addressed by changing flushing and compaction to work on a per column familybasis).* When changes are made to either Tables or ColumnFamilies (e.g., region size, block size), thesechanges take effect the next time there is a major compaction and the StoreFiles get re-written.* StoreFiles are composed of blocks. The blocksize is configured on a per-ColumnFamily basis.Compression happens at the block level within StoreFiles.

--------------------------------------------------------------------------------------------------
Question: 200
You want to do mostly full table scans on your data. In order to improve performance you increase

your block size. Why does this improve your scan performance?

A.It does not. Increasing block size does not improve scan performance.

B.It does not. Increasing block size means that fewer blocks fit into your block cache. This requires HBase to read each block from disk rather than cache for each scan, thereby decreasing scan performance.

C.Increasing block size requires HBase to read from disk fewer times, thereby increasing scan performance.

D.Increasing block size means fewer block indexes that need to be read from disk, thereby increasing scan performance.

--------------------------------------------------------------------------------------------------
Question: 201
Your data load application is maintaining a custom versioning scheme (not using the timestamp asthe version number). You accidentally executed three writes to a given cell all with the sameversion during which time no flushes have occurred. Which of the three data writes will dBasemaintain?

A.None of the writes to cell

B.The last write to cell

C.The first write to cell

D.All of the writes to cell

--------------------------------------------------------------------------------------------------
Question: 202
Your client connects to HBase for the first time to read a row user_1234 located in a table Users.What process does your client use to find the correct RegionServer to which it should send therequest?

A.The client looks up the location of ROOT, in which it looks up the location of META, in which it looks up the location of the correct Users region.

B.The client looks up the location of the master, in which it looks up the location of META, in which it looks up the location of the correct Users region.

C.The client looks up the location of ROOT in which it looks up the location of the correct Users region.

D.The client queries the master to find the location of the Users table.

Explanation: *The general flow is that a new client contacts the Zookeeper quorum (a separatecluster of Zookeeper nodes) first to find a particular row key. It does so by retrieving the servername (i.e. host name) that hosts the -ROOT- region from Zookeeper. With that information it can

query that server to get the server that hosts the .META. table. Both of these two details arecached and only looked up once. Lastly it can query the .META. server and retrieve the server thathas the row the client is looking for.*The HBase client HTable is responsible for finding RegionServers that are serving the particularrow range of interest. It does this by querying the .META. and -ROOT- catalog tables.After locatingthe required region(s), the client directly contacts the RegionServer serving that region (i.e., it doesnot go through the master) and issues the read or write request. This information is cached in theclient so that subsequent requests need not go through the lookup process. Should a region bereassigned either by the master load balancer or because a RegionServer has died, the client willrequery the catalog tables to determine the new location of the user region.Reference:HBase Architecture 101 – Storage

--------------------------------------------------------------------------------------------------
Question: 203
You have a table where keys range from “A” to “Z”, and you want to scan from “D” to “H.” Which ofthe following is true?

A.A MultiGet must be issued for rows D, E, F, G, H.

B.The scan class supports ranges via the stop and start rows.

C.All scans are full table scans, the client must implement filtering.

D.In order to range scan, raw scan mode must be enabled.

--------------------------------------------------------------------------------------------------
Question: 204
From within an HBase application, you want to retrieve two versions of a row, if they exist. Whereyour application should configure the maximum number of versions to be retrieved?

A.HTableDescriptor

B.HTable

C.Get or scan

D.HColumnDescriptor

--------------------------------------------------------------------------------------------------
Question: 205
You have two tables in an existing RDBMS. One table contains order information (item, quantity,price, etc.) and the other contains store information (address, phone, manager, etc).  These twotables are not often accessed simultaneously. You would like to move this data into HBase. Howwould you design the schema?

A.Create two tables each with a single column family

B.Create a single table with one column family

C.Create a single table with two column families

D.Create two tables each with multiple column families

--------------------------------------------------------------------------------------------------
Question: 206
Your client application needs to write a row to a region that has, recently split. Where will the rowbe written?

A.One of the daughter regions

B.The original region

C.The .META. table

D.The HMaster

Explanation:

Note:*With a roughly uniform data distribution and growth, eventually all the regions in the table willneed to be split at the same time. Immediately following a split, compactions will run on thedaughter regions to rewrite their data into separate files. This causes a large amount of disk I/Oand network traffic.*Splits run unaided on the RegionServer; i.e. the Master does not participate. The RegionServersplits a region, offlines the split region and then adds the daughter regions to META, opensdaughters on the parent’s hosting RegionServer and then reports the split to the Master.

--------------------------------------------------------------------------------------------------
Question: 207
Your client application connects to HBase for the first time to perform a write. Which of thefollowing sequences will it traverse to find the region serving the row range of interest?

A.ZooKeeper -&gt; RegionServer -&gt; Region

B.ZooKeeper -&gt;.META. -&gt; RegionServer -&gt; Region

C.ZooKeeper -&gt; ROOT -&gt; .META. -&gt; RegionServer -&gt; Region

D.ZooKeeper -&gt; HMaster -&gt; -ROOT- -&gt; .META. -&gt; RegionServer -&gt; Region

Explanation: The general flow is that a new client contacts the Zookeeper quorum (a separatecluster of Zookeeper nodes) first to find a particular row key. It does so by retrieving the servername (i.e. host name) that hosts the -ROOT- region from Zookeeper. With that information it canquery that server to get the server that hosts the .META. table. Both of these two details arecached and only looked up once. Lastly it can query the .META. server and retrieve the server thathas the row the client is looking for.

Note:*The .META. table keeps a list of all regions in the system. The .META. table structure is asfollows:Key:Region key of the format ([table],[region start key],[region id])Values:info:regioninfo (serialized HRegionInfo instance for this region)info:server (server:port of the RegionServer containing this region)info:serverstartcode (start-time of the RegionServer process containing this region)Reference:HBase Architecture 101 – Storage

--------------------------------------------------------------------------------------------------
Question: 208
Which of the following configuration values determines automated splitting?

A.hbase.hregion.majorcompaction

B.hbase.hregion.flush.size

C.hbase.balancer.period

D.hbase.hregion.max.filesize

--------------------------------------------------------------------------------------------------
Question: 209
You have a key-value pair size of l00 bytes. You increase your HFile block size from its default64k. What results from this change?

A.scan throughput increases and random-access latency decreases

B.scan throughput decreases and random-access latency increases

C.scan throughput decreases and random-access latency decreases

D.scan throughput increases and random-access latency increases

--------------------------------------------------------------------------------------------------
Question: 210
You have two tables in existing RDBMS. One contains information about the products you sell(name, size, color, etc.) The other contains images of the products in JPEG format. These tables

are frequently joined in queries to your database. You would like to move this data into HBase.How would you design the schema?

A.Create two tables each with multiple column families

B.Create two tables each with a single column family

C.Create a single table with two column families

D.Create a single table with one column family

--------------------------------------------------------------------------------------------------
Question: 211
Your HBase cluster has hit a performance wall and doesn’t seem to be getting faster as you addRegionServers. Adding an additional HMaster will:

A.Have no effect on performance.

B.Improve the performance of region writes but decrease the performance of metadata changes.

C.Improve the performance of metadata chancier, but decrease the performance of region writes.

D.Make the performance problem even worse, as operations will have to be replicated to multiple masters.

--------------------------------------------------------------------------------------------------
Question: 212
Your client is writing to a region when the RegionServer crashes. At what point in the write is yourdata secure?

A.From the moment the RegionServer wrote to the WAL (write-ahead log)

B.From the moment the RegionServer returned the call

C.From the moment the RegionServer received the call

D.From the moment the RegionServer wrote to the MemStore

Explanation: Each RegionServer adds updates (Puts, Deletes) to its write-ahead log (WAL) first,and then to the Section 9.7.5.1, “MemStore” for the affected Section 9.7.5, “Store”. This ensuresthat HBasehas durable writes. Without WAL, there is the possibility of data loss in the case of aRegionServer failure before each MemStore is flushed and new StoreFiles are written. HLog is theHBase WAL implementation, and there is one HLog instance per RegionServer.Note:In computer science, write-ahead logging (WAL) is a family of techniques for providing atomicityand durability (two of the ACID properties) in database systems.

In a system using WAL, all modifications are written to a log before they are applied. Usually bothredo and undo information is stored in the log.The purpose of this can be illustrated by an example. Imagine a program that is in the middle ofperforming some operation when the machine it is running on loses power. Upon restart, thatprogram might well need to know whether the operation it was performing succeeded, halfsucceeded, or failed. If a write-ahead log were used, the program could check this log andcompare what it was supposed to be doing when it unexpectedly lost power to what was actuallydone. On the basis of this comparison, the program could decide to undo what it had started,complete what it had started, or keep things as they are.WAL allows updates of a database to be done in-place. Another way to implement atomic updatesis with shadow paging, which is not in-place. The main advantage of doing updates in-place is thatit reduces the need to modify indexes and block lists.

--------------------------------------------------------------------------------------------------
Question: 213
Given that the following is your entire dataset:<a href="http://cdn.aiotestking.com/wp-content/uploads/ccb-400/3.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ccb-400/3.jpg"/></a>

How many regions will be read during a scan of the entire dataset?

A.Four

B.Two

C.One

D.Three

--------------------------------------------------------------------------------------------------
Question: 214
For a given Column Family, you want to always retain at least one version, but expire all otherversions that are older than 5 days. Which of the following Column Family attribute settings wouldyou set to do this?

A.LENGTH = 5, MIN_VERSIONS = 1

B.TTL = 5, MIN_VERSIONS = 1

C.TTL = 432000, MIN_VERSIONS = 1

D.TTL = 432000, VERSIONS =1

Explanation: * Time To Live (TTL)ColumnFamilies can set a TTL length in seconds, and HBase will automatically delete rows oncethe expiration time is reached. This applies to all versions of a row – even the current one. The TTLtime encoded in the HBase for the row is specified in UTC.5 days is 43200 (5x24x60x60) seconds* Minimum Number of VersionsLike maximum number of row versions, the minimum number of row versions to keep is configuredper column family via HColumnDescriptor. The default for min versions is 0, which means thefeature is disabled. The minimum number of row versions parameter is used together with thetime-to-live parameter and can be combined with the number of row versions parameter to allowconfigurations such as “keep the last T minutes worth of data, at most N versions, but keep atleast M versions around” (where M is the value for minimum number of row versions, M&lt;N). Thisparameter should only be set when time-to-live is enabled for a column family and must be lessthan the number of row versions.

Reference: HBase and Schema Design

--------------------------------------------------------------------------------------------------
Question: 215
You have a table with 5 TB of data, 10 RegionServers, and a region size of 256MB. You want tocontinue with puts to widely disbursed row ids in your table. Which of the following will improve

write performance?

A.Increase your buffer cache in the RegionServers

B.Increase the number of RegionServers to 15

C.Decrease your number of RegionServers to 5

D.Decrease your region size to 128MB

Explanation:

Note:*Region SizeDetermining the “right” region size can be tricky, and there are a few factors to consider:HBase scales by having regions across many servers. Thus if you have 2 regions for 16GB data,on a 20 node machine your data will be concentrated on just a few machines – nearly the entirecluster will be idle. This really cant be stressed enough, since a common problem is loading 200MB data into HBase then wondering why your awesome 10 node cluster isn’t doing anything.On the other hand, high region count has been known to make things slow. This is getting betterwith each release of HBase, but it is probably better to have 700 regions than 3000 for the sameamount of data.There is not much memory footprint difference between 1 region and 10 in terms of indexes, etc,held by the RegionServer.

--------------------------------------------------------------------------------------------------
Question: 216
Yon are storing page view data for a large number of Web sites, each of which has manysubdomains (www.example.com, archive.example.com, beta.example.com, etc.) Your reportingtool needs to retrieve the total number of page views for a given subdomain of a Web site. Whichof the following rowkeys should you use?

A.The reverse domain name (e.g., com.example.beta)

B.The domain name followed by the URL

C.The URL

D.The URL followed by the reverse domain name

--------------------------------------------------------------------------------------------------
Question: 217
From within an HBase application, you would like to create a new table named weblogs. You havestarted with the following Java code:HBaseAdmin admin = new HBaseAdmin (conf);HTableDescriptor t = new HTableDescriptor(“weblogs”);Which of the following method(s) would you use next?

A.admin.createTable(t);admin.enable.Table(t);

B.admin.createTable(t);

C.HTable.createTable(t);HTable.enableTable(t);

D.HTable.createTable(t);

--------------------------------------------------------------------------------------------------
Question: 218
Given the following HBase dataset, which is labeled with row numbers. . .<a href="http://cdn.aiotestking.com/wp-content/uploads/ccb-400/4.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ccb-400/4.jpg"/></a>

Which of the following lists of row numbers is the correct order that HBase would store this data?

A.1, 5, 2, 4, 3, 6

B.4, 1, 2, 6, 3, 5

C.4, 6, 3, 1, 5, 2

D.3, 4, 6, 1, 2, 5

--------------------------------------------------------------------------------------------------
Question: 219
Your client application connects to HBase for the first time and queries the .META. table. Whatinformation does the .META. table provide to your client application?

A.The location of a Region

B.The location of an MMaster

C.The location of a ZooKeeper

D.The location of a –ROOT– table

Explanation: The .META. table keeps a list of all regions in the system. The .META. tablestructure is as follows:Key:Region key of the format ([table],[region start key],[region id])Values:info:regioninfo (serialized HRegionInfo instance for this region)info:server (server:port of the RegionServer containing this region)info:serverstartcode (start-time of the RegionServer process containing this region)

Reference:The Apache HBaseReference Guidehttp://hbase.apache.org/book/arch.catalog.html(9.2.2. Meta)

--------------------------------------------------------------------------------------------------
Question: 220
Which feature of HBase ensures predictable disk head seek performance within a RegionServer?

A.Data is stored distributed in HDFS

B.Data stored in HBase is sparse

C.Data is stored sorted on row keys

D.Data is stored as an uninterpreted array of bytes

--------------------------------------------------------------------------------------------------
Question: 221
You have 40 Web servers producing timeseries data from Web traffic logs. You want to attain highwrite throughput for storing this data in an HBase table. Which of these should you choose for arow key to maximize your write throughput?

A.&lt;hashCode (centralServerGeneratedSequenceID) &gt;&lt;timestamp&gt;

B.&lt;Long.MAX_VALUE – timestamp&gt;

C.&lt;timestamp&gt;

D.&lt;hashCode (serverGeneratingTheWeblog)&gt;&lt;timestamp&gt;

Explanation:

Note: In the HBase chapter of Tom White’s book Hadoop: The Definitive Guide (O’Reilly) there is aan optimization note on watching out for a phenomenon where an import process walks in lockstep with all clients in concert pounding one of the table’s regions (and thus, a single node), thenmoving onto the next region, etc. With monotonically increasing row-keys (i.e., using a timestamp),this will happen. The pile-up on a single region brought on by monotonically increasing keys canbe mitigated by randomizing the input records to not be in sorted order, but in general it’s best toavoid using a timestamp or a sequence (e.g. 1, 2, 3) as the row-key.

--------------------------------------------------------------------------------------------------
Question: 222
Your client application needs to scan s region for the row key value 104.Given a store that contains the following list of Row Key values:100, 101, 102, 103, 104, 105, 106, 107A bloom filter would return which of the following?

A.Confirmation that 104 may be contained in the set

B.Confirmation that 104 is contained in the set

C.The hash of column family

D.The file offset of the value 104

Explanation:

Note:* When a HFile is opened, typically when a region is deployed to a RegionServer, the bloom filteris loaded into memory and used to determine if a given key is in that store file.* Get/Scan(Row) currently does a parallel N-way get of that Row from all StoreFiles in a Region.This means that you are doing N read requests from disk. BloomFilters provide a lightweight inmemory structure to reduce those N disk reads to only the files likely to contain that Row (N-B).* Keep in mind that HBase only has a block index per file, which is rather course grained and tellsthe reader that a key may be in the file because it falls into a start and end key range in the blockindex. But if the key is actually present can only be determined by loading that block and scanningit. This also places a burden on the block cache and you may create a lot of unnecessary churnthat the bloom filters would help avoid.

--------------------------------------------------------------------------------------------------
Question: 223
You want to do a full table scan on your data. You decide to disable block caching to see if thisimproves scan performance. Will disabling block caching improve scan performance?

A.No. Disabling block caching does not improve scan performance.

B.Yes. When you disable block caching, you free up that memory for other operations. With a full table scan, you cannot take advantage of block caching anyway because your entire table won’t fit into cache.

C.No. If you disable block caching, HBase must read each block index from disk for each scan, thereby decreasing scan performance.

D.Yes. When you disable block caching, you free up memory for MemStore, which improves, scan performance.

--------------------------------------------------------------------------------------------------
Question: 224
Your client application if; writing data to a Region. By default, where is the data saved first?

A.StoreFile

B.WAL

C.MemStore

D.Local disk on the RegionServer

--------------------------------------------------------------------------------------------------
Question: 225
You need to free up disk space on your HBase cluster. You delete all versions of your data that isolder than one week. You notice your delete has had minimal impact on your storage availability.This is because:

A.You have large store file indexes

B.HBase has not flushed the MemStore

C.HBase has not run a minor compaction

D.HBase has not run a major compaction

--------------------------------------------------------------------------------------------------
Question: 226
You have data already stored in HDFS and are considering using HBase. Which additional featuredoes HBase provide to HDFS?

A.Random writes

B.Batch processing

C.Fault tolerance

D.Scalability

--------------------------------------------------------------------------------------------------
Question: 227
Given the following HBase table schema:Row Key, colFam_A:a, colFam_A:b, colFamB:2, colFam_B:10A table scan will return the column data in which of the following sorted orders:

A.Row Key, colFam_A:a, colFam__A:b, colFam_B:10, colFam_B:2

B.Row Key, colFam_A:a, colFam__A:b, colFam_B:2, colFam_B:10

C.Row Key, colFam_A:a, colFam__B:2, colFam_A:b, colFam_B:10

D.Row Key, colFam_A:a, colFam__B:10, colFam_A:b, colFam_B:2

--------------------------------------------------------------------------------------------------
Question: 228
Given the following dataset:<a href="http://cdn.aiotestking.com/wp-content/uploads/ccb-400/2.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ccb-400/2.jpg"/></a>

How many store files will be contained in your region(s) immediately following a majorcompaction?

A.Four

B.Three

C.Two

D.One

Explanation: There are two columns families (Managers and Skills) so there will be two files.Note:* Physically, all column family members are stored together on the filesystem. Because tuningsand storage specifications are done at the column family level, it is advised that all column family

members have the same general access pattern and size characteristics.* HBase currently does not do well with anything above two or three column families so keep thenumber of column families in your schema low. Currently, flushing and compactions are done on aper Region basis so if one column family is carrying the bulk of the data bringing on flushes, theadjacent families will also be flushed though the amount of data they carry is small. When manycolumn families the flushing and compaction interaction can make for a bunch of needless i/oloading (To be addressed by changing flushing and compaction to work on a per column familybasis).* When changes are made to either Tables or ColumnFamilies (e.g., region size, block size), thesechanges take effect the next time there is a major compaction and the StoreFiles get re-written.* StoreFiles are composed of blocks. The blocksize is configured on a per-ColumnFamily basis.Compression happens at the block level within StoreFiles.

--------------------------------------------------------------------------------------------------
Question: 229
You have a total of three tables stored in HBase. Exchanging catalog regions, how many regionswill your RegionServers have?

A.Exactly three

B.Exactly one

C.At least one

D.At least three

--------------------------------------------------------------------------------------------------
Question: 230
Why should stop an interactive machine learning algorithm as soon as the performance of themodel on a test set stops improving?

A.To avoid the need for cross-validating the model

B.To prevent overfitting

C.To increase the VC (VAPNIK-Chervonenkis) dimension for the model

D.To keep the number of terms in the model as possible

E.To maintain the highest VC (Vapnik-Chervonenkis) dimension for the model

--------------------------------------------------------------------------------------------------
Question: 231
What is default delimiter for Hive tables?

A.^A (Control-A)

B., (comma)

C.\t (tab)

D.: (colon)

--------------------------------------------------------------------------------------------------
Question: 232
Certain individuals are more susceptible to autism if they have particular combinations of genesexpressed in their DNA. Given a sample of DNA from persons who have autism and a sample ofDNA from persons who do not have autism, determine the best technique for predicting whetheror not a given individual is susceptible to developing autism?

A.Native Bayes

B.Linear Regression

C.Survival analysis

D.Sequencealignment

--------------------------------------------------------------------------------------------------
Question: 233
You are working with a logistic regression model to predict the probability that a user will click onan ad. Your model has hundreds of features, and you’re not sure if all of those features arehelping your prediction. Which regularization technique should you use to prune features thataren’t contributing to the model?

A.Convex

B.Uniform

C.L2

D.L1

--------------------------------------------------------------------------------------------------
Question: 234
Refer to the exhibit.<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/1.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/1.jpg"/></a>

Which point in the figure is the median?

A.A

B.B

C.C

--------------------------------------------------------------------------------------------------
Question: 235
Refer to the exhibit.<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/2.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/2.jpg"/></a>

Which point in the figure is the mode?

A.A

B.B

C.C

--------------------------------------------------------------------------------------------------
Question: 236
Refer to the exhibit.<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/3.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/3.jpg"/></a>

Which point in the figure is the mean?

A.A

B.B

C.C

--------------------------------------------------------------------------------------------------
Question: 237
Under what two conditions does stochastic gradient descent outperform 2nd-order optimizationtechniques such as iteratively reweighted least squares?

A.When the volume of input data is so large and diverse that a 2nd-order optimization technique can be fit to a sample of the data

B.When the model’s estimates must be updated in real-time in order to account for newobservations.

C.When the input data can easily fit into memory on a single machine, but we want to calculate confidence intervals for all of the parameters in the model.

D.When we are required to find the parameters that return the optimal value of the objective function.

--------------------------------------------------------------------------------------------------
Question: 238
What is the result of the following command (the database username is foo and password is bar)?$ sqoop list-tables – – connect jdbc : mysql : / / localhost/databasename – – table – – username foo –– password bar

A.sqoop lists only those tables in the specified MySql database that have not already been imported into FDFS

B.sqoop returns an error

C.sqoop lists the available tables from the database

D.sqoopimports all the tables from SQLHDFS

--------------------------------------------------------------------------------------------------
Question: 239
What is the most common reason for a k-means clustering algorithm to returns a sub-optimalclustering of its input?

A.Non-negative values for the distance function

B.Input data set is too large

C.Non-normal distribution of the input data

D.Poor selection of the initial controls

--------------------------------------------------------------------------------------------------
Question: 240
Consider the following sample from a distribution that contains a continuous X and label Y that iseither A or B:<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/11.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/11.jpg"/></a>

Which is the best cut point for X if you want to discretize these values into two buckets in a waythat minimizes the sum of chi-square values?

A.X8

B.X6

C.X5

D.X4

E.X2

--------------------------------------------------------------------------------------------------
Question: 241
Consider the following sample from a distribution that contains a continuous X and label Y that iseither A or B:<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/12.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/12.jpg"/></a>

Which is the best choice of cut points for X if you want to discretize these values into three bucketsthat minimizes the sum of chi-square values?

A.X5 and X8

B.X4 and X6

C.X3 and X8

D.X3 and X6

E.X2 and X9

--------------------------------------------------------------------------------------------------
Question: 242
You want to understand more about how users browse your public website. For example, you warknow which pages they visit prior to placing an order. You have a server farm of 200 web serverhosting your website. Which is the most efficient process to gather these web servers access logsinto your Hadoop cluster for analysis?

A.Sample the web server logs web servers and copy them into HDFS using curl

B.Channel these click streams into Hadoop using Hadoop Streaming

C.Write a MapReduce job with the web servers for mappers and the Hadoop cluster nodes for reducers

D.Import all user clicks from your OLTP databases Into Hadoop using Sqoop

E.Ingest the server web logs into HDFS using Flume

--------------------------------------------------------------------------------------------------
Question: 243
You have a large file of N records (one per line), and want to randomly sample 10% them. Youhave two functions that are perfect random number generators (through they are a bit slow):Random_uniform () generates a uniformly distributed number in the interval [0, 1]random_permotation (M) generates a random permutation of the number O through M -1.Below are three different functions that implement the sampling.Method AFor line in file:If random_uniform () &lt; 0.1;Print lineMethod Bi = 0for line in file:if i % 10 = = 0;print linei += 1Method Cidxs = random_permotation (N) [: (N/10)]i = 0for line in file:if i in idxs:print linei +=1

Which method will have the best runtime performance?

A.Method A

B.Method B

C.Method C

--------------------------------------------------------------------------------------------------
Question: 244
You have a large file of N records (one per line), and want to randomly sample 10% them. Youhave two functions that are perfect random number generators (through they are a bit slow):Random_uniform () generates a uniformly distributed number in the interval [0, 1]random_permotation (M) generates a random permutation of the number O through M -1.Below are three different functions that implement the sampling.Method AFor line in file:If random_uniform () &lt; 0.1;Print lineMethod Bi = 0for line in file:if i % 10 = = 0;print linei += 1Method Cidxs = random_permotation (N) [: (N/10)]i = 0

for line in file:if i in idxs:print linei +=1Which method requires the most RAM?

A.Method A

B.Method B

C.Method C

--------------------------------------------------------------------------------------------------
Question: 245
You have a large file of N records (one per line), and want to randomly sample 10% them. Youhave two functions that are perfect random number generators (through they are a bit slow):Random_uniform () generates a uniformly distributed number in the interval [0, 1]random_permotation (M) generates a random permutation of the number O through M -1.Below are three different functions that implement the sampling.Method AFor line in file:If random_uniform () &lt; 0.1;Print lineMethod Bi = 0for line in file:if i % 10 = = 0;print line

i += 1Method Cidxs = random_permotation (N) [: (N/10)]i = 0for line in file:if i in idxs:print linei +=1Which method might introduce unexpected correlations?

A.Method A

B.Method B

C.Method C

--------------------------------------------------------------------------------------------------
Question: 246
You have a large file of N records (one per line), and want to randomly sample 10% them. Youhave two functions that are perfect random number generators (through they are a bit slow):Random_uniform () generates a uniformly distributed number in the interval [0, 1]random_permotation (M) generates a random permutation of the number O through M -1.Below are three different functions that implement the sampling.Method AFor line in file:If random_uniform () &lt; 0.1;Print lineMethod B

i = 0for line in file:if i % 10 = = 0;print linei += 1Method Cidxs = random_permotation (N) [: (N/10)]i = 0for line in file:if i in idxs:print linei +=1Which method is least likely to give you exactly 10% of your data?

A.Method A

B.Method B

C.Method C

--------------------------------------------------------------------------------------------------
Question: 247
Assuming the trends shown in this chart continue, what would we expect the value of the revenue to be in Q1 of 2013?<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/13.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/13.jpg"/></a>

A.$125,000

B.$170,000

C.$220,000

D.$250,000

--------------------------------------------------------------------------------------------------
Question: 248
From historical data, you know that 50% of students who take Clouderaâ€™s Introduction to DataScience: Building Recommenders Systems training course pass this exam, while only 25% ofstudents who did not take the training course pass this exam. You also know that 50% of thisexamâ€™s candidates also take Clouderaâ€™s Introduction to Data Science: Building RecommendationsSystems training course.If we know that a person has passed this exam, what is the probability that they took clouderaâ€™sintroduction to Data Science: Building Recommender Systems training course?

A.2/3

B.1/2

C.3/4

D.3/5

--------------------------------------------------------------------------------------------------
Question: 249
From historical data, you know that 50% of students who take Cloudera’s Introduction to DataScience: Building Recommenders Systems training course pass this exam, while only 25% ofstudents who did not take the training course pass this exam. You also know that 50% of thisexam’s candidates also take Cloudera’s Introduction to Data Science: Building RecommendationsSystems training course.What is the probability that any individual exam candidate will pass the data science exam?

A.3/8

B.1/4

C.1/8

D.1/2

--------------------------------------------------------------------------------------------------
Question: 250
company has 20 software engineers working to fix on a project. Over the past week, the teamhas fixed 100 bugs. Although the average number of bugs. Although the average number of bugsfixed per engineer id five. None of the engineer fixed exactly five bugs last week.One engineer points out that some bugs are more difficult to fix than others. What metric shouldyou use to estimate how hard a particular bug is to fix?

A.The tech lead’s estimate of how many hours would be needed to fix the bug.

B.The priority of the bug according to the project manager

C.The number of years that the engineer who was assigned the bug has worked at the company

D.The number of bugs that had been found in each sub-component of the project

--------------------------------------------------------------------------------------------------
Question: 251
In what way can Hadoop be used to improve the performance of LIoyd’s algorithm for k-means

clustering on large data sets?

A.Parallelizing the centroid computations to improve numerical stability

B.Distributing the updates of the cluster centroids

C.Reducing the number of iterations required for the centroids to converge

D.Mapping the input data into a non-Euclidean metric space

--------------------------------------------------------------------------------------------------
Question: 252
You have a data file that contains two trillion records, one record per line (comma separated).Each record lists two friends and unique message sent between them. Their names will not havecommas.Michael, John, Pabst, Blue RibbonTiffany, James, BMX RacingJohn, Michael, Natural Lemon FlavorAnalyze the pseudo code examples below and determine which set of mappers and reducers inthe below pseudo code snippets will solve for the mean number of messages each user sends toall of the friends?For example pseudo code may have three friends to whom he sends 6, 10, and 200 messages,respectively, so Michael’s mean would be (6+10+200)/3. The solution may require a pipeline oftwo MapReduce jobs.

A.def mapper1 (line): key1, key2, message = line.split (‘ , ’) emit ( (key1, key2) , 1) def reducer1(key, values): emit (key, sum(values)) def mapper2(key, value): key1, key2 = key / / unpack both friends name into separate keys emit (key1, value) def reducer2(key, values): emit (key, mean (values) )

B.def mapper1 (line): key1, key2, message = line.split (‘ , ’) emit ( (key1, key2) , 1) emit ( (key1, key2) , 1) def reducer1(key, values): emit (key, sum(values)) def mapper2(key, value): key1, key2 = key / / unpack both friends name into separate keys emit (key1, value) def reducer2(key, values): emit (key, mean (values) )

C.def mapper1 (line): key1, key2, message = line.split (‘ , ’) emit ( (key1, key2) , 1) emit ( (key1, key2) , 1) def reducer1(key, values): emit (key, sum(values))

D.defmapper (line): Key1, key2, message =line.split(‘ , ’) Sort (key1, key2) / /a fiven pair will always besorted the same Emit( ( key 1, key2), 1) Def reducer1(key, values) : Emit (key, sum (values) ) Def Mapper2 (key, value) Key1, key2 = key / / unpack both friends names into separate keys Emit (key1, value) Emit(key2, value) Def reducer2(key, values); Emit (key, mean (values) )

--------------------------------------------------------------------------------------------------
Question: 253
You have just run a MapReduce job to filter user messages to only those of a selectedgeographical region. The output for this job in a directory named westUsers, located just belowyour home directory in HDFS. Which command gathers these records into a single file on yourlocal file system?

A.Hadoop fs – getmerge westUsers WestUsers.txt

B.Hadoop fs –get westUsers WestUsers.txt

C.Hadoop fs – cp westUsers/* westUsers.txt

D.Hadoop fs –getmerge –R westUsers westUsers.txt

--------------------------------------------------------------------------------------------------
Question: 254
Function  is convex if the line segment between two points, a and b is greater than equal to thevalue of the a x b<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/10.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/10.jpg"/></a>

Which two functions are convex?

A.X1/2

B.Ex

C.2x-1

D.1-x2

--------------------------------------------------------------------------------------------------
Question: 255
You need to analyze 60,000,000 images stored in JPEG format, each of which is approximately 25KB. Because your Hadoop cluster isn’t optimized for storing and processing many small files youdecide to do the following actions:1. Group the individual images into a set of larger files2. Use the set of larger files as input for a MapReduce job that processes them directly withPython using Hadoop streamingWhich data serialization system gives you the flexibility to do this?

A.CSV

B.XML

C.HTML

D.Avro

E.Sequence Files

F.JSON

--------------------------------------------------------------------------------------------------
Question: 256
You have user profile records in an OLTP database that you want to join with web server logswhich you have already ingested into HDFS. What is the best way to acquire the user profile foruse in HDFS?

A.Ingest with Hadoop streaming

B.Ingest with Apache Flume

C.Ingest using Hive’s LOAD DATA command

D.Ingest using Sqoop

E.Ingest using Pig’s LOAD command

--------------------------------------------------------------------------------------------------
Question: 257
You are building a system to perform outlier detection for a large online retailer. You need to builda system to detect if the total dollar value of sales are outside the norm for each U.S. state, asdetermined from the physical location of the buyer for each purchase.The retailer’s data sources are scattered across multiple systems and databases and areunorganized with little coordination or shared data or keys between the various data sources.Below are the sources of data available to you. Determine which three will give you the smallestset of data sources but still allow you to implement the outlier detector by state.

A.Database of employees that Includes only the employee ID, start date, and department

B.Database of users that contains only their user ID, name, and a list of every Item the user has viewed

C.Transaction log that contains only basket ID, basket amount, time of sale completion, and a session ID

D.Database of user sessions that includes only session ID, corresponding user ID, and the corresponding IP address

E.External database mapping IP addresses to geographic locations

F.Database of items that includes only the item name, item ID, and warehouse location

G.Database of shipments that includes only the basket ID, shipment address, shipment date, and shipment method

--------------------------------------------------------------------------------------------------
Question: 258
How can the naiveté of the naive Bayes classifier be advantageous?

A.It does not require you to make strong assumptions about the data because it is a nonparametric

B.It significantly reduces the size of the parameter space, thus reducing the risk of over fitting

C.It allows you to reduce bias with no tradeoff in variance

D.It guarantees convergence of the estimator

--------------------------------------------------------------------------------------------------
Question: 259
What are two defining features of RMSE (root-mean square error or root-mean-square deviation)?

A.It is sensitive to outliers

B.It is the mean value of recommendations of the K-equal partitions in the input data

C.It is the square of the median value of the error where error is the difference between predicted rating and actual ratings

D.It is appropriate for numeric data

E.It considers the order of recommendations

--------------------------------------------------------------------------------------------------
Question: 260
You are building a k-nearest neighbor classifier (k-NN) on a labeled set of points in a highdimensional space. You determine that the classifier has a large error on the training data. What isthe most likely problem?

A.High-dimensional spaces effectively make local neighborhoods global

B.k-NN compotation does not coverage in high dimensions

C.k was too small

D.The VC-dimension of a k-NN classifier is too high

--------------------------------------------------------------------------------------------------
Question: 261
Which best describes the primary function of Flume?

A.Flume is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with an infrastructure consisting of sources and sinks for importing and evaluating large data sets

B.Flume acts as a Hadoop filesystem for log files

C.Flume Imports data from SQL/relational database into your Hadoop cluster

D.Flume provides a query languages for Hadoop similar to SQL

E.Flume is a distributed server for collecting and moving large amount of data into HDFS as it’s produced from streaming data flows

--------------------------------------------------------------------------------------------------
Question: 262
You have a directory containing a number of comma-separated files. Each file has three columnsand each filename has a .csv extension. You want to have a single tab-separated file (all .tsv) thatcontains all the rows from all the files.Which command is guaranteed to produce the desired output if you have more than 20,000 files toprocess?

A.Find . – name ‘*, CSV’ – print0 | sargs -0 cat | tr ‘,’ ‘\t’ &gt; all.tsv

B.Find . –name ‘name * .CSV’ | cat | awk ‘BEGIN {FS = “,” OFS = “\t”} {print $1, $2, $3}’ &gt; all.tsv

C.Find . – name ‘*.CSV’ | tr ‘,’ ‘\t’ | cat &gt; all.tsv

D.Find . –name ‘*.CSV’ | cat &gt; all.tsv

E.Cat *.CSV &gt; all.tsv

--------------------------------------------------------------------------------------------------
Question: 263
What are three benefits of running feature selection analysis before filtering a classification model?

A.Eliminates the need to include a regularization term

B.Reduces the number of subjective decisions required to construct the model

C.Guarantee the optimally of the final model

D.Speeds up the model fitting process

E.Develops an understanding of the importance of different features

F.Improves the predictive performance of the model

--------------------------------------------------------------------------------------------------
Question: 264
When optimizing a function using stochastic gradient descent, how frequently should you updateyour estimate of the gradient?

A.Once after every pass through the data set

B.Once per observation

C.For each observation with a probability that you choose ahead of time

D.After a random number of observations

E.Once every N observations, where you decide N ahead of time

--------------------------------------------------------------------------------------------------
Question: 265
In what format are web server log files usually generated and how must you transform them inorder to make them usable for analysis in Hadoop?

A.XML files that you need to convert to JSON

B.Text files that require parsing into useful fields

C.CSV files that require parsing into useful fields

D.HTML files that you need to convert to plain text or CSV

E.Binary files that may require decompression and conversion using AVRO

--------------------------------------------------------------------------------------------------
Question: 266
Which recommender system technique is domain specific?

A.Content-based collaboration filtering

B.Item-based collaborative filtering

C.User-based collaborative filtering

D.Naïve Bayes classifier

--------------------------------------------------------------------------------------------------
Question: 267
You are about to sample a 100-dimensinal unit-cube. To adequately sample any single givendimension, you need only capture 10 points. How many points do you need to order to sample thecomplete 100-dimensional unit cube adequately?

A.10010

B.1010

C.Log2(100)

D.100

E.1000

F.1010

--------------------------------------------------------------------------------------------------
Question: 268
You have acquired a new data source of millions of customer records, and you’ve this data intoHDFS. Prior to analysis, you want to change all customer registration to the same date format,make all addresses uppercase, and remove all customer names (for anonymization). Whichprocess will accomplish all three objectives?

A.Adapt the data cleansing module in Mahout to your data, and invoke the Mahout library when you run your analysis

B.Pull this data into an RDBMS using sqoop and scrub records using stored procedures

C.Write a script that receives records on stdin, corrects them, and then writes them to stdout. Then, invoke this script in a map-only Hadoop Streaming Job

D.Write a MapReduce job with a mapper to change words to uppercase and to reduce different forms of dates to a single form

--------------------------------------------------------------------------------------------------
Question: 269
company has 20 software engineers working to fix on a project. Over the past week, the teamhas fixed 100 bugs. Although the average number of bugs. Although the average number of bugsfixed per engineer id five. None of the engineer fixed exactly five bugs last week.You want to understand how productive each engineer is at fixing bugs. What is the best way tovisualize the distribution of bug fixes per engineer?

A.A bar chart of engineers vs. number of bugs fixed

B.A scatter plot of engineers vs. number of bugs fixed

C.A normal distribution of the mean and standard deviation of bug fixes per engineer

D.A histogram that groups engineers to together based on the number of bugs they fixed

--------------------------------------------------------------------------------------------------
Question: 270
There are 20 patients with acute lymphoblastic leukemia (ALL) and 32 patients with acute myeloidleukemia (AML), both variants of a blood cancer.The makeup of the groups as follows:<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/4.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/4.jpg"/></a>

Each individual has an expression value for each of 10000 different genes. The expression valuefor each gene is a continuous value between -1 and 1.You’ve built your model for discriminating between AML and ALL patients and you find that itworks quite well on your current data. One month later, a collaboration tells you she has freshdata from 100 new AML/ALL patients. You run the samples through your model, and turns outyour model has very poor predictive accuracy on the new samples; specifically, your modelpredicts that all males have ALL. What is the most reliable way to fix this problem?

A.Change the distance metric

B.Reduce the number of dimensions

C.Use a Gibbs sampler on a Bayesian network

D.Perform matched sampling across other provided variables

--------------------------------------------------------------------------------------------------
Question: 271
There are 20 patients with acute lymphoblastic leukemia (ALL) and 32 patients with acute myeloidleukemia (AML), both variants of a blood cancer.The makeup of the groups as follows:<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/5.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/5.jpg"/></a>

Each individual has an expression value for each of 10000 different genes. The expression valuefor each gene is a continuous value between -1 and 1.You want to use the data from the 52 patients in the scenario to improve the ability of doctorsbeing able to distinguish between ALL and AML. What type of data science problem is this?

A.Classification

B.Regression

C.Clustering

D.Filtering

--------------------------------------------------------------------------------------------------
Question: 272
There are 20 patients with acute lymphoblastic leukemia (ALL) and 32 patients with acute myeloidleukemia (AML), both variants of a blood cancer.The makeup of the groups as follows:<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/6.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/6.jpg"/></a>

Each individual has an expression value for each of 10000 different genes. The expression valuefor each gene is a continuous value between -1 and 1.With which type of plot can you encode the most amount of the data visually?

A.A heat map sorting the individuals by group

B.A histogram of the expression values

C.A scatter plot of two largest principal components

--------------------------------------------------------------------------------------------------
Question: 273
There are 20 patients with acute lymphoblastic leukemia (ALL) and 32 patients with acute myeloidleukemia (AML), both variants of a blood cancer.The makeup of the groups as follows:<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/7.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/7.jpg"/></a>

Each individual has an expression value for each of 10000 different genes. The expression valuefor each gene is a continuous value between -1 and 1.With which type of plot can you encode the most amount of the data visually?Rather than use all 10,000 features to separate AML from ALL, you pick a small subnet of featuresto separate them optimally. You feature vectors have 10,000 dimensions while you only have 52data points. You use cross-validation to test your chosen set of features. What three methods willchoose the features in an optimal way?

A.Singular value Decomposition

B.Bootstrapping

C.Markov chain Monte Carlo

D.Hidden Markov

E.Bayesian Information Criterion

F.Mutual Information

--------------------------------------------------------------------------------------------------
Question: 274
There are 20 patients with acute lymphoblastic leukemia (ALL) and 32 patients with acute myeloidleukemia (AML), both variants of a blood cancer.The makeup of the groups as follows:<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/8.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/8.jpg"/></a>

Each individual has an expression value for each of 10000 different genes. The expression valuefor each gene is a continuous value between -1 and 1.With which type of plot can you encode the most amount of the data visually?You choose to perform agglomerative hierarchical clustering on the 10,000 features. How muchRAM do you need to hold the distance Matrix, assuming each distance value is 64-bit double?

A.~ 800 MB

B.~ 400 MB

C.~ 160 KB

D.~ 4 MB

--------------------------------------------------------------------------------------------------
Question: 275
You have a large m x n data matrix M. You decide you want to perform dimensionreduction/clustering on your data and have decide to use the singular value decomposition (SVD;also called principal components analysis PCA)

You performed singular value decomposition (SVD; also called principal components analysis orPCA) on you data matrix but you did not center your data first. What does your first singularcomponent describe?

A.The mean of the data set

B.The variance of the data set

C.The standard deviation of the data set

D.The maximum of the data set

E.The median of the data set

--------------------------------------------------------------------------------------------------
Question: 276
You have a large m x n data matrix M. You decide you want to perform dimensionreduction/clustering on your data and have decide to use the singular value decomposition (SVD;also called principal components analysis PCA)Refer to the passage above.What represents the SVD of the Matrix standard M given the following information:U is m x m unitaryV is n x n unitaryS is m x n diagonalQ is n x n invertibleD is n x n diagonalL is m x m lower triangularU is m x m upper triangular

A.M = U S V

B.M = U P

C.M = Q D Q-1

D.M = L U

--------------------------------------------------------------------------------------------------
Question: 277
You have a large m x n data matrix M. You decide you want to perform dimensionreduction/clustering on your data and have decide to use the singular value decomposition (SVD;also called principal components analysis PCA)For the moment, assume that your data matrix M is 500 x 2. The figure below shows a plot of the data.<a href="http://cdn.aiotestking.com/wp-content/uploads/ds-200/9.jpg"><img alt="" class="aligncenter size-full" src="http://cdn.aiotestking.com/wp-content/uploads/ds-200/9.jpg"/></a>

Which line represents the second principal component?

A.Blue

B.Yellow

--------------------------------------------------------------------------------------------------
Question: 278
Many machine learning algorithm involve finding the Global minimum of a convex loss function,primarily because:

A.The additive inverse of a convex function is concave

B.The derivative of convex function is always defined

C.The second derivative of a convex function is a constant

D.Any local minimum of a convex is also a global minimum

--------------------------------------------------------------------------------------------------
Question: 279
Which two techniques should you use to avoid overfitting a classification model to a data set?

A.Include a small number “noise” features that are not through to be correlated with the dependent variable.

B.Replicate features that are through to be significant predicators of the dependent variable multiple time for each observation.

C.Separate your input data into a training set that is used for fitting and a test set that is used forevaluating the model’s performance

D.Include a regularization term in the model’s objective function to control how precisely the model fits the data

E.Preprocess the data to exclude a typical observation from the model input

--------------------------------------------------------------------------------------------------
Question: 280
You have just run a MapReduce job to filter user messages to only those of a selectedgeographical region. The output for this job is in a directory named westUsers, located justbelow your home directory in HDFS. Which command gathers these into a single file on

your local file system?

A. Hadoop fs –getmerge –R westUsers.txt

B. Hadoop fs –get westUsers westUsers.txt

C. Hadoop fs –getemerge westUsers westUsers.txt

D. Hadoop fs –cp westUsers/* westUsers.txt

--------------------------------------------------------------------------------------------------
Question: 281
In CDH4 and later, which file contains a serialized form of all the directory and files inodes inthe filesystem, giving the NameNode a persistent checkpoint of the filesystem metadata?

A. fstime

B. VERSION

C. Fsimage_N (where N reflects transactions up to transaction ID N)

D. Edits_N-M (where N-M transactions between transaction ID N and transaction ID N)

--------------------------------------------------------------------------------------------------
Question: 282
You are running a Hadoop cluster with a NameNode on host mynamenode. What are two

ways to determine available HDFS space in your cluster?

A. Run hdfs fs –du / and locate the DFS Remaining value

B. Connect to http://mynamenode:50070/dfshealth.jsp and locate the DFS remaining value

C. Run hdfs dfs / and subtract NDFS Used from configured Capacity

D. Run hdfs dfsadmin –report and locate the DFS Remaining value

--------------------------------------------------------------------------------------------------
Question: 283
You have recently converted your Hadoop cluster from a MapReduce 1 (MRv1) architectureto MapReduce 2 (MRv2) on YARN architecture. Your developers are accustomed tospecifying map and reduce tasks (resource allocation) tasks when they run jobs: Adeveloper wants to know how specify to reduce tasks when a specific job runs. Whichmethod should you tell that developers to implement?

A. MapReduce version 2 (MRv2) on YARN abstracts resource allocation away from the idea of “tasks” into memory and virtual cores, thus eliminating the need for a developer to specify the number of reduce tasks, and indeed preventing the developer from specifying the number of reduce tasks.

B. In YARN, resource allocations is a function of megabytes of memory in multiples of 1024mb. Thus, they should specify the amount of memory resource they need by executing –D mapreducereduces. memory-mb-2048

C. In YARN, the ApplicationMaster is responsible for requesting the resource required for a specific launch. Thus, executing –D yarn.applicationmaster.reduce.tasks=2 will specify that the ApplicationMaster launch two task contains on the worker nodes.

D. Developers specify reduce tasks in the exact same way for both MapReduce version 1 

(MRv1) and MapReduce version 2 (MRv2) on YARN. Thus, executing –D mapreduce.job.reduces-2 will specify reduce tasks.</div>]
--------------------------------------------------------------------------------------------------
Question: 284
Your Hadoop cluster contains nodes in three racks. You have not configured the dfs.hostsproperty in the NameNode’s configuration file. What results?

A. No new nodes can be added to the cluster until you specify them in the dfs.hosts file

B. Any machine running the DataNode daemon can immediately join the cluster

C. The NameNode will update the dfs.hosts property to include machines running the DataNode daemon on the next NameNode reboot or with the command dfsadmin –refreshNodes

D. Presented with a blank dfs.hosts property, the NameNode will permit DataNodes specified in mapred.hosts to join the cluster

Explanation:



--------------------------------------------------------------------------------------------------
Question: 285
You are running a Hadoop cluster with MapReduce version 2 (MRv2) on YARN. Youconsistently see that MapReduce map tasks on your cluster are running slowly because ofexcessive garbage collection of JVM, how do you increase JVM heap size property to 3GBto optimize performance?

A. yarn.application.child.java.opts=-Xsx3072m

B. yarn.application.child.java.opts=-Xmx3072m

C. mapreduce.map.java.opts=-Xms3072m

D. mapreduce.map.java.opts=-Xmx3072m

--------------------------------------------------------------------------------------------------
Question: 286
You have a cluster running with a FIFO scheduler enabled. You submit a large job A to thecluster, which you expect to run for one hour. Then, you submit job B to the cluster, whichyou expect to run a couple of minutes only. You submit both jobs with the same priority.Which two best describes how FIFO Scheduler arbitrates the cluster resources for job andits tasks?

A. The order of execution of job may vary

B. The FIFO Scheduler will pass an exception back to the client when Job B is submitted, since all slots on the cluster are use

C. Given job A and submitted in that order, all tasks from job A are guaranteed to finish before all tasks from job B 

D. Because there is a more than a single job on the cluster, the FIFO Scheduler will enforce a limit on the percentage of resources allocated to a particular job at any given time

E. Tasks are scheduled on the order of their job submission

F. The FIFO Scheduler will give, on average, and equal share of the cluster resources over the job lifecycle

--------------------------------------------------------------------------------------------------
Question: 287
user comes to you, complaining that when she attempts to submit a Hadoop job, it fails.There is a Directory in HDFS named /data/input. The Jar is named j.jar, and the driver classis named DriverClass. She runs the command: Hadoop jar j.jar DriverClass/data/input/data/output The error message returned includes the line:PriviligedActionException as:training (auth:SIMPLE)cause:org.apache.hadoop.mapreduce.lib.input.invalidInputException: Input path does notexist: file:/data/input What is the cause of the error?

A. The user is not authorized to run the job on the cluster

B. The output directory already exists

C. The name of the driver has been spelled incorrectly on the command line

D. The directory name is misspelled in HDFS

E. The Hadoop configuration files on the client do not point to the cluster

Explanation:



--------------------------------------------------------------------------------------------------
Question: 288
Your company stores user profile records in an OLTP databases. You want to join theserecords with web server logs you have already ingested into the Hadoop file system. Whatis the best way to obtain and ingest these user records?

A. Ingest with Hadoop streaming

B. Ingest using the HDFS put command

C. Ingest with Pig’s LOAD command

D. Ingest using Hive’s IQAD DATA command

E. Ingest with sqoop import

--------------------------------------------------------------------------------------------------
Question: 289
Which two are features of Hadoop’s rack topology?

A. Even for small clusters on a single rack, configuring rack awareness will improve performance

B. Configuration of rack awareness is accomplished using a configuration file. You cannot use a rack topology script.

C. Hadoop gives preference to intra-rack data transfer in order to conserve bandwidth

D. Rack location is considered in the HDFS block placement policy

E. HDFS is rack aware but MapReduce daemon are not 

--------------------------------------------------------------------------------------------------
Question: 290
You have an employee who is a Date Analyst and is very comfortable with SQL. He would like torun ad-hoc analysis on data in your HDFS duster. Which of the following is a data warehousingsoftware built on top of Apache Hadoop that defines a simple SQL-like query language well-suitedfor this kind of user?

A.Pig

B.Hue

C.Hive

D.Sqoop

E.Oozie

F.Flume

G.Hadoop Streaming

Explanation:Hive defines a simple SQL-like query language, called QL, that enables usersfamiliar with SQL to query the data. At the same time, this language also allows programmers whoare familiar with the MapReduce framework to be able to plug in their custom mappers andreducers to perform more sophisticated analysis that may not be supported by the built-incapabilities of the language. QL can also be extended with custom scalar functions (UDF’s),aggregations (UDAF’s), and table functions (UDTF’s).

Reference:https://cwiki.apache.org/Hive/(Apache Hive, first sentence and second paragraph)

--------------------------------------------------------------------------------------------------
Question: 291
Which of the following statements most accurately describes the relationship between MapReduceand Pig?

A.Pig provides additional capabilities that allow certain types of data manipulation not possible with MapReduce.

B.Pig provides no additional capabilities to MapReduce. Pig programs are executed as MapReduce jobs via the Pig interpreter.

C.Pig programs rely on MapReduce but are extensible, allowing developers to do special-purpose processing not provided by MapReduce.

D.Pig provides the additional capability of allowing you to control the flow of multiple MapReduce jobs.

--------------------------------------------------------------------------------------------------
Question: 292
You need to import a portion of a relational database every day as files to HDFS, and generateJava classes to Interact with your imported data. Which of the following tools should you use toaccomplish this?

A.Pig

B.Hue

C.Hive

D.Flume

E.Sqoop

F.Oozie

G.fuse-dfs

Explanation:Sqoop (“SQL-to-Hadoop”) is a straightforward command-line tool with the followingcapabilities:Imports individual tables or entire databases to files in HDFSGenerates Java classes to allow you to interact with your imported data

Provides the ability to import from SQL databases straight into your Hive data warehouseNote:Data Movement Between Hadoop and Relational DatabasesData can be moved between Hadoop and a relational database as a bulk data transfer, orrelational tables can be accessed from within a MapReduce map function.Note:*Cloudera’s Distribution for Hadoop provides a bulk data transfer tool (i.e., Sqoop) that importsindividual tables or entire databases into HDFS files. The tool also generates Java classes thatsupport interaction with the imported data. Sqoop supports all relational databases over JDBC,and Quest Software provides a connector (i.e., OraOop) that has been optimized for access todata residing in Oracle databases.Reference:http://log.medcl.net/item/2011/08/hadoop-and-mapreduce-big-data-analyticsgartner/(Data Movement between hadoop and relational databases, second paragraph)

--------------------------------------------------------------------------------------------------
Question: 293
Workflows expressed in Oozie can contain:

A.Iterative repetition of MapReduce jobs until a desired answer or state is reached.

B.Sequences of MapReduce and Pig jobs. These are limited to linear sequences of actions with exception handlers but no forks.

C.Sequences of MapReduce jobs only; no Pig or Hive tasks or jobs. These MapReduce sequences can be combined with forks and path joins.

D.Sequences of MapReduce and Pig. These sequences can be combined with other actions including forks, decision points, and path joins.

--------------------------------------------------------------------------------------------------
Question: 294
You need a distributed, scalable, data Store that allows you random, realtime read/write access tohundreds of terabytes of data. Which of the following would you use?

A.Hue

B.Pig

C.Hive

D.Oozie

E.HBase

F.Flume

G.Sqoop

Explanation:Use Apache HBase when you need random, realtime read/write access to your BigData.Note:This project’s goal is the hosting of very large tables — billions of rows X millions of columns — atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned,

column-oriented store modeled after Google’s Bigtable: A Distributed Storage System forStructured Data by Chang et al. Just as Bigtable leverages the distributed data storage providedby the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoopand HDFS.FeaturesLinear and modular scalability.Strictly consistent reads and writes.Automatic and configurable sharding of tablesAutomatic failover support between RegionServers.Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.Easy to use Java API for client access.Block cache and Bloom Filters for real-time queries.Query predicate push down via server side FiltersThrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary dataencoding optionsExtensible jruby-based (JIRB) shellSupport for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMXReference:http://hbase.apache.org/(when would I use HBase? First sentence)

--------------------------------------------------------------------------------------------------
Question: 295
What is the preferred way to pass a small number of configuration parameters to a mapper orreducer?

A.As key-value pairs in the jobconf object.

B.As a custom input key-value pair passed to each mapper or reducer.

C.Using a plain text file via the Distributedcache, which each mapper or reducer reads.

D.Through a static variable in the MapReduce driver class (i.e., the class that submits the MapReduce job).

--------------------------------------------------------------------------------------------------
Question: 296
Given a Mapper, Reducer, and Driver class packaged into a jar, which is the correct way ofsubmitting the job to the cluster?

A.jar MyJar.jar

B.jar MyJar.jar MyDriverClass inputdir outputdir

C.hadoop jar MyJar.jar MyDriverClass inputdir outputdir

D.hadoop jar class MyJar.jar MyDriverClass inputdir outputdir

Explanation:Example:

Run the application:$ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input/usr/joe/wordcount/output

--------------------------------------------------------------------------------------------------
Question: 297
Which of the following utilities allows you to create and run MapReduce jobs with any executableor script as the mapper and/or the reducer?

A.Oozie

B.Sqoop

C.Flume

D.Hadoop Streaming

--------------------------------------------------------------------------------------------------
Question: 298
What is the difference between a failed task attempt and a killed task attempt?

A.A failed task attempt is a task attempt that threw an unhandled exception. A killed task attempt is one that was terminated by the JobTracker.

B.A failed task attempt is a task attempt that did not generate any key value pairs. A killed task attempt is a task attempt that threw an exception, and thus killed by the execution framework.

C.A failed task attempt is a task attempt that completed, but with an unexpected status value. A killed task attempt is a duplicate copy of a task attempt that was started as part of speculative execution.

D.A failed task attempt is a task attempt that threw a RuntimeException (i.e., the task fails). A killed task attempt is a task attempt that threw any other type of exception (e.g., IOException); the execution framework catches these exceptions and reports them as killed.

--------------------------------------------------------------------------------------------------
Question: 299
Custom programmer-defined counters in MapReduce are:

A.Lightweight devices for bookkeeping within MapReduce programs.

B.Lightweight devices for ensuring the correctness of a MapReduce program. Mappers Increment counters, and reducers decrement counters. If at the end of the program the counters read zero, then you are sure that the job completed correctly.

C.Lightweight devices for synchronization within MapReduce programs. You can use counters to coordinate execution between a mapper and a reducer.

--------------------------------------------------------------------------------------------------
Question: 300
For each job, the Hadoop framework generates task log files. Where are Hadoop’s task log filesstored?

A.Cached on the local disk of the slave node running the task, then purged immediately upon task completion.

B.Cached on the local disk of the slave node running the task, then copied into HDFS.

C.In HDFS, in the directory of the user who generates the job.

D.On the local disk of the slave node running the task.

--------------------------------------------------------------------------------------------------
Question: 301
Compare the hardware requirements of the NameNode with that of the DataNodes in a Hadoopcluster running MapReduce v1 (MRv1):

A.The NameNode requires more memory and requires greater disk capacity than the DataNodes.

B.The NameNode and DataNodes should the same hardware configuration.

C.The NameNode requires more memory and no disk drives.

D.The NameNode requires more memory but less disk capacity.

E.The NameNode requires less memory and less disk capacity than the DataNodes.

--------------------------------------------------------------------------------------------------
Question: 302
You are a Hadoop cluster with a NameNode on host mynamenode. What are two ways todetermine available HDFS space in your cluster?

A.Run hadoop fs –du/and locate the DFS Remaining value.

B.Connect to http://mynamemode:50070/ and locate the DFS Remaining value.

C.Run hadoop dfsadmin –report and locate the DFS Remaining value.

D.Run hadoop dfsadmin –SpaceQuota and subtract HDFS Used from Configured Capacity.

--------------------------------------------------------------------------------------------------
Question: 303
You are planning a Hadoop duster, and you expect to be receiving just under 1TB of data perweek which will be stored on the cluster, using Hadoop’s default replication. You decide that yourslave nodes will be configured with 4 x 1TB disks.Calculate how many slave nodes you need to deploy at a minimum to store one year’s worth ofdata.

A.100 slave nodes

B.100 slave nodes

C.10 slave nodes

D.50 slave nodes

--------------------------------------------------------------------------------------------------
Question: 304
Which scheduler would you deploy to ensure that your cluster allows short jobs to finish within areasonable time without starving long-running jobs?

A.FIFO Scheduler 

B.Fair Scheduler 

C.Capacity Scheduler 

D.Completely Fair Scheduler (CFS)

Explanation:Fair scheduling is a method of assigning resources to jobs such that all jobs get, on

average, an equal share of resources over time. When there is a single job running, that job usesthe entire cluster. When other jobs are submitted, tasks slots that free up are assigned to the newjobs, so that each job gets roughly the same amount of CPU time. Unlike the default Hadoopscheduler, which forms a queue of jobs, this lets short jobs finish in reasonable time while notstarving long jobs. It is also a reasonable way to share a cluster between a number of users.Finally, fair sharing can also work with job priorities – the priorities are used as weights todetermine the fraction of total compute time that each job should get. Hadoop, Fair Scheduler Guide

--------------------------------------------------------------------------------------------------
Question: 305
Your Hadoop cluster has 12 slave nodes, a block size set to 64MB, and a replication factor ofthree.Choose which best describes how the Hadoop Framework distributes block writes into HDFS froma Reducer outputting a 150MB file?

A.The Reducer will generate twelve blocks and write them to slave nodes nearest the node on which the Reducer runs.

B.The Reducer will generate nine blocks and write them randomly to nodes throughout the cluster.

C.The slave node on which the Reducer runs gets the first copy of every block written. Other block replicas will be placed on other nodes.

D.Reducers don’t write blocks into HDFS

Explanation:Note: *The placement of replicas is critical to HDFS reliability and performance. Optimizing replicaplacement distinguishes HDFS from most other distributed file systems. This is a feature thatneeds lots of tuning and experience. The purpose of a rack-aware replica placement policy is toimprove data reliability, availability, and network bandwidth utilization. The current implementationfor the replica placement policy is a first effort in this direction. The short-term goals ofimplementing this policy are to validate it on production systems, learn more about its behavior,and build a foundation to test and research more sophisticated policies.* In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each blockis typically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicateeach block three times. Replicas are stored on different nodes. HDFS utilizes the local file systemto store each HDFS block as a separate file. HDFS Block size can not be compared with the

traditional file system block size.

--------------------------------------------------------------------------------------------------
Question: 306
You has a cluster running with the Fail Scheduler enabled. There are currently no jobs running onthe cluster you submit a job A, so that only job A is running on the cluster. A while later, yousubmit job B. Now job A and Job B are running on the cluster al the same time. How will the Fair’Scheduler handle these two Jobs?

A.When job A gets submitted, it consumes all the task slot

B.When job A gets submitted, it doesn’t consume all the task slot

C.When job B gets submitted, Job A has to finish first, before job it can get scheduled.

D.When job B gets submitted, it will get assigned tasks, while job A continues to run with fewer tasks.

--------------------------------------------------------------------------------------------------
Question: 307
In the context of configuring a Hadoop cluster for HDFS High Availability (HA), ‘fencing’ refers to: 

A.Isolating a failed NameNode from write access to the fsimage and edits files so that is cannot resume write operations if it recovers.

B.Isolating the cluster’s master daemon to limit write access only to authorized clients.

C.Isolating both HA NameNodes to prevent a client application from killing the NameNode daemons.

D.Isolating the standby NameNode from write access to the fsimage and edits files.

Explanation:A fencing method is a method by which one node can forcibly prevent another nodefrom making continued progress.This might be implemented by killing a process on the other node, by denying the other node’saccess to shared storage, or by accessing a PDU to cut the other node’s power.Since these methods are often vendor- or device-specific, operators may implement this interfacein order to achieve fencing.Fencing is configured by the operator as an ordered list of methods to attempt. Each method willbe tried in turn, and the next in the list will only be attempted if the previous one fails. SeeNodeFencer for more information.Note: failover – initiate a failover between two NameNodesThis subcommand causes a failover from the first provided NameNode to the second. If the firstNameNode is in the Standby state, this command simply transitions the second to the Active statewithout error. If the first NameNode is in the Active state, an attempt will be made to gracefullytransition it to the Standby state. If this fails, the fencing methods (as configured bydfs.ha.fencing.methods) will be attempted in order until one of the methods succeeds. Only afterthis process will the second NameNode be transitioned to the Active state. If no fencing methodsucceeds, the second NameNode will not be transitioned to the Active state, and an error will bereturned. org.apache.hadoop.ha, Interface FenceMethod

 HDFS High Availability Administration, HA Administration using the haadmin command

--------------------------------------------------------------------------------------------------
Question: 308
Under which scenario would it be most appropriate to consider using faster (e.g 10 Gigabit)Ethernet as the network fabric for your Hadoop cluster?

A.When the typical workloads generates a large amount of intermediate data, on the order of the input data itself.

B.When the typical workloads consists of processor-intensive tasks.

C.When the typical workloads consumes a large amount of input data, relative to the entire capacity of HDFS.

D.When the typical workloads generates a large amount of output data, significantly larger than the amount of intermediate data. 

Explanation:When we encounter applications that produce large amounts of intermediatedata–on the order of the same amount as is read in–we recommend two ports on a single Ethernetcard or two channel-bonded Ethernet cards to provide 2 Gbps per machine. Alternatively for

customers who have already moved to 10 Gigabit Ethernet or Infiniband, these solutions can beused to address network bound workloads. Be sure that your operating system and BIOS arecompatible if you’re considering switching to 10 Gigabit Ethernet. Cloudera’s Support Team Shares Some Basic Hardware Recommendations

--------------------------------------------------------------------------------------------------
Question: 309
What determines the number of Reduces that run a given MapReduce job on a cluster runningMapReduce v1 (MRv1)?

A.It is set by the Hadoop framework and is based on the number of InputSplits of the job.

B.It is set by the developer.

C.It is set by the JobTracker based on the amount of intermediate data.

D.It is set and fixed by the cluster administrator in mapred-site.xml. The number set always run for any submitted job.

--------------------------------------------------------------------------------------------------
Question: 310
Indentify which best defines a SequenceFile?

A.A SequenceFile contains a binary encoding of an arbitrary number of homogeneous Writable objects

B.A SequenceFile contains a binary encoding of an arbitrary number of heterogeneous Writable objects

C.A SequenceFile contains a binary encoding of an arbitrary number of WritableComparable objects, in sorted order.

D.A SequenceFile contains a binary encoding of an arbitrary number key-value pairs. Each key must be the same type. Each value must be the same type. 

--------------------------------------------------------------------------------------------------
Question: 311
client application creates an HDFS file named foo.txt with a replication factor of 3. Identify whichbest describes the file access rules in HDFS if the file has a single block that is stored on datanodes A, B and C?

A.The file will be marked as corrupted if data node B fails during the creation of the file.

B.Each data node locks the local file to prohibit concurrent readers and writers of the file.

C.Each data node stores a copy of the file in the local file system with the same name as the HDFS file.

D.The file can be accessed if at least one of the data nodes storing the file is available.

--------------------------------------------------------------------------------------------------
Question: 312
In a MapReduce job, you want each of your input files processed by a single map task. How doyou configure a MapReduce job so that a single map task processes each input file regardless ofhow many blocks the input file occupies?

A.Increase the parameter that controls minimum split size in the job configuration.

B.Write a custom MapRunner that iterates over all key-value pairs in the entire file.

C.Set the number of mappers equal to the number of input files you want to process.

D.Write a custom FileInputFormat and override the method isSplitable to always return false.

--------------------------------------------------------------------------------------------------
Question: 313
Which process describes the lifecycle of a Mapper?

A.The JobTracker calls the TaskTracker’s configure () method, then its map () method and finally its close () method.

B.The TaskTracker spawns a new Mapper to process all records in a single input split.

C.The TaskTracker spawns a new Mapper to process each key-value pair.

D.The JobTracker spawns a new Mapper to process all records in a single file.

Explanation:For each map instance that runs, the TaskTracker creates a new instance of yourmapper.Note:* The Mapper is responsible for processing Key/Value pairs obtained from the InputFormat. Themapper may perform a number of Extraction and Transformation functions on the Key/Value pairbefore ultimately outputting none, one or many Key/Value pairs of the same, or different Key/Valuetype.* With the new Hadoop API, mappers extend the org.apache.hadoop.mapreduce.Mapper class.This class defines an ‘Identity’ map function by default – every input Key/Value pair obtained fromthe InputFormat is written out.Examining the run() method, we can see the lifecycle of the mapper:/*** Expert users can override this method for more complete control over the* execution of the Mapper.

* @param context* @throws IOException*/public void run(Context context) throws IOException, InterruptedException {setup(context);while (context.nextKeyValue()) {map(context.getCurrentKey(), context.getCurrentValue(), context);}cleanup(context);}setup(Context) – Perform any setup for the mapper. The default implementation is a no-op method.map(Key, Value, Context) – Perform a map operation in the given Key / Value pair. The defaultimplementation calls Context.write(Key, Value)cleanup(Context) – Perform any cleanup for the mapper. The default implementation is a no-opmethod.Reference: Hadoop/MapReduce/Mapper

--------------------------------------------------------------------------------------------------
Question: 314
Determine which best describes when the reduce method is first called in a MapReduce job?

A.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The programmer can configure in the job what percentage of the intermediate data should arrive before the reduce method begins.

B.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The reduce method is called only after all intermediate data has been copied and sorted.

C.Reduce methods and map methods all start at the beginning of a job, in order to provide optimal performance for map-only or reduce-only jobs.

D.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The reduce method is called as soon as the intermediate key-value pairs start to arrive.

Explanation:* In a MapReduce job reducers do not start executing the reduce method until the allMap jobs have completed. Reducers start copying intermediate key-value pairs from the mappersas soon as they are available. The programmer defined reduce method is called only after all themappers have finished.

* Reducers start copying intermediate key-value pairs from the mappers as soon as they areavailable. The progress calculation also takes in account the processing of data transfer which isdone by reduce process, therefore the reduce progress starts showing up as soon as anyintermediate key-value pair for a mapper is available to be transferred to reducer. Though thereducer progress is updated still the programmer defined reduce method is called only after all themappers have finished.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , When is thereducers are started in a MapReduce job?

--------------------------------------------------------------------------------------------------
Question: 315
You have written a Mapper which invokes the following five calls to the OutputColletor.collectmethod:output.collect (new Text (“Apple”), new Text (“Red”) ) ;output.collect (new Text (“Banana”), new Text (“Yellow”) ) ;output.collect (new Text (“Apple”), new Text (“Yellow”) ) ;output.collect (new Text (“Cherry”), new Text (“Red”) ) ;output.collect (new Text (“Apple”), new Text (“Green”) ) ;How many times will the Reducer’s reduce method be invoked?

A.6

B.3

C.1

D.0

E.5

Explanation:reduce() gets called once for each [key, (list of values)] pair. To explain, let’s sayyou called:out.collect(new Text(“Car”),new Text(“Subaru”);out.collect(new Text(“Car”),new Text(“Honda”);out.collect(new Text(“Car”),new Text(“Ford”);out.collect(new Text(“Truck”),new Text(“Dodge”);out.collect(new Text(“Truck”),new Text(“Chevy”);Then reduce() would be called twice with the pairs

reduce(Car, &lt;Subaru, Honda, Ford&gt;)reduce(Truck, &lt;Dodge, Chevy&gt;)Reference: Mapper output.collect()?

--------------------------------------------------------------------------------------------------
Question: 316
To process input key-value pairs, your mapper needs to lead a 512 MB data file in memory. Whatis the best way to accomplish this?

A.Serialize the data file, insert in it the JobConf object, and read the data into memory in the configure method of the mapper.

B.Place the data file in the DistributedCache and read the data into memory in the map method of the mapper.

C.Place the data file in the DataCache and read the data into memory in the configure method of the mapper.

D.Place the data file in the DistributedCache and read the data into memory in the configure method of the mapper.

Explanation:Hadoop has a distributed cache mechanism to make available file locally that maybe needed by Map/Reduce jobsUse CaseLets understand our Use Case a bit more in details so that we can follow-up the code snippets.We have a Key-Value file that we need to use in our Map jobs. For simplicity, lets say we need toreplace all keywords that we encounter during parsing, with some other value.So what we need isA key-values files (Lets use a Properties files)The Mapper code that uses the codeWrite the Mapper code that uses itview sourceprint?01.public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {02.

03.Properties cache;04.05.@Override06.protected void setup(Context context) throws IOException, InterruptedException {07.super.setup(context);08.Path[] localCacheFiles = DistributedCache.getLocalCacheFiles(context.getConfiguration());09.10.if(localCacheFiles != null) {11.// expecting only single file here12.for (int i = 0; i &lt; localCacheFiles.length; i++) {13.Path localCacheFile = localCacheFiles[i];14.cache = new Properties();15.cache.load(new FileReader(localCacheFile.toString()));16.}17.} else {18.// do your error handling here19.}20.21.}22.

23.@Override24.public void map(LongWritable key, Text value, Context context) throws IOException,InterruptedException {25.// use the cache here26.// if value contains some attribute, cache.get(&lt;value&gt;)27.// do some action or replace with something else28.}29.30.}Note:* Distribute application-specific large, read-only files efficiently.DistributedCache is a facility provided by the Map-Reduce framework to cache files (text, archives,jars etc.) needed by applications.Applications specify the files, via urls (hdfs:// or http://) to be cached via the JobConf. TheDistributedCache assumes that the files specified via hdfs:// urls are already present on theFileSystem at the path specified by the url.Reference: Using Hadoop Distributed Cache

--------------------------------------------------------------------------------------------------
Question: 317
In a MapReduce job, the reducer receives all values associated with same key. Which statementbest describes the ordering of these values?

A.The values are in sorted order.

B.The values are arbitrarily ordered, and the ordering may vary from run to run of the same MapReduce job.

C.The values are arbitrary ordered, but multiple runs of the same MapReduce job will always have the same ordering.

D.Since the values come from mapper outputs, the reducers will receive contiguous sections of sorted values.

--------------------------------------------------------------------------------------------------
Question: 318
You need to create a job that does frequency analysis on input data. You will do this by writing aMapper that uses TextInputFormat and splits each value (a line of text from an input file) intoindividual characters. For each one of these characters, you will emit the character as a key andan InputWritable as the value. As this will produce proportionally more intermediate data than inputdata, which two resources should you expect to be bottlenecks?

A.Processor and network I/O

B.Disk I/O and network I/O

C.Processor and RAM

D.Processor and disk I/O

--------------------------------------------------------------------------------------------------
Question: 319
Your client application submits a MapReduce job to your Hadoop cluster. Identify the Hadoopdaemon on which the Hadoop framework will look for an available slot schedule a MapReduceoperation.

A.TaskTracker

B.NameNode

C.DataNode

D.JobTracker

E.Secondary NameNode

--------------------------------------------------------------------------------------------------
Question: 320
Under which of the following scenarios would it be most appropriate to consider using faster (e.g.,10 Gigabit) Ethernet as the network fabric for your Hadoop cluster?

A.When the typical workload consists of processor-intensive tasks.

B.When the typical workload consumes a large amount of input data, relative to the entire Canada of HDFS.

C.When the typical workload generates a large amount of intermediate data, on the order of the input data itself.

D.When the typical workload generates a large amount of output data, significantly larger than the amount of intermediate data.

--------------------------------------------------------------------------------------------------
Question: 321
It is recommended that you run the HDFS balancer periodically. Why? (Choose 3)

A.To improve data locality for MapReduce tasks.

B.To ensure that there is capacity in HDTS for additional data.

C.To help HDFS deliver consistent performance under heavy loads.

D.To ensure that all blocks in the cluster are 128MB in size.

E.To ensure that there is consistent disk utilization across the DataNodes.

--------------------------------------------------------------------------------------------------
Question: 322
What is the Secondary NameNode?

A.An alternate data channel for clients to reach HDFS, should the NameNode become too busy.

B.A process that performs a checkpoint operation on the files produced by the NameNode.

C.A data channel between the primary name node and the tertiary NameNode.

D.A process purely intended to perform backups of the NameNode.

E.A standby NameNode, for high availability.

--------------------------------------------------------------------------------------------------
Question: 323
Someone in your data center unplugs a slave node by accident. Users of the cluster notice via theHadoop Web UI that the cluster size has shrunken and express concerns about data loss andHDFS performance. The replication factor of all the files in the cluster is unchanged from thedefault of 3. What can you tell the users?

A.The HDFS filesystem is corrupt until the administrator re adds the DataNode to the cluster. The warnings associated with the event should be reported.

B.After identifying the outage, the NameNode will naturally re-replicate the data and there will be no data loss. The administrator can re-add the DataNode at any time. The client can disregard warnings concerned with this event. Data will be under-replicated but will become properly replicated over time.

C.The NameNode will re replicate the data after the administrator issues a special command. The data is not lost but is underreplicated until the administrator issues this command.

D.The NameNode will identify the outage and re-replicate the data when the clients receive connection failures to the DataNode, so the end users can disregard such warnings.

--------------------------------------------------------------------------------------------------
Question: 324
How does the HDFS architecture provide redundancy?

A.Storing multiple replicas of data blocks on different DataNodes.

B.Reliance on RAID at each datanode.

C.Reliance on SAN devices as a DataNode interface.

D.DataNodes make copies of their data blocks, and put them on different local disks.

--------------------------------------------------------------------------------------------------
Question: 325
You have a cluster running with the Fair Scheduler enabled and configured. You submit multiplejobs to cluster. Each job is assigned to a pool. How are jobs scheduled? (Choose 2)

A.Each pool’s share of task slots may change throughout the course of job execution.

B.Pools get a dynamically-allocated share of the available task slots (subject to additional constraints).

C.Each pool gets 1/M of the total available task slots, where M is the number of nodes in the cluster

D.Pools are assigned priorities. Pools with higher priorities are executed before pools with lower priorities.

E.Each pool gets 1/N of the total available task slots, where N is the number of jobs running on the cluster.

F.Each pool’s share of task slots remains static within the execution of any individual job.

--------------------------------------------------------------------------------------------------
Question: 326
You have just run a MapReduce job to filter user messages to only those of a selectedgeographical region. The output for this job is in a directory named westUsers, located just belowyour home directory in HDFS. Which command gathers these into a single file on your local filesystem?

A.Hadoop fs –getmerge –R westUsers.txt

B.Hadoop fs –getemerge westUsers westUsers.txt

C.Hadoop fs –cp westUsers/* westUsers.txt

D.Hadoop fs –get westUsers westUsers.txt

--------------------------------------------------------------------------------------------------
Question: 327
In CDH4 and later, which file contains a serialized form of all the directory and files inodes in thefilesystem, giving the NameNode a persistent checkpoint of the filesystem metadata?

A.fstime

B.VERSION

C.Fsimage_N (where N reflects transactions up to transaction ID N)

D.Edits_N-M (where N-M transactions between transaction ID N and transaction ID N)

--------------------------------------------------------------------------------------------------
Question: 328
You are running a Hadoop cluster with a NameNode on host mynamenode. What are two ways todetermine available HDFS space in your cluster?

A.Run hdfs fs –du / and locate the DFS Remaining value

B.Run hdfs dfsadmin –report and locate the DFS Remaining value

C.Run hdfs dfs / and subtract NDFS Used from configured Capacity

D.Connect to http://mynamenode:50070/dfshealth.jsp and locate the DFS remaining value

--------------------------------------------------------------------------------------------------
Question: 329
You have recently converted your Hadoop cluster from a MapReduce 1 (MRv1) architecture toMapReduce 2 (MRv2) on YARN architecture. Your developers are accustomed to specifying mapand reduce tasks (resource allocation) tasks when they run jobs: A developer wants to know howspecify to reduce tasks when a specific job runs. Which method should you tell that developers toimplement?

A.MapReduce version 2 (MRv2) on YARN abstracts resource allocation away from the idea of “tasks” into memory and virtual cores, thus eliminating the need for a developer to specify the number of reduce tasks, and indeed preventing the developer from specifying the number of reduce tasks.

B.In YARN, resource allocations is a function of megabytes of memory in multiples of 1024mb. Thus, they should specify the amount of memory resource they need by executing –D mapreducereduces.memory-mb-2048

C.In YARN, the ApplicationMaster is responsible for requesting the resource required for a specific launch. Thus, executing –D yarn.applicationmaster.reduce.tasks=2 will specify that the ApplicationMaster launch two task contains on the worker nodes.

D.Developers specify reduce tasks in the exact same way for both MapReduce version 1 (MRv1) and MapReduce version 2 (MRv2) on YARN. Thus, executing –D mapreduce.job.reduces-2 will specify reduce tasks.

E.In YARN, resource allocation is function of virtual cores specified by the ApplicationManager making requests to the NodeManager where a reduce task is handeled by a single container (and thus a single virtual core). Thus, the developer needs to specify the number of virtual cores to the NodeManager by executing –p yarn.nodemanager.cpu-vcores=2

--------------------------------------------------------------------------------------------------
Question: 330
Your Hadoop cluster contains nodes in three racks. You have not configured the dfs.hostsproperty in the NameNode’s configuration file. What results?

A.The NameNode will update the dfs.hosts property to include machines running the DataNode daemon on the next NameNode reboot or with the command dfsadmin –refreshNodes

B.No new nodes can be added to the cluster until you specify them in the dfs.hosts file

C.Any machine running the DataNode daemon can immediately join the cluster

D.Presented with a blank dfs.hosts property, the NameNode will permit DataNodes specified in mapred.hosts to join the cluster

--------------------------------------------------------------------------------------------------
Question: 331
You are running a Hadoop cluster with MapReduce version 2 (MRv2) on YARN. You consistentlysee that MapReduce map tasks on your cluster are running slowly because of excessive garbagecollection of JVM, how do you increase JVM heap size property to 3GB to optimize performance?

A.yarn.application.child.java.opts=-Xsx3072m

B.yarn.application.child.java.opts=-Xmx3072m

C.mapreduce.map.java.opts=-Xms3072m

D.mapreduce.map.java.opts=-Xmx3072m

--------------------------------------------------------------------------------------------------
Question: 332
You have a cluster running with a FIFO scheduler enabled. You submit a large job A to the cluster,which you expect to run for one hour. Then, you submit job B to the cluster, which you expect torun a couple of minutes only.You submit both jobs with the same priority.Which two best describes how FIFO Scheduler arbitrates the cluster resources for job and itstasks?

A.Because there is a more than a single job on the cluster, the FIFO Scheduler will enforce a limit on the percentage of resources allocated to a particular job at any given time

B.Tasks are scheduled on the order of their job submission

C.The order of execution of job may vary

D.Given job A and submitted in that order, all tasks from job A are guaranteed to finish before all tasks from job B

E.The FIFO Scheduler will give, on average, and equal share of the cluster resources over the job lifecycle

F.The FIFO Scheduler will pass an exception back to the client when Job B is submitted, since all slots on the cluster are use

--------------------------------------------------------------------------------------------------
Question: 333
user comes to you, complaining that when she attempts to submit a Hadoop job, it fails. There isa Directory in HDFS named /data/input. The Jar is named j.jar, and the driver class is namedDriverClass.

She runs the command:Hadoop jar j.jar DriverClass /data/input/data/outputThe error message returned includes the line:PriviligedActionException as:training (auth:SIMPLE)cause:org.apache.hadoop.mapreduce.lib.input.invalidInputException:Input path does not exist: file:/data/inputWhat is the cause of the error?

A.The user is not authorized to run the job on the cluster

B.The output directory already exists

C.The name of the driver has been spelled incorrectly on the command line

D.The directory name is misspelled in HDFS

E.The Hadoop configuration files on the client do not point to the cluster

--------------------------------------------------------------------------------------------------
Question: 334
Your company stores user profile records in an OLTP databases. You want to join these recordswith web server logs you have already ingested into the Hadoop file system. What is the best wayto obtain and ingest these user records?

A.Ingest with Hadoop streaming

B.Ingest using Hive’s IQAD DATA command

C.Ingest with sqoop import

D.Ingest with Pig’s LOAD command

E.Ingest using the HDFS put command

--------------------------------------------------------------------------------------------------
Question: 335
Which two are features of Hadoop’s rack topology?

A.Configuration of rack awareness is accomplished using a configuration file. You cannot use a rack topology script.

B.Hadoop gives preference to intra-rack data transfer in order to conserve bandwidth

C.Rack location is considered in the HDFS block placement policy

D.HDFS is rack aware but MapReduce daemon are not

E.Even for small clusters on a single rack, configuring rack awareness will improve performance

--------------------------------------------------------------------------------------------------
Question: 336
You want to build a classification model to identify spam comments on a blog. You decide to usethe words in the comment text as inputs to your model. Which criteria should you use whendeciding which words to use as features in order to contribute to making the correct classificationdecision?

A.Choose words for your sample that are most correlated with the Spam label

B.Choose wordsfor your sample thatoccur most frequently in the text

C.Choose words, for your sample that have the largest mutual information with the spam label

D.Choose words for your sample that are least correlated with the spam label

--------------------------------------------------------------------------------------------------
Question: 337
Given the following sample of numbers from a distribution:1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89What are the five numbers that summarize this distribution (the five number summary of samplepercentiles)?

A.1, 3, 8, 34, 89

B.1, 4, 13, 34, 89

C.1, 1.5, 5, 24.5, 89

D.1, 2.5, 8, 27.5, 89

--------------------------------------------------------------------------------------------------
Question: 338
Given the following sample of numbers from a distribution:1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89What are two benefits of using the five-number summary of sample percentiles to summarize adata set?

A.You can calculate unbiased estimators for the parameters of the distribution

B.It’s robust to outliers

C.It’s well-defined for any probability distribution

D.You can calculate it quickly using a relational database like MySQL, even when we have a large sample

--------------------------------------------------------------------------------------------------
Question: 339
Given the following sample of numbers from a distribution:1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89How do high-level languages like Apache Hive and Apache Pig efficiently calculate approximatelypercentiles for a distribution?

A.They sort all of the input samples and the lookup the samples for each percentile

B.They maintain index of input data as it is loaded into HDFS and load them into memory

C.They use pivots to assign each observations to the reducer that calculate each percentile

D.They assign sample observations to buckets and then aggregate the buckets to compute the approximations

--------------------------------------------------------------------------------------------------
Question: 340
What is the best way to determine the learning rate parameters for stochastic gradient descentwhen the distribution of the input data shifts over time?

A.The learning rate should be adjusted periodically based on the setting that optimizes the objective function over a sample of recent observations

B.The learning rate should be fixed number that decays as the number of observations in the data set increases

C.The learning rate should be the value that optimizes the value of the objective function over the first N samples in the dataset

D.The learning rate should be a fixed number with a constant decay factor

E.The learning rate should be continuously adjusted based on the value that optimizes the objective function for the most recent observation from the input data

--------------------------------------------------------------------------------------------------
Question: 341
Which two machine learning algorithm should you consider as likely to benefit from discretizingcontinuous features?

A.Support vector machine

B.Naïve Bayes

C.Decision trees

D.Logistic regression

E.Singular value decomposition

--------------------------------------------------------------------------------------------------
Question: 342
You’ve built a model that has ten different variables with complicated independence relationshipsbetween them, and both continuous and discrete variables that have complicated, multi-parameterdistributions.Computing the joint probability distribution is complex, but it turns out that computing theconditional probabilities for the variables is easy. What is the most computationally efficient for

computing the expected value?

A.Method of moments

B.Markov Chain Monte Carlo

C.Gibbs sampling

D.Numerical quadrature

--------------------------------------------------------------------------------------------------
Question: 343
What is one limitation encountered by all systems that employ collaborative filtering and usepreferences as input. In order to output product recommendations to consumers?

A.Consumers do not have stable ratings for the same product over time

B.There are too many consumers and too few products

C.Not every product has been rated by every consumer

D.There are too few consumers and too many products

--------------------------------------------------------------------------------------------------
Question: 344
Why is the naive Bayes classifier “naive”?

A.It generally performs worsethan more complex methods

B.It Is an unbiased estimator

C.It assumes Independence between all features

D.It makes no assumptions on the underlying distributions (i.e., it is non-parametric)

--------------------------------------------------------------------------------------------------
Question: 345
Which three metrics are useful in measuring the accuracy and quality of a recommender system?

A.Mutual Information

B.RMSF

C.Tanimoto coefficient

D.Pearson correlation

E.Precision

F.Recall

--------------------------------------------------------------------------------------------------
Question: 346
You have a “Users” table in HBase and you would like to insert a row that consists of a UserID,“jsmith70” and an email address, “jane@example.com”. The table has a single Column Familynamed “Meta” and the row key will be the user’s ID. The shell command you should use tocomplete this is:

A.put ‘Users’, ‘jsmith70’, ‘jane@example.com’

B.put ‘Users’, ‘UserID:jsmith70’, ‘Email:jane@example.com’

C.put ‘Users’, ‘jsmith70’, ‘Meta:Email’, ‘jane@example.com’

D.put ‘Users’, ‘Meta:UserID’, ‘jsmith70’, ‘Meta:Email, ‘jane@example.com’

--------------------------------------------------------------------------------------------------
Question: 347
You have tin linage table live in production. The table users &lt;timestamp&gt; as the rowkey. You wantto change the existing rowkeys to &lt;userid&gt;&lt;timestamp&gt;. Which of the following should you do?

A.Modify the client application to write to both the old table and a new table while migrating the old data separately

B.Use the ALTER table command to modify the rowkeys

C.Use the ASSIGN command to modify the rowkeys

D.Add a new column to store the userid

--------------------------------------------------------------------------------------------------
Question: 348
Your client application calls the following method for all puts to the single table notifications:‘put.setWriteToWAL, (false);One region, region1, for the notifications table is assigned to RegionServer rs1. Which of thefollowing statements describes the result of RegionServer rs1 crashing?

A.All data in the notifications table is lost

B.No data is lost

C.All data for all tables not flushed to disk on RegionServer rs1 is lost

D.Data for your client application in the MemStores for region1 is lost

--------------------------------------------------------------------------------------------------
Question: 349
You need to insert a cell with a specific timestamp (version) 13353903160532. Which of thefollowing is true?

A.The timestamp for the entire row must be updated to 13353903160532

B.The Put class allows setting a cell specific timestamp

C.The Put class allows setting a column family-specific timestamp

D.The HTable class allows you to temporarily roll back the newer versions of the cell

--------------------------------------------------------------------------------------------------
Question: 350
Which of the following describes how a client reads a file from HDFS?

A.The client queries the NameNode for the block location(s). The NameNode returns the block location(s) to the client. The client reads the data directly off the DataNode(s).

B.The client queries all DataNodes in parallel. The DataNode that contains the requested data responds directly to the client. The client reads the data directly off the DataNode.

C.The client contacts the NameNode for the block location(s). The NameNode then queries the DataNodes for block locations. The DataNodes respond to the NameNode, and the NameNode redirects the client to the DataNode that holds the requested data block(s). The client then reads the data directly off the DataNode.

D.The client contacts the NameNode for the block location(s). The NameNode contacts the DataNode that holds the requested data block. Data is transferred from the DataNode to the NameNode, and then from the NameNode to the client.

--------------------------------------------------------------------------------------------------
Question: 351
Which of the following statements best describes how a large (100 GB) file is stored in HDFS?

A.The file is divided into variable size blocks, which are stored on multiple data nodes. Each block is replicated three times by default.

B.The file is replicated three times by default. Each copy of the file is stored on a separate datanodes.

C.The master copy of the file is stored on a single datanode. The replica copies are divided into fixed-size blocks, which are stored on multiple datanodes.

D.The file is divided into fixed-size blocks, which are stored on multiple datanodes. Each block is replicated three times by default. Multiple blocks from the same file might reside on the same datanode.

E.The file is divided into fixed-size blocks, which are stored on multiple datanodes. Each block is replicated three times by default. HDFS guarantees that different blocks from the same file are never on the same datanode.

Explanation:HDFS is designed to reliably store very large files across machines in a largecluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are thesame size. The blocks of a file are replicated for fault tolerance. The block size and replicationfactor are configurable per file. An application can specify the number of replicas of a file. Thereplication factor can be specified at file creation time and can be changed later. Files in HDFS arewrite-once and have strictly one writer at any time. The NameNode makes all decisions regarding

replication of blocks. HDFS uses rack-aware replica placement policy. In default configurationthere are total 3 copies of a datablock on HDFS, 2 copies are stored on datanodes on same rackand 3rd copy on a different rack.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , How theHDFS Blocks are replicated?

--------------------------------------------------------------------------------------------------
Question: 352
Your cluster has 10 DataNodes, each with a single 1 TB hard drive. You utilize all your diskcapacity for HDFS, reserving none for MapReduce. You implement default replication settings.What is the storage capacity of your Hadoop cluster (assuming no compression)?

A.about 3 TB

B.about 5 TB

C.about 10 TB

D.about 11 TB

--------------------------------------------------------------------------------------------------
Question: 353
user comes to you, complaining that when she attempts to submit a Hadoop job, it fails.There is a directory in HDFS named /data/input. The Jar is named j.jar, and the driver classis named DriverClass. She runs command: hadoop jar j.jar DriverClass/data/input/data/output The error message returned includes the line:PrivilegedActionException as:training (auth:SIMPLE)cause.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exits:file :/data/input What is the cause of the error?

A. The Hadoop configuration files on the client do not point to the cluster

B. The directory name is misspelled in HDFS

C. The name of the driver has been spelled incorrectly on the command line

D. The output directory already exists

E. The user is not authorized to run the job on the cluster

--------------------------------------------------------------------------------------------------
Question: 354
In CDH4 and later, which file contains a serialized form of all the directory and files inodes inthe filesystem, giving the NameNode a persistent checkpoint of the filesystem metadata?

A. VERSION

B. Fsimage_N (Where N reflects all transactions up to transaction ID N)

C. fstime

D. Edits_N-M (Where N-M specifies transactions between transactions ID N and transaction ID N)

--------------------------------------------------------------------------------------------------
Question: 355
Which process instantiates user code, and executes map and reduce tasks on a clusterrunning MapReduce V2 (MRv2) on YARN?

A. NodeManager

B. ApplicationMaster

C. ResourceManager

D. TaskTracker

E. JobTracker

F. DataNode

G. NameNode

--------------------------------------------------------------------------------------------------
Question: 356
You are migrating a cluster from MapReduce version 1 (MRv1) to MapReduce version2(MRv2) on YARN. To want to maintain your MRv1 TaskTracker slot capacities when youmigrate. What should you do?

A. Configure mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum ub yarn.site.xml to match your cluster’s configured capacity set by yarn.scheduler.minimum-allocation

B. Configure yarn.applicationmaster.resource.memory-mb and yarn.applicationmaster.cpuvcores so that ApplicationMaster container allocations match the capacity you require.

C. You don’t need to configure or balance these properties in YARN as YARN dynamically balances resource management capabilities on your cluster

D. Configure yarn.nodemanager.resource.memory-mb and 

yarn.nodemanager.resource.cpu-vcores to match the capacity you require under YARN for each NodeManager</div>]
--------------------------------------------------------------------------------------------------
Question: 357
user comes to you, complaining that when she attempts to submit a Hadoop job, it fails. There isa directory in HDFS named /data/input. The Jar is named j.jar, and the driver class is namedDriverClass. She runs command:hadoop jar j.jar DriverClass /data/input/data/outputThe error message returned includes the line:PrivilegedActionException as:training (auth:SIMPLE)cause.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exits: file:/data/inputWhat is the cause of the error?

A.The Hadoop configuration files on the client do not point to the cluster

B.The directory name is misspelled in HDFS

C.The name of the driver has been spelled incorrectly on the command line

D.The output directory already exists

E.The user is not authorized to run the job on the cluster

--------------------------------------------------------------------------------------------------
Question: 358
In CDH4 and later, which file contains a serialized form of all the directory and files inodes in thefilesystem, giving the NameNode a persistent checkpoint of the filesystem metadata?

A.fstime

B.VERSION

C.Fsimage_N (Where N reflects all transactions up to transaction ID N)

D.Edits_N-M (Where N-M specifies transactions between transactions ID N and transaction ID N)

--------------------------------------------------------------------------------------------------
Question: 359
Which process instantiates user code, and executes map and reduce tasks on a cluster runningMapReduce V2 (MRv2) on YARN?

A.NodeManager

B.ApplicationMaster

C.ResourceManager

D.TaskTracker

E.JobTracker

F.DataNode

G.NameNode

--------------------------------------------------------------------------------------------------
Question: 360
You are migrating a cluster from MapReduce version 1 (MRv1) to MapReduce version2 (MRv2) onYARN. To want to maintain your MRv1 TaskTracker slot capacities when you migrate. Whatshould you do?

A.Configure yarn.applicationmaster.resource.memory-mb and yarn.applicationmaster.cpu-vcores so that ApplicationMaster container allocations match the capacity you require.

B.You don’t need to configure or balance these properties in YARN as YARN dynamically balances resource management capabilities on your cluster

C.Configure yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpuvcores to match the capacity you require under YARN for each NodeManager

D.Configure mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum ub yarn.site.xml to match your cluster’s configured capacity set by yarn.scheduler.minimum-allocation

--------------------------------------------------------------------------------------------------
Question: 361
Which project gives you a distributed, Scalable, data store that allows you random, realtimeread/write access to hundreds of terabytes of data?

A.HBase

B.Hue

C.Pig

D.Hive

E.Oozie

F.Flume

G.Sqoop

--------------------------------------------------------------------------------------------------
