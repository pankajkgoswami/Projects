Answer1 : B
--------------------------------------------------------------------------------------------------
Answer2: A.HBase

Explanation:Use Apache HBase when you need random, realtime read/write access to your BigData.Note: This project’s goal is the hosting of very large tables — billions of rows X millions of columns— atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned,column-oriented store modeled after Google’s Bigtable: A Distributed Storage System forStructured Data by Chang et al. Just as Bigtable leverages the distributed data storage providedby the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoopand HDFS.FeaturesLinear and modular scalability.Strictly consistent reads and writes.Automatic and configurable sharding of tablesAutomatic failover support between RegionServers.Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.Easy to use Java API for client access.Block cache and Bloom Filters for real-time queries.Query predicate push down via server side FiltersThrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary dataencoding optionsExtensible jruby-based (JIRB) shellSupport for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMXReference: http://hbase.apache.org/ (when would I use HBase? First sentence)
--------------------------------------------------------------------------------------------------
Answer3: C.Configure yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpuvcores to match the capacity you require under YARN for each NodeManager
--------------------------------------------------------------------------------------------------
Answer4: C.Fsimage_N (Where N reflects all transactions up to transaction ID N)

Explanation:http://mikepluta.com/tag/namenode/
--------------------------------------------------------------------------------------------------
Answer5: A.about 3 TB

Explanation:In default configuration there are total 3 copies of a datablock on HDFS, 2 copiesare stored on datanodes on same rack and 3rd copy on a different rack.Note: HDFS is designed to reliably store very large files across machines in a large cluster. Itstores each file as a sequence of blocks; all blocks in a file except the last block are the samesize. The blocks of a file are replicated for fault tolerance. The block size and replication factor areconfigurable per file. An application can specify the number of replicas of a file. The replicationfactor can be specified at file creation time and can be changed later. Files in HDFS are write-onceand have strictly one writer at any time. The NameNode makes all decisions regarding replicationof blocks. HDFS uses rack-aware replica placement policy.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , How theHDFS Blocks are replicated?
--------------------------------------------------------------------------------------------------
Answer 6. Explanation:HDFS is designed to reliably store very large files across machines in a largecluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are thesame size. The blocks of a file are replicated for fault tolerance. The block size and replicationfactor are configurable per file. An application can specify the number of replicas of a file. Thereplication factor can be specified at file creation time and can be changed later. Files in HDFS arewrite-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. HDFS uses rack-aware replica placement policy. In default configurationthere are total 3 copies of a datablock on HDFS, 2 copies are stored on datanodes on same rackand 3rd copy on a different rack.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , How theHDFS Blocks are replicated?

Answer: E.The file is divided into fixed-size blocks, which are stored on multiple datanodes. Each block is replicated three times by default. HDFS guarantees that different blocks from the same file are never on the same datanode.
--------------------------------------------------------------------------------------------------
Answer 7: C.The client contacts the NameNode for the block location(s). The NameNode then queries the DataNodes for block locations. The DataNodes respond to the NameNode, and the NameNode redirects the client to the DataNode that holds the requested data block(s). The client then reads the data directly off the DataNode.

Explanation:The Client communication to HDFS happens using Hadoop HDFS API. Clientapplications talk to the NameNode whenever they wish to locate a file, or when they want toadd/copy/move/delete a file on HDFS. The NameNode responds the successful requests byreturning a list of relevant DataNode servers where the data lives. Client applications can talkdirectly to a DataNode, once the NameNode has provided the location of the data.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, How the Clientcommunicates with HDFS?

--------------------------------------------------------------------------------------------------
Answer 8: E.Even for small clusters on a single rack, configuring rack awareness will improve performance

--------------------------------------------------------------------------------------------------
Answer 9: D.Ingest with Pig’s LOAD command

--------------------------------------------------------------------------------------------------
Answer 10: D.Connect to http://mynamenode:50070/dfshealth.jsp and locate the DFS remaining value
--------------------------------------------------------------------------------------------------
Answer 11 : B.Hadoop fs –getemerge westUsers westUsers.txt
--------------------------------------------------------------------------------------------------
Answer 12: A.Storing multiple replicas of data blocks on different DataNodes.

Explanation:http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-thenetwork/(writing files to HDFS)
--------------------------------------------------------------------------------------------------

Answer 13: B.After identifying the outage, the NameNode will naturally re-replicate the data and there will be no data loss. The administrator can re-add the DataNode at any time. The client can disregard warnings concerned with this event. Data will be under-replicated but will become properly replicated over time.

--------------------------------------------------------------------------------------------------
Answer 14: B.A process that performs a checkpoint operation on the files produced by the NameNode.

Explanation:http://wiki.apache.org/hadoop/FAQ#What_is_the_purpose_of_the_secondary_namenode.3F(3.2)

--------------------------------------------------------------------------------------------------
Answer 15: E.To ensure that there is consistent disk utilization across the DataNodes.

Explanation:http://hadoop.apache.org/docs/hdfs/r0.21.0/api/org/apache/hadoop/hdfs/server/balancer/Balance.html

--------------------------------------------------------------------------------------------------
Answer 16 : B.Disk I/O and network I/O

--------------------------------------------------------------------------------------------------
Answer 17: B.The values are arbitrarily ordered, and the ordering may vary from run to run of the same MapReduce job.

Explanation:Note: * Input to the Reducer is the sorted output of the mappers. * The framework calls the application’s Reduce function once for each unique key in the sortedorder.* Example:For the given sample input the first map emits:< Hello, 1> < World, 1> < Bye, 1> < World, 1> The second map emits:< Hello, 1> < Hadoop, 1> < Goodbye, 1> < Hadoop, 1>
--------------------------------------------------------------------------------------------------
Answer 18 :
Explanation:Hadoop has a distributed cache mechanism to make available file locally that maybe needed by Map/Reduce jobsUse CaseLets understand our Use Case a bit more in details so that we can follow-up the code snippets.We have a Key-Value file that we need to use in our Map jobs. For simplicity, lets say we need toreplace all keywords that we encounter during parsing, with some other value.So what we need isA key-values files (Lets use a Properties files)The Mapper code that uses the codeWrite the Mapper code that uses itview sourceprint?01.public class DistributedCacheMapper extends Mapper<LongWritable, Text, Text, Text> {02.

03.Properties cache;04.05.@Override06.protected void setup(Context context) throws IOException, InterruptedException {07.super.setup(context);08.Path[] localCacheFiles = DistributedCache.getLocalCacheFiles(context.getConfiguration());09.10.if(localCacheFiles != null) {11.// expecting only single file here12.for (int i = 0; i < localCacheFiles.length; i++) {13.Path localCacheFile = localCacheFiles[i];14.cache = new Properties();15.cache.load(new FileReader(localCacheFile.toString()));16.}17.} else {18.// do your error handling here19.}20.21.}22.

23.@Override24.public void map(LongWritable key, Text value, Context context) throws IOException,InterruptedException {25.// use the cache here26.// if value contains some attribute, cache.get(<value>)27.// do some action or replace with something else28.}29.30.}Note:* Distribute application-specific large, read-only files efficiently.DistributedCache is a facility provided by the Map-Reduce framework to cache files (text, archives,jars etc.) needed by applications.Applications specify the files, via urls (hdfs:// or http://) to be cached via the JobConf. TheDistributedCache assumes that the files specified via hdfs:// urls are already present on theFileSystem at the path specified by the url.Reference: Using Hadoop Distributed Cache

Answer: B.Place the data file in the DistributedCache and read the data into memory in the map method of the mapper.
--------------------------------------------------------------------------------------------------
Explanation:reduce() gets called once for each [key, (list of values)] pair. To explain, let’s sayyou called:out.collect(new Text(“Car”),new Text(“Subaru”);out.collect(new Text(“Car”),new Text(“Honda”);out.collect(new Text(“Car”),new Text(“Ford”);out.collect(new Text(“Truck”),new Text(“Dodge”);out.collect(new Text(“Truck”),new Text(“Chevy”);Then reduce() would be called twice with the pairs

reduce(Car, <Subaru, Honda, Ford>)reduce(Truck, <Dodge, Chevy>)Reference: Mapper output.collect()?

Answer 19 : B.3
--------------------------------------------------------------------------------------------------
Explanation:* In a MapReduce job reducers do not start executing the reduce method until the allMap jobs have completed. Reducers start copying intermediate key-value pairs from the mappersas soon as they are available. The programmer defined reduce method is called only after all themappers have finished.

* Reducers start copying intermediate key-value pairs from the mappers as soon as they areavailable. The progress calculation also takes in account the processing of data transfer which isdone by reduce process, therefore the reduce progress starts showing up as soon as anyintermediate key-value pair for a mapper is available to be transferred to reducer. Though thereducer progress is updated still the programmer defined reduce method is called only after all themappers have finished.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , When is thereducers are started in a MapReduce job?

Answer 20 : D.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The reduce method is called as soon as the intermediate key-value pairs start to arrive.


--------------------------------------------------------------------------------------------------
Explanation:For each map instance that runs, the TaskTracker creates a new instance of yourmapper.Note:* The Mapper is responsible for processing Key/Value pairs obtained from the InputFormat. Themapper may perform a number of Extraction and Transformation functions on the Key/Value pairbefore ultimately outputting none, one or many Key/Value pairs of the same, or different Key/Valuetype.* With the new Hadoop API, mappers extend the org.apache.hadoop.mapreduce.Mapper class.This class defines an ‘Identity’ map function by default – every input Key/Value pair obtained fromthe InputFormat is written out.Examining the run() method, we can see the lifecycle of the mapper:/*** Expert users can override this method for more complete control over the* execution of the Mapper.

* @param context* @throws IOException*/public void run(Context context) throws IOException, InterruptedException {setup(context);while (context.nextKeyValue()) {map(context.getCurrentKey(), context.getCurrentValue(), context);}cleanup(context);}setup(Context) – Perform any setup for the mapper. The default implementation is a no-op method.map(Key, Value, Context) – Perform a map operation in the given Key / Value pair. The defaultimplementation calls Context.write(Key, Value)cleanup(Context) – Perform any cleanup for the mapper. The default implementation is a no-opmethod.Reference: Hadoop/MapReduce/Mapper

Answer 21: C.The TaskTracker spawns a new Mapper to process each key-value pair.
--------------------------------------------------------------------------------------------------
Answer 22: D.Write a custom FileInputFormat and override the method isSplitable to always return false.

Explanation:FileInputFormat is the base class for all file-based InputFormats. This provides ageneric implementation of getSplits(JobContext). Subclasses of FileInputFormat can also overridethe isSplitable(JobContext, Path) method to ensure input-files are not split-up and are processedas a whole by Mappers.Reference: org.apache.hadoop.mapreduce.lib.input, Class FileInputFormat<K,V>

--------------------------------------------------------------------------------------------------
Answer 23: D.The file can be accessed if at least one of the data nodes storing the file is available.

Explanation:HDFS keeps three copies of a block on three different datanodes to protect againsttrue data corruption. HDFS also tries to distribute these three replicas on more than one rack toprotect against data availability issues. The fact that HDFS actively monitors any faileddatanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed)implies that three copies of data on three different nodes is sufficient to avoid corrupted files.Note: HDFS is designed to reliably store very large files across machines in a large cluster. It storeseach file as a sequence of blocks; all blocks in a file except the last block are the same size. Theblocks of a file are replicated for fault tolerance. The block size and replication factor areconfigurable per file. An application can specify the number of replicas of a file. The replicationfactor can be specified at file creation time and can be changed later. Files in HDFS are write-onceand have strictly one writer at any time. The NameNode makes all decisions regarding replicationof blocks. HDFS uses rack-aware replica placement policy. In default configuration there are total3 copies of a datablock on HDFS, 2 copies are stored on datanodes on same rack and 3rd copyon a different rack.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , How theHDFS Blocks are replicated?

--------------------------------------------------------------------------------------------------
Answer 24: D.A SequenceFile contains a binary encoding of an arbitrary number key-value pairs. Each key must be the same type. Each value must be the same type. 

Explanation:SequenceFile is a flat file consisting of binary key/value pairs.There are 3 different SequenceFile formats:Uncompressed key/value records.Record compressed key/value records – only ‘values’ are compressed here.Block compressed key/value records – both keys and values are collected in ‘blocks’ separatelyand compressed. The size of the ‘block’ is configurable.Reference: http://wiki.apache.org/hadoop/SequenceFile

--------------------------------------------------------------------------------------------------
Answer 25: B.It is set by the developer.

Explanation:Number of ReducesThe right number of reduces seems to be 0.95 or 1.75 * (nodes *mapred.tasktracker.tasks.maximum). At 0.95 all of the reduces can launch immediately and starttransfering map outputs as the maps finish. At 1.75 the faster nodes will finish their first round ofreduces and launch a second round of reduces doing a much better job of load balancing.Currently the number of reduces is limited to roughly 1000 by the buffer size for the output files(io.buffer.size * 2 * numReduces << heapSize). This will be fixed at some point, but until it is itprovides a pretty firm upper bound.The number of reduces also controls the number of output files in the output directory, but usuallythat is not important because the next map/reduce step will split them into even smaller splits forthe maps.The number of reduce tasks can also be increased in the same way as the map tasks, viaJobConf’s conf.setNumReduceTasks(int num). org.apache.hadoop.mapred Class JobConf

--------------------------------------------------------------------------------------------------
Explanation:A fencing method is a method by which one node can forcibly prevent another nodefrom making continued progress.This might be implemented by killing a process on the other node, by denying the other node’saccess to shared storage, or by accessing a PDU to cut the other node’s power.Since these methods are often vendor- or device-specific, operators may implement this interfacein order to achieve fencing.Fencing is configured by the operator as an ordered list of methods to attempt. Each method willbe tried in turn, and the next in the list will only be attempted if the previous one fails. SeeNodeFencer for more information.Note: failover – initiate a failover between two NameNodesThis subcommand causes a failover from the first provided NameNode to the second. If the firstNameNode is in the Standby state, this command simply transitions the second to the Active statewithout error. If the first NameNode is in the Active state, an attempt will be made to gracefullytransition it to the Standby state. If this fails, the fencing methods (as configured bydfs.ha.fencing.methods) will be attempted in order until one of the methods succeeds. Only afterthis process will the second NameNode be transitioned to the Active state. If no fencing methodsucceeds, the second NameNode will not be transitioned to the Active state, and an error will bereturned. org.apache.hadoop.ha, Interface FenceMethod

 HDFS High Availability Administration, HA Administration using the haadmin command

Answer 26 : A.Isolating a failed NameNode from write access to the fsimage and edits files so that is cannot resume write operations if it recovers.
--------------------------------------------------------------------------------------------------
Explanation:Note: *The placement of replicas is critical to HDFS reliability and performance. Optimizing replicaplacement distinguishes HDFS from most other distributed file systems. This is a feature thatneeds lots of tuning and experience. The purpose of a rack-aware replica placement policy is toimprove data reliability, availability, and network bandwidth utilization. The current implementationfor the replica placement policy is a first effort in this direction. The short-term goals ofimplementing this policy are to validate it on production systems, learn more about its behavior,and build a foundation to test and research more sophisticated policies.* In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each blockis typically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicateeach block three times. Replicas are stored on different nodes. HDFS utilizes the local file systemto store each HDFS block as a separate file. HDFS Block size can not be compared with the

traditional file system block size.

Answer 27: C.The slave node on which the Reducer runs gets the first copy of every block written. Other block replicas will be placed on other nodes.

--------------------------------------------------------------------------------------------------
Answer 28 : D.50 slave nodes

Explanation:Total number available space required: 52 (weeks) * 1 (disk space per week) * 3(default replication factor) = 156 TBMinimum number of slave nodes required: 156 /4 = 39

--------------------------------------------------------------------------------------------------
Answer 29 : D.On the local disk of the slave node running the task.

Explanation: Apache Hadoop Log Files: Where to find them in CDH, and what info they contain
--------------------------------------------------------------------------------------------------
Answer 30 : A.Lightweight devices for bookkeeping within MapReduce programs.

Explanation:Countersare a useful channel for gathering statistics about the job; for qualitycontrol, or for application-level statistics. They are also useful for problem diagnosis. Hadoopmaintains somebuilt-in counters for every job, which reports various metrics for your job.Hadoop MapReduce also allows the user to define a set of user-defined counters that can beincremented (or decremented by specifying a negative value as the parameter), by the driver,mapper or the reducer.Reference:Iterative MapReduce and Counters,Introduction to Iterative MapReduce and Countershttp://hadooptutorial.wikispaces.com/Iterative+MapReduce+and+Counters(counters, secondparagraph)
--------------------------------------------------------------------------------------------------
Answer 31: C.A failed task attempt is a task attempt that completed, but with an unexpected status value. A killed task attempt is a duplicate copy of a task attempt that was started as part of speculative execution.

Explanation:Note:*Hadoop uses “speculative execution.” The same task may be started on multiple boxes. The firstone to finish wins, and the other copies are killed.Failed tasks are tasks that error out.*There are a few reasons Hadoop can kill tasks by his own decisions:a) Task does not report progress during timeout (default is 10 minutes)b) FairScheduler or CapacityScheduler needs the slot for some other pool (FairScheduler) orqueue (CapacityScheduler).c) Speculative execution causes results of task not to be needed since it has completed on otherplace.Reference:Difference failed tasks vs killed tasks

--------------------------------------------------------------------------------------------------
Answer 32 : D.Hadoop Streaming

Explanation:Hadoop streaming is a utility that comes with the Hadoop distribution. The utilityallows you to create and run Map/Reduce jobs with any executable or script as the mapper and/orthe reducer.Reference:http://hadoop.apache.org/common/docs/r0.20.1/streaming.html(HadoopStreaming,second sentence)
--------------------------------------------------------------------------------------------------
Explanation:Example:

Run the application:$ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input/usr/joe/wordcount/output

Answer 33: C.hadoop jar MyJar.jar MyDriverClass inputdir outputdir
--------------------------------------------------------------------------------------------------
Answer 34: A.As key-value pairs in the jobconf object.

Explanation:In Hadoop, it is sometimes difficult to pass arguments to mappers and reducers. Ifthe number of arguments is huge (e.g., big arrays), DistributedCache might be a good choice.However, here, we’re discussing small arguments, usually a hand of configuration parameters.In fact, the way to configure these parameters is simple. When you initialize“JobConf”object tolaunch a mapreduce job, you can set the parameter by using“set”method like:1JobConf job = (JobConf)getConf();2job.set(“NumberOfDocuments”, args[0]);Here,“NumberOfDocuments”is the name of parameter and its value is read from“args[0]“, acommand line argument.Reference:Passing Parameters and Arguments to Mapper and Reducer in Hadoop

--------------------------------------------------------------------------------------------------

Answer 35: C.Write a script that receives records on stdin, corrects them, and then writes them to stdout. Then, invoke this script in a map-only Hadoop Streaming Job
--------------------------------------------------------------------------------------------------
Answer 36: D.Flume provides a query languages for Hadoop similar to SQL
--------------------------------------------------------------------------------------------------
Answer 37 : F.JSON
--------------------------------------------------------------------------------------------------
Answer 38 : B.def mapper1 (line): key1, key2, message = line.split (‘ , ’) emit ( (key1, key2) , 1) emit ( (key1, key2) , 1) def reducer1(key, values): emit (key, sum(values)) def mapper2(key, value): key1, key2 = key / / unpack both friends name into separate keys emit (key1, value) def reducer2(key, values): emit (key, mean (values) )
--------------------------------------------------------------------------------------------------
Answer 39: A.SampleJar.Jar is sent to the ApplicationMaster which allocates a container for SampleJar.Jar
--------------------------------------------------------------------------------------------------
Answer 40 : B.Hdfs fsck

Explanation:https://twiki.grid.iu.edu/bin/view/Storage/HadoopRecovery
--------------------------------------------------------------------------------------------------

Answer 41 : D.They will see the file with its original name. If they attempt to view the file, they will get a ConcurrentFileAccessException until the entire file write is completed on the cluster
--------------------------------------------------------------------------------------------------

Answer 42 : C.The Mapper transfers the intermediate data immediately to the reducers as it is generated by the Map Task
--------------------------------------------------------------------------------------------------
Answer 43 : E.You get a error message telling you that foo.txt already exists. The file is not written to HDFS

--------------------------------------------------------------------------------------------------

Answer 44 : A.Without creating a dfs.hosts file or making any entries, run the commands hadoop.dfsadminrefreshModes on the NameNode
--------------------------------------------------------------------------------------------------
Answer 45 : B.The cluster will re-replicate the file the next time the system administrator reboots the NameNode daemon (as long as the file’s replication factor doesn’t fall below)
--------------------------------------------------------------------------------------------------

Answer 46 : D.A maximum if 100 GB on each hard drive may be used to store HDFS blocks
--------------------------------------------------------------------------------------------------
Answer 47 : D.You must restart the NameNode daemon to apply the changes to the cluster


--------------------------------------------------------------------------------------------------
Answer 48 : D.To ensure that there is consistent disk utilization across the DataNodes

Explanation:NOTE: There is only one correct answer in the options for this question. Pleasecheck the following reference:http://www.quora.com/Apache-Hadoop/It-is-recommended-that-you-run-the-HDFS-balancerperiodically-Why-Choose-3
--------------------------------------------------------------------------------------------------
Answer 49 : A.It only keeps track of which NameNode is Active at any given time

Explanation:http://www.cloudera.com/content/cloudera-content/clouderadocs/CDH4/latest/PDF/CDH4-High-Availability-Guide.pdf(page 15)
--------------------------------------------------------------------------------------------------
Answer 50 : D.Run the ResourceManager on a different master from the NameNode in order to load-share HDFS metadata processing
--------------------------------------------------------------------------------------------------
Answer 51 : B.The NameNode goes down
--------------------------------------------------------------------------------------------------
Answer 52 : E.10
--------------------------------------------------------------------------------------------------
Answer 53 : C.Set dfs.block.size to 128 M on all the worker nodes and client machines, and set the parameter to final. You do not need to set this value on the NameNode
--------------------------------------------------------------------------------------------------
Answer 54 : B.The client queries the NameNode for the locations of the block, and reads from the first location in the list it receives.
--------------------------------------------------------------------------------------------------

Answer 55 : A.Yes. The daemon will receive data from the NameNode to run Map tasks
--------------------------------------------------------------------------------------------------
Answer 56: D.Tune the io.sort.mb value until you observe that the number of spilled records equals (or is as close to equals) the number of map output records.
--------------------------------------------------------------------------------------------------
Answer 57: D.Set vm.swapfile file on the node
--------------------------------------------------------------------------------------------------
Answer 58: B.One active NameNode and one Standby NameNode
--------------------------------------------------------------------------------------------------
Answer 59 : D.8.2 GB
--------------------------------------------------------------------------------------------------
Explanation:B: HDFS is designed to support very large files. Applications that are compatiblewith HDFS are those that deal with large data sets. These applications write their data only once

but they read it one or more times and require these reads to be satisfied at streaming speeds.HDFS supports write-once-read-many semantics on files.D: * Hadoop Distributed File System: A distributed file system that provides high-throughput accessto application data.* DFS is designed to support very large files. Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers

Answer 60 : D.HDFS is a distributed file system that runs on top of native OS filesystems and is well suited to storage of very large data sets.
--------------------------------------------------------------------------------------------------
Answer 61 : B.A single reducer gathers and processes all the output from all the mappers. The output is written to a single file in HDFS.
--------------------------------------------------------------------------------------------------
Answer 62 : A.Because combiners perform local aggregation of word counts, thereby allowing the mappers to process input data faster.

Explanation:* Simply speaking a combiner can be considered as a “mini reducer” that will beapplied potentially several times still during the map phase before to send the new (hopefullyreduced) set of key/value pairs to the reducer(s). This is why a combiner must implement theReducer interface (or extend the Reducer class as of hadoop 0.20).* Combiners are used to increase the efficiency of a MapReduce program. They are used toaggregate intermediate map output locally on individual mapper outputs. Combiners can help youreduce the amount of data that needs to be transferred across to the reducers. You can use yourreducer code as a combiner if the operation performed is commutative and associative. Theexecution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, ifrequired it may execute it more then 1 times. Therefore your MapReduce jobs should not dependon the combiners execution.
--------------------------------------------------------------------------------------------------
Answer 63 : A.There will be r files, each with exactly k/r key-value pairs.

Explanation:Note: * A MapReduce job with m mappers and r reducers involves up to m * r distinct copy operations,since each mapper may have intermediate output going to every reducer.* In the canonical example of word counting, a key-value pair is emitted for every word found. Forexample, if we had 1,000 words, then 1,000 key-value pairs will be emitted from the mappers tothe reducer(s).
--------------------------------------------------------------------------------------------------

Answer 64: B.Yes.

--------------------------------------------------------------------------------------------------
Answer 65 : B.No reducer executes, and the output of each mapper is written to a separate file in HDFS.

Explanation:* It is legal to set the number of reduce-tasks to zero if no reduction is desired.In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set bysetOutputPath(Path). The framework does not sort the map-outputs before writing them out to theFileSystem.* Often, you may want to process input data using a map function only. To do this, simply setmapreduce.job.reduces to zero. The MapReduce framework will not create any reducer tasks.Rather, the outputs of the mapper tasks will be the final output of the job.
--------------------------------------------------------------------------------------------------
Explanation:Combiners are used to increase the efficiency of a MapReduce program. They areused to aggregate intermediate map output locally on individual mapper outputs. Combiners canhelp you reduce the amount of data that needs to be transferred across to the reducers. You canuse your reducer code as a combiner if the operation performed is commutative and associative.The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also,if required it may execute it more then 1 times. Therefore your MapReduce jobs should not

depend on the combiners execution.http://www.fromdev.com/2010/12/interview-questions-hadoop-mapreduce.html (question no. 12)

Answer 66: C.They aggregate intermediate map output locally on each individual machine and therefore reduce the amount of data that needs to be shuffled across the network to the reducers.

--------------------------------------------------------------------------------------------------
Answer 67: D.mxr (i.e., m multiplied by r)

Explanation:A MapReduce job with m mappers and r reducers involves up to m * r distinct copyoperations, since each mapper may have intermediate output going to every reducer.
--------------------------------------------------------------------------------------------------
Answer 68: B.Each slave node runs a TaskTracker and a DataNode daemon.

Explanation:Single instance of a Task Tracker is run on each Slave node. Task tracker is run asa separate JVM process.Single instance of a DataNode daemon is run on each Slave node. DataNode daemon is run as aseparate JVM process.One or Multiple instances of Task Instance is run on each slave node. Each task instance is run asa separate JVM process. The number of Task instances can be controlled by configuration.Typically a high end machine is configured to run more task instances.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What isconfiguration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?
--------------------------------------------------------------------------------------------------
Answer 69 : A.HDFS becomes unavailable until the NameNode is restored.

Explanation:The NameNode is a Single Point of Failure for the HDFS Cluster. When theNameNode goes down, the file system goes offline.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What is aNameNode? How many instances of NameNode run on a Hadoop Cluster?
--------------------------------------------------------------------------------------------------
Answer 70 : C.The node on which this InputSplit is stored

Explanation:The TaskTrackers send out heartbeat messages to the JobTracker, usually everyfew minutes, to reassure the JobTracker that it is still alive. These message also inform theJobTracker of the number of available slots, so the JobTracker can stay up to date with where inthe cluster work can be delegated. When the JobTracker tries to find somewhere to schedule atask within the MapReduce operations, it first looks for an empty slot on the same server thathosts the DataNode containing the data, and if not, it looks for an empty slot on a machine in thesame rack.
--------------------------------------------------------------------------------------------------
Answer 71 : B.When the NameNode fails to receive periodic heartbeats from the DataNode, it considers the DataNode as failed.

Explanation:NameNode periodically receives a Heartbeat and a Blockreport from each of theDataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly.A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has notrecieved a hearbeat message from a data node after a certain amount of time, the data node ismarked as dead. Since blocks will be under replicated the system begins replicating the blocksthat were stored on the dead datanode. The NameNode Orchestrates the replication of datablocks from one datanode to another. The replication data transfer happens directly betweendatanodes and the data never passes through the namenode.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, HowNameNode Handles data node failures?

--------------------------------------------------------------------------------------------------
Explanation:The NameNode is the centerpiece of an HDFS file system. It keeps the directorytree of all files in the file system, and tracks where across the cluster the file data is kept. It doesnot store the data of these files itself. There is only One NameNode process run on any hadoopcluster. NameNode runs on its own JVM process. In a typical production cluster its run on aseparate machine. The NameNode is a Single Point of Failure for the HDFS Cluster. When theNameNode goes down, the file system goes offline. Client applications talk to the NameNodewhenever they wish to locate a file, or when they want to add/copy/move/delete a file. The

NameNode responds the successful requests by returning a list of relevant DataNode serverswhere the data livesReference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What is aNameNode? How many instances of NameNode run on a Hadoop Cluster?

Answer 72 : B.To store filenames, list of blocks and other meta information.


--------------------------------------------------------------------------------------------------
Answer 73 : A.Writable is an interface that all keys and values in MapReduce must implement. Classes implementing this interface must implement methods for serializing and deserializing themselves.

Explanation:public interface WritableA serializable object which implements a simple, efficient, serialization protocol, based onDataInput and DataOutput.Any key or value type in the Hadoop Map-Reduce framework implements this interface.Implementations typically implement a static read(DataInput) method which constructs a newinstance, calls readFields(DataInput) and returns the instance.Reference: org.apache.hadoop.io, Interface Writable
--------------------------------------------------------------------------------------------------
Answer 74 : D.Keys are presented to a reducer in random order; values for a given key are sorted in ascending order.

--------------------------------------------------------------------------------------------------

Answer 75 : D.The default partitioner computes the hash of the key and divides that value modulo the number of reducers. The result determines the reducer assigned to process the key-value pair.

Explanation:The default partitioner computes a hash value for the key and assigns the partitionbased on this result.The default Partitioner implementation is called HashPartitioner. It uses the hashCode() method ofthe key objects modulo the number of partitions total to determine which partition to send a given(key, value) pair to.In Hadoop, the default partitioner is HashPartitioner, which hashes a record’s key to determinewhich partition (and thus which reducer) the record belongs in. The number of partition is thenequal to the number of reduce tasks for the job.Reference: Getting Started With (Customized) Partitioning

--------------------------------------------------------------------------------------------------
Answer 76: C.Intermediate key-value pairs are written to the local disks of the machines running the map tasks, and then copied to the machine running the reduce tasks.

Explanation:The mapper output (intermediate data) is stored on the Local file system (NOTHDFS) of each individual mapper nodes. This is typically a temporary directory location which canbe setup in config by the hadoop administrator. The intermediate data is cleaned up after theHadoop Job completes.Note: * Reducers start copying intermediate key-value pairs from the mappers as soon as they areavailable. The progress calculation also takes in account the processing of data transfer which isdone by reduce process, therefore the reduce progress starts showing up as soon as anyintermediate key-value pair for a mapper is available to be transferred to reducer. Though thereducer progress is updated still the programmer defined reduce method is called only after all themappers have finished.* Reducer is input the grouped output of a Mapper. In the phase the framework, for each Reducer,fetches the relevant partition of the output of all the Mappers, via HTTP.* Mapper maps input key/value pairs to a set of intermediate key/value pairs.Maps are the individual tasks that transform input records into intermediate records. Thetransformed intermediate records do not need to be of the same type as the input records. A giveninput pair may map to zero or many output pairs.* All intermediate values associated with a given output key are subsequently grouped by theframework, and passed to the Reducer(s) to determine the final output.Reference: Questions &amp; Answers for Hadoop MapReduce developers, Where is the MapperOutput (intermediate kay-value data) stored ?

--------------------------------------------------------------------------------------------------
Answer 77: D.No, each reducer runs independently and in isolation.

Explanation:MapReduce programming model does not allow reducers to communicate with eachother. Reducers run in isolation.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developershttp://www.fromdev.com/2010/12/interview-questions-hadoop-mapreduce.html (See question no.9)

--------------------------------------------------------------------------------------------------
Answer 78: D.It accepts a single key-value pair as input and can emit any number of key-value pairs as output, including zero.

Explanation:public class Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>extends ObjectMaps input key/value pairs to a set of intermediate key/value pairs.Maps are the individual tasks which transform input records into a intermediate records. Thetransformed intermediate records need not be of the same type as the input records. A given inputpair may map to zero or many output pairs.Reference: org.apache.hadoop.mapreduce Class Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>

--------------------------------------------------------------------------------------------------
Answer 79: C.Online transaction processing (OLTP) for an e-commerce Website.

Explanation:Hadoop Map/Reduce is designed for batch-oriented work load.MapReduce is well suited for data warehousing (OLAP), but not for OLTP.


--------------------------------------------------------------------------------------------------
Answer 80: C.200

Explanation:Each file would be split into two as the block size (64 MB) is less than the file size(100 MB), so 200 mappers would be running.Note:If you’re not compressing the files then hadoop will process your large files (say 10G), with anumber of mappers related to the block size of the file.Say your block size is 64M, then you will have ~160 mappers processing this 10G file (160*64 ~=10G). Depending on how CPU intensive your mapper logic is, this might be an acceptable blockssize, but if you find that your mappers are executing in sub minute times, then you might want toincrease the work done by each mapper (by increasing the block size to 128, 256, 512m – theactual size depends on how you intend to process the data).Reference: http://stackoverflow.com/questions/11014493/hadoop-mapreduce-appropriate-inputfiles-size (first answer, second paragraph)


--------------------------------------------------------------------------------------------------
Answer 81: C.hadoop jar MyJar.jar MyDriverClass inputdir outputdir

Explanation:Example:Run the application:$ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input/usr/joe/wordcount/output

--------------------------------------------------------------------------------------------------
Answer 82: C.A failed task attempt is a task attempt that completed, but with an unexpected status value. A killed task attempt is a duplicate copy of a task attempt that was started as part of speculative execution.

Explanation:Note:* Hadoop uses “speculative execution.” The same task may be started on multiple boxes. The firstone to finish wins, and the other copies are killed.Failed tasks are tasks that error out.* There are a few reasons Hadoop can kill tasks by his own decisions: a) Task does not report progress during timeout (default is 10 minutes) b) FairScheduler or CapacityScheduler needs the slot for some other pool (FairScheduler) orqueue (CapacityScheduler). c) Speculative execution causes results of task not to be needed since it has completed on otherplace.Reference: Difference failed tasks vs killed tasks


--------------------------------------------------------------------------------------------------
Answer 83: A.Lightweight devices for bookkeeping within MapReduce programs.

Explanation:Counters are a useful channel for gathering statistics about the job; for qualitycontrol, or for application-level statistics. They are also useful for problem diagnosis. Hadoopmaintains some built-in counters for every job, which reports various metrics for your job.Hadoop MapReduce also allows the user to define a set of user-defined counters that can beincremented (or decremented by specifying a negative value as the parameter), by the driver,mapper or the reducer.Reference: Iterative MapReduce and Counters, Introduction to Iterative MapReduce and Countershttp://hadooptutorial.wikispaces.com/Iterative+MapReduce+and+Counters (counters, secondparagraph)
--------------------------------------------------------------------------------------------------
Answer 84: A.As key-value pairs in the jobconf object.

Explanation:In Hadoop, it is sometimes difficult to pass arguments to mappers and reducers. Ifthe number of arguments is huge (e.g., big arrays), DistributedCache might be a good choice.However, here, we’re discussing small arguments, usually a hand of configuration parameters.In fact, the way to configure these parameters is simple. When you initialize “JobConf” object tolaunch a mapreduce job, you can set the parameter by using “set” method like:1 JobConf job = (JobConf)getConf();2 job.set(“NumberOfDocuments”, args[0]);Here, “NumberOfDocuments” is the name of parameter and its value is read from “args[0]“, acommand line argument.Reference: Passing Parameters and Arguments to Mapper and Reducer in Hadoop

--------------------------------------------------------------------------------------------------
Explanation:Sqoop (“SQL-to-Hadoop”) is a straightforward command-line tool with the following

capabilities:Imports individual tables or entire databases to files in HDFSGenerates Java classes to allow you to interact with your imported dataProvides the ability to import from SQL databases straight into your Hive data warehouseNote: Data Movement Between Hadoop and Relational DatabasesData can be moved between Hadoop and a relational database as a bulk data transfer, orrelational tables can be accessed from within a MapReduce map function.Note: * Cloudera’s Distribution for Hadoop provides a bulk data transfer tool (i.e., Sqoop) that importsindividual tables or entire databases into HDFS files. The tool also generates Java classes thatsupport interaction with the imported data. Sqoop supports all relational databases over JDBC,and Quest Software provides a connector (i.e., OraOop) that has been optimized for access todata residing in Oracle databases.Reference: http://log.medcl.net/item/2011/08/hadoop-and-mapreduce-big-data-analytics-gartner/(Data Movement between hadoop and relational databases, second paragraph)

Answer 85: E.Sqoop

--------------------------------------------------------------------------------------------------
Answer 86: D.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReader of the split that contains the end of the broken line.

Explanation:As the Map operation is parallelized the input file set is first split to several piecescalled FileSplits. If an individual file is so large that it will affect seek time it will be split to severalSplits. The splitting does not know anything about the input file’s internal logical structure, forexample line-oriented text files are split on arbitrary byte boundaries. Then a new map task iscreated per FileSplit.When an individual map task starts it will open a new output writer per configured reduce task. Itwill then proceed to read its FileSplit using the RecordReader it gets from the specifiedInputFormat. InputFormat parses the input and generates key-value pairs. InputFormat must alsohandle records that may be split on the FileSplit boundary. For example TextInputFormat will readthe last line of the FileSplit past the split boundary and, when reading other than the first FileSplit,TextInputFormat ignores the content up to the first newline.Reference: How Map and Reduce operations are actually carried outhttp://wiki.apache.org/hadoop/HadoopMapReduce (Map, second paragraph)


--------------------------------------------------------------------------------------------------

Answer 87 : A.SequenceFiles

Explanation:Using Hadoop Sequence FilesSo what should we do in order to deal with huge amount of images? Use hadoop sequence files!Those are map files that inherently can be read by map reduce applications – there is an inputformat especially for sequence files – and are splitable by map reduce, so we can have one hugefile that will be the input of many map tasks. By using those sequence files we are letting hadoopuse its advantages. It can split the work into chunks so the processing is parallel, but the chunksare big enough that the process stays efficient.Since the sequence file are map file the desired format will be that the key will be text and hold theHDFS filename and the value will be BytesWritable and will contain the image content of the file.Reference: Hadoop binary files processing introduced by image duplicates finder

--------------------------------------------------------------------------------------------------
Answer 88: D.It accepts a single key-value pairs as input and can emit any number of key-value pair as output, including zero.

Explanation:public class Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>extends ObjectMaps input key/value pairs to a set of intermediate key/value pairs.Maps are the individual tasks which transform input records into a intermediate records. Thetransformed intermediate records need not be of the same type as the input records. A given inputpair may map to zero or many output pairs.Reference: org.apache.hadoop.mapreduce Class Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
--------------------------------------------------------------------------------------------------
Answer 89 : A.When the types of the reduce operation’s input key and input value match the types of the reducer’s output key and output value and when the reduce operation is both communicative and associative.

Explanation:You can use your reducer code as a combiner if the operation performed iscommutative and associative.


--------------------------------------------------------------------------------------------------

Answer 90: C.Decrease the block size on your remaining files.

Explanation:Note:* -put localSrc destCopies the file or directory from the local file system identified by localSrc todest within the DFS.* What is HDFS Block size? How is it different from traditional file system block size?In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each block istypically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicateeach block three times. Replicas are stored on different nodes. HDFS utilizes the local file systemto store each HDFS block as a separate file. HDFS Block size can not be compared with thetraditional file system block size.

--------------------------------------------------------------------------------------------------

Explanation:The default partitioner computes a hash value for the key and assigns the partitionbased on this result.The default Partitioner implementation is called HashPartitioner. It uses the hashCode() method ofthe key objects modulo the number of partitions total to determine which partition to send a given(key, value) pair to.In Hadoop, the default partitioner is HashPartitioner, which hashes a record’s key to determine

which partition (and thus which reducer) the record belongs in.The number of partition is thenequal to the number of reduce tasks for the job.Reference: Getting Started With (Customized) Partitioning

Answer 91 : D.The default partitioner computers the hash of the key and divides that valule modulo the number of reducers. The result determines the reducer assigned to process the key-value pair.

--------------------------------------------------------------------------------------------------

Answer 92: C.It returns a reference to the same Writable object each time, but populated with different data.

Explanation:Calling Iterator.next() will always return the SAME EXACT instance of IntWritable,with the contents of that instance replaced with the next value.Reference: manupulating iterator in mapreduce
--------------------------------------------------------------------------------------------------
Answer 93: D.At least 500.

Explanation:From Cloudera Training Course:Task attempt is a particular instance of an attempt to execute a task– There will be at least as many task attempts as there are tasks– If a task attempt fails, another will be started by the JobTracker– Speculative execution can also result in more task attempts than completed tasks

--------------------------------------------------------------------------------------------------
Answer 94: C.Two, file names with a leading period or underscore are ignored

Explanation:Files starting with ‘_’ are considered ‘hidden’ like unix files starting with ‘.’.# characters are allowed in HDFS file names.

--------------------------------------------------------------------------------------------------
Explanation:* It is legal to set the number of reduce-tasks to zero if no reduction is desired.In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set bysetOutputPath(Path). The framework does not sort the map-outputs before writing them out to theFileSystem.* Often, you may want to process input data using a map function only. To do this, simply setmapreduce.job.reduces to zero. The MapReduce framework will not create any reducer tasks.Rather, the outputs of the mapper tasks will be the final output of the job.Note:

ReduceIn this phase the reduce(WritableComparable, Iterator, OutputCollector, Reporter) method iscalled for each <key, (list of values)> pair in the grouped inputs.The output of the reduce task is typically written to the FileSystem viaOutputCollector.collect(WritableComparable, Writable).Applications can use the Reporter to report progress, set application-level status messages andupdate Counters, or just indicate that they are alive.The output of the Reducer is not sorted.

Answer 95 : D.With zero reducers, instances of matching patterns are stored in multiple files on HDFS. With one reducer, all instances of matching patterns are gathered together in one file on HDFS.

--------------------------------------------------------------------------------------------------
Answer 96: B.The amount of intermediate data that must be transferred between the mapper and reducer.

Explanation:Combiners are used to increase the efficiency of a MapReduce program. They areused to aggregate intermediate map output locally on individual mapper outputs. Combiners canhelp you reduce the amount of data that needs to be transferred across to the reducers. You canuse your reducer code as a combiner if the operation performed is commutative and associative.The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, ifrequired it may execute it more then 1 times. Therefore your MapReduce jobs should not dependon the combiners execution.
--------------------------------------------------------------------------------------------------

Answer 97: A.Yes, because the sum operation is both associative and commutative and the input and output types to the reduce method match.

Explanation:Combiners are used to increase the efficiency of a MapReduce program. They areused to aggregate intermediate map output locally on individual mapper outputs. Combiners canhelp you reduce the amount of data that needs to be transferred across to the reducers. You canuse your reducer code as a combiner if the operation performed is commutative and associative.The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, ifrequired it may execute it more then 1 times. Therefore your MapReduce jobs should not dependon the combiners execution.
--------------------------------------------------------------------------------------------------
Explanation:Note: * Input to the Reducer is the sorted output of the mappers. * The framework calls the application’s Reduce function once for each unique key in the sorted

order.* Example:For the given sample input the first map emits:< Hello, 1> < World, 1> < Bye, 1> < World, 1> The second map emits:< Hello, 1> < Hadoop, 1> < Goodbye, 1> < Hadoop, 1>

Answer 98: B.The values are arbitrarily ordered, and the ordering may vary from run to run of the same MapReduce job.
--------------------------------------------------------------------------------------------------
Answer 99: B.3

Explanation:reduce() gets called once for each [key, (list of values)] pair. To explain, let’s sayyou called:out.collect(new Text(“Car”),new Text(“Subaru”);out.collect(new Text(“Car”),new Text(“Honda”);out.collect(new Text(“Car”),new Text(“Ford”);out.collect(new Text(“Truck”),new Text(“Dodge”);out.collect(new Text(“Truck”),new Text(“Chevy”);Then reduce() would be called twice with the pairsreduce(Car, <Subaru, Honda, Ford>)reduce(Truck, <Dodge, Chevy>)Reference: Mapper output.collect()?

--------------------------------------------------------------------------------------------------

Explanation:* In a MapReduce job reducers do not start executing the reduce method until the allMap jobs have completed. Reducers start copying intermediate key-value pairs from the mappersas soon as they are available. The programmer defined reduce method is called only after all themappers have finished.* Reducers start copying intermediate key-value pairs from the mappers as soon as they areavailable. The progress calculation also takes in account the processing of data transfer which isdone by reduce process, therefore the reduce progress starts showing up as soon as anyintermediate key-value pair for a mapper is available to be transferred to reducer. Though thereducer progress is updated still the programmer defined reduce method is called only after all themappers have finished.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , When is the

reducers are started in a MapReduce job?

Answer 100: D.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The reduce method is called as soon as the intermediate key-value pairs start to arrive.

--------------------------------------------------------------------------------------------------

Answer 101: C.Implement WritableComparable.

Explanation:The MapReduce framework operates exclusively on <key, value> pairs, that is, theframework views the input to the job as a set of <key, value> pairs and produces a set of <key,value> pairs as the output of the job, conceivably of different types.The key and value classes have to be serializable by the framework and hence need to implementthe Writable interface. Additionally, the key classes have to implement the WritableComparableinterface to facilitate sorting by the framework.Reference: MapReduce Tutorial

--------------------------------------------------------------------------------------------------
Answer 102: C.All data for a given key, regardless of which mapper(s) produced it.

Explanation:Reducing lets you aggregate values together. A reducer function receives an iteratorof input values from an input list. It then combines these values together, returning a single outputvalue.All values with the same key are presented to a single reduce task.Reference: Yahoo! Hadoop Tutorial, Module 4: MapReduce

--------------------------------------------------------------------------------------------------
Answer 103: E.One final key-value pair per key; no restrictions on the type.

Explanation:Reducer reduces a set of intermediate values which share a key to a smaller set ofvalues.Reducing lets you aggregate values together. A reducer function receives an iterator of inputvalues from an input list. It then combines these values together, returning a single output value.Reference: Hadoop Map-Reduce Tutorial; Yahoo! Hadoop Tutorial, Module 4: MapReduce
--------------------------------------------------------------------------------------------------
Answer 104 : E.Speculative Execution

Explanation:Speculative execution: One problem with the Hadoop system is that by dividing thetasks across many nodes, it is possible for a few slow nodes to rate-limit the rest of the program.For example if one node has a slow disk controller, then it may be reading its input at only 10% thespeed of all the other nodes. So when 99 map tasks are already complete, the system is stillwaiting for the final map task to check in, which takes much longer than all the other nodes.By forcing tasks to run in isolation from one another, individual tasks do not know where theirinputs come from. Tasks trust the Hadoop platform to just deliver the appropriate input. Therefore,the same input can be processed multiple times in parallel, to exploit differences in machinecapabilities. As most of the tasks in a job are coming to a close, the Hadoop platform will scheduleredundant copies of the remaining tasks across several nodes which do not have other work toperform. This process is known as speculative execution. When tasks complete, they announcethis fact to the JobTracker. Whichever copy of a task finishes first becomes the definitive copy. Ifother copies were executing speculatively, Hadoop tells the TaskTrackers to abandon the tasksand discard their outputs. The Reducers then receive their inputs from whichever Mappercompleted successfully, first.Reference: Apache Hadoop, Module 4: MapReduceNote:* Hadoop uses “speculative execution.” The same task may be started on multiple boxes. The firstone to finish wins, and the other copies are killed.Failed tasks are tasks that error out.* There are a few reasons Hadoop can kill tasks by his own decisions: a) Task does not report progress during timeout (default is 10 minutes) b) FairScheduler or CapacityScheduler needs the slot for some other pool (FairScheduler) orqueue (CapacityScheduler). c) Speculative execution causes results of task not to be needed since it has completed on otherplace.Reference: Difference failed tasks vs killed tasks

--------------------------------------------------------------------------------------------------
Answer 105: C.When submitting the job on the command line, specify the –libjars option followed by the JAR file path. 

Explanation:The usage of the jar command is like this,Usage: hadoop jar <jar> [mainClass] args… If you want the commons-math3.jar to be available for all the tasks you can do any one of these 1. Copy the jar file in $HADOOP_HOME/lib dir or 2. Use the generic option -libjars.
--------------------------------------------------------------------------------------------------
Explanation:Note:The output format for your first MR job should be SequenceFileOutputFormat – this will store theKey/Values output from the reducer in a binary format, that can then be read back in, in yoursecond MR job using SequenceFileInputFormat.

Reference: How to parse CustomWritable from text in Hadoophttp://stackoverflow.com/questions/9721754/how-to-parse-customwritable-from-text-in-hadoop(see answer 1 and then see the comment #1 for it)

Answer 106 : B.SequenceFileInputFormat
--------------------------------------------------------------------------------------------------

Answer 107: C.By using multiple reducers with the default HashPartitioner, output files may not be in globally sorted order.

Explanation:Multiple reducers and total orderingIf your sort job runs with multiple reducers (either because mapreduce.job.reduces in mapredsite.xml has been set to a number larger than 1, or because you’ve used the -r option to specifythe number of reducers on the command-line), then by default Hadoop will use the HashPartitionerto distribute records across the reducers. Use of the HashPartitioner means that you can’tconcatenate your output files to create a single sorted output file. To do this you’ll need totalordering,Reference: Sorting text files with MapReduce
--------------------------------------------------------------------------------------------------

Answer 108: A.Six 

Explanation:Only one key value pair will be passed from the two (The, 1) key value pairs.
--------------------------------------------------------------------------------------------------
Answer 109: E.As many intermediate key-value pairs as designed, as long as all the keys have the same types and all the values have the same type.

Explanation:Mapper maps input key/value pairs to a set of intermediate key/value pairs.Maps are the individual tasks that transform input records into intermediate records. Thetransformed intermediate records do not need to be of the same type as the input records. A giveninput pair may map to zero or many output pairs.Reference: Hadoop Map-Reduce Tutorial
--------------------------------------------------------------------------------------------------

Answer 110: E.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReader of the split that contains the end of the broken line.

Explanation:As the Map operation is parallelized the input file set is first split to several piecescalled FileSplits. If an individual file is so large that it will affect seek time it will be split to severalSplits. The splitting does not know anything about the input file’s internal logical structure, forexample line-oriented text files are split on arbitrary byte boundaries. Then a new map task iscreated per FileSplit.When an individual map task starts it will open a new output writer per configured reduce task. Itwill then proceed to read its FileSplit using the RecordReader it gets from the specifiedInputFormat. InputFormat parses the input and generates key-value pairs. InputFormat must alsohandle records that may be split on the FileSplit boundary. For example TextInputFormat will readthe last line of the FileSplit past the split boundary and, when reading other than the first FileSplit,TextInputFormat ignores the content up to the first newline.Reference: How Map and Reduce operations are actually carried out

--------------------------------------------------------------------------------------------------
Answer111: D.The InputFormat used by the job determines the mapper’s input key and value types.

Explanation:The input types fed to the mapper are controlled by the InputFormat used. Thedefault input format, “TextInputFormat,” will load data in as (LongWritable, Text) pairs. The longvalue is the byte offset of the line in the file. The Text object holds the string contents of the line ofthe file.Note: The data types emitted by the reducer are identified by setOutputKeyClass()andsetOutputValueClass(). The data types emitted by the reducer are identified bysetOutputKeyClass() and setOutputValueClass().By default, it is assumed that these are the output types of the mapper as well. If this is not thecase, the methods setMapOutputKeyClass() and setMapOutputValueClass() methods of theJobConf class will override these.Reference: Yahoo! Hadoop Tutorial, THE DRIVER METHOD
--------------------------------------------------------------------------------------------------
Answer 112 : C.hadoop MyDrive –D mapred.job.name=Example input output

Explanation:Configure the property using the -D key=value notation:-D mapred.job.name=’My Job’You can list a whole bunch of options by calling the streaming jar with just the -info argumentReference: Python hadoop streaming : Setting a job name

--------------------------------------------------------------------------------------------------
Answer 113: D.Into in-memory buffers that spill over to the local file system (outside HDFS) of the TaskTracker node running the Reducer

Explanation:The mapper output (intermediate data) is stored on the Local file system (NOTHDFS) of each individual mapper nodes. This is typically a temporary directory location which canbe setup in config by the hadoop administrator. The intermediate data is cleaned up after theHadoop Job completes.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, Where is theMapper Output (intermediate kay-value data) stored ?

--------------------------------------------------------------------------------------------------
Explanation:Note: * Join Algorithms in MapReduceA) Reduce-side joinB) Map-side joinC) In-memory join/ Striped Striped variant variant

/ Memcached variant* Which join to use?/ In-memory join > map-side join > reduce-side join/ Limitations of each?In-memory join: memoryMap-side join: sort order and partitioningReduce-side join: general purpose

Answer 114: A.Yes.
--------------------------------------------------------------------------------------------------
Answer 115: F.Combiner

Explanation:Combiners are used to increase the efficiency of a MapReduce program. They areused to aggregate intermediate map output locally on individual mapper outputs. Combiners canhelp you reduce the amount of data that needs to be transferred across to the reducers. You canuse your reducer code as a combiner if the operation performed is commutative and associative.

--------------------------------------------------------------------------------------------------
Explanation:See 3) below.Here is an illustrative example on how to use the DistributedCache:// Setting up the cache for the application1. Copy the requisite files to the FileSystem:$ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar$ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar$ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz$ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz

2. Setup the application’s JobConf:JobConf job = new JobConf();DistributedCache.addCacheFile(new URI(“/myapp/lookup.dat#lookup.dat”), job);DistributedCache.addCacheArchive(new URI(“/myapp/map.zip”, job);DistributedCache.addFileToClassPath(new Path(“/myapp/mylib.jar”), job);DistributedCache.addCacheArchive(new URI(“/myapp/mytar.tar”, job);DistributedCache.addCacheArchive(new URI(“/myapp/mytgz.tgz”, job);DistributedCache.addCacheArchive(new URI(“/myapp/mytargz.tar.gz”, job);3. Use the cached files in the Mapperor Reducer:public static class MapClass extends MapReduceBase implements Mapper<K, V, K, V> {private Path[] localArchives;private Path[] localFiles;public void configure(JobConf job) {// Get the cached archives/fileslocalArchives = DistributedCache.getLocalCacheArchives(job);localFiles = DistributedCache.getLocalCacheFiles(job);}public void map(K key, V value, OutputCollector<K, V> output, Reporter reporter) throws IOException {// Use data from the cached archives/files here// …// …output.collect(k, v);}}Reference: org.apache.hadoop.filecache , Class DistributedCache

Answer 116: D.configure 
--------------------------------------------------------------------------------------------------
Explanation:There will be four failed task attempts for each of the five file splits.

Note: When the jobtracker is notified of a task attempt that has failed (by the tasktracker’s heartbeat tall),it will reschedule execution of the task. The jobtracker will try to avoid rescheduling the task on atasktracker where it has previously tailed. Furthermore, if a task fails four times (or more), it willnot be retried further. This value is configurable: the maximum number of attempts to run a task iscontrolled by the mapred.map.max.attempts property for map tasks andmapred.reduce.max.attempts for reduce tasks. By default, if any task fails four times (or whateverthe maximum number of attempts is configured to), the whole job fails.

Answer 117: E.You will have twenty failed task attempts 

--------------------------------------------------------------------------------------------------
Explanation:Reducer has 3 primary phases:1. ShuffleThe Reducer copies the sorted output from each Mapper using HTTP across the network.2. SortThe framework merge sorts Reducer inputs by keys (since different Mappers may have output thesame key).

The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they aremerged.SecondarySortTo achieve a secondary sort on the values returned by the value iterator, the application shouldextend the key with the secondary key and define a grouping comparator. The keys will be sortedusing the entire key, but will be grouped using the grouping comparator to decide which keys andvalues are sent in the same call to reduce.3. ReduceIn this phase the reduce(Object, Iterable, Context) method is called for each <key, (collection ofvalues)> in the sorted inputs.The output of the reduce task is typically written to a RecordWriter viaTaskInputOutputContext.write(Object, Object).The output of the Reducer is not re-sorted.Reference: org.apache.hadoop.mapreduce, ClassReducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>

Answer 118: D.The keys given to a reducer are in sorted order but the values associated with each key are in no predictable order
--------------------------------------------------------------------------------------------------
Explanation:Reducer has 3 primary phases:1. ShuffleThe Reducer copies the sorted output from each Mapper using HTTP across the network.2. SortThe framework merge sorts Reducer inputs by keys (since different Mappers may have output thesame key).The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they aremerged.

SecondarySortTo achieve a secondary sort on the values returned by the value iterator, the application shouldextend the key with the secondary key and define a grouping comparator. The keys will be sortedusing the entire key, but will be grouped using the grouping comparator to decide which keys andvalues are sent in the same call to reduce.3. ReduceIn this phase the reduce(Object, Iterable, Context) method is called for each <key, (collection ofvalues)> in the sorted inputs.The output of the reduce task is typically written to a RecordWriter viaTaskInputOutputContext.write(Object, Object).The output of the Reducer is not re-sorted.Reference: org.apache.hadoop.mapreduce, ClassReducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>

Answer 119: A.Keys are presented to reducer in sorted order; values for a given key are not sorted.
--------------------------------------------------------------------------------------------------

Answer 120 : D.Reducer <Text, IntWritable, Text, IntWritable> 

--------------------------------------------------------------------------------------------------
Answer 121 : C.The client contacts the NameNode for the block location(s). The NameNode then queries the DataNodes for block locations. The DataNodes respond to the NameNode, and the NameNode redirects the client to the DataNode that holds the requested data block(s). The client then reads the data directly off the DataNode.

Explanation:The Client communication to HDFS happens using Hadoop HDFS API. Clientapplications talk to the NameNode whenever they wish to locate a file, or when they want toadd/copy/move/delete a file on HDFS. The NameNode responds the successful requests byreturning a list of relevant DataNode servers where the data lives. Client applications can talkdirectly to a DataNode, once the NameNode has provided the location of the data.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, How the Clientcommunicates with HDFS?
--------------------------------------------------------------------------------------------------
Answer: C.Not until all mappers have finished processing all records.

Explanation:In a MapReduce job reducers do not start executing the reduce method until the allMap jobs have completed. Reducers start copying intermediate key-value pairs from the mappersas soon as they are available. The programmer defined reduce method is called only after all themappers have finished.Note: The reduce phase has 3 steps: shuffle, sort, reduce. Shuffle is where the data is collected bythe reducer from each mapper. This can happen while mappers are generating data since it is onlya data transfer. On the other hand, sort and reduce can only start once all the mappers are done.Why is starting the reducers early a good thing? Because it spreads out the data transfer from themappers to the reducers over time, which is a good thing if your network is the bottleneck.Why is starting the reducers early a bad thing? Because they “hog up” reduce slots while onlycopying data. Another job that starts later that will actually use the reduce slots now can’t usethem.You can customize when the reducers startup by changing the default value ofmapred.reduce.slowstart.completed.maps in mapred-site.xml. A value of 1.00 will wait for all themappers to finish before starting the reducers. A value of 0.0 will start the reducers right away. Avalue of 0.5 will start the reducers when half of the mappers are complete. You can also changemapred.reduce.slowstart.completed.maps on a job-by-job basis.Typically, keep mapred.reduce.slowstart.completed.maps above 0.9 if the system ever hasmultiple jobs running at once. This way the job doesn’t hog up reducers when they aren’t doinganything but copying data. If you only ever have one job running at a time, doing 0.1 wouldprobably be appropriate.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, When is thereducers are started in a MapReduce job?
--------------------------------------------------------------------------------------------------
Answer 123: B. The Mapper transfers the intermediate data immediately to the Reducers as it generated by the Map task

--------------------------------------------------------------------------------------------------
Answer 124: C,D,E F.An edit log of changes that have been made since the last snapshot compaction by the Secondary NameNode.

--------------------------------------------------------------------------------------------------
Answer 125: D.Keeping track of tasks running on each individual slave node.

-------------------------------------------------------------------------------------------------
Answer 126: B.There are always at least as many task attempts as there are tasks.

--------------------------------------------------------------------------------------------------

