1 : B.The application reads the blocks directly from the DataNodes.

Explanation: http://kazman.shidler.hawaii.edu/ArchDocOverview.html

--------------------------------------------------------------------------------------------------
2 : B.Rack location is considered in the HDFS block placement policy.

2 : C.Hadoop gives preference to intra-rack data transfer In order to conserve bandwidth.

--------------------------------------------------------------------------------------------------
3 : B.Linux is preferable to Windows and Solaris/OpenSolaris. Some Linux distributions are intended for cluster environments more than others.

Explanation:http://wiki.apache.org/hadoop/FAQ#What_platforms_and_Java_versions_does_Hadoop_run_on.3F (1.2)

--------------------------------------------------------------------------------------------------
4 : B.There are always at least as many task attempts as there are tasks.

--------------------------------------------------------------------------------------------------
5 : C.When job A gets submitted, it consumes all the task slots.

5 : D.When job B gets submitted, job A has to finish first, before job B can get scheduled.

Explanation: http://hadoop.apache.org/common/docs/r0.20.2/fair_scheduler.html (introduction, first paragraph)

--------------------------------------------------------------------------------------------------
6 : D.Keeping track of tasks running on each individual slave node.

--------------------------------------------------------------------------------------------------
7 : B.The NameNode requires more memory but less disk capacity.

Explanation: http://www.atlantbh.com/how-to-build-optimal-hadoop-cluster/ (Hardwarerequirements, basic hardware recommendation)

--------------------------------------------------------------------------------------------------
8 : C.A catalog of DataNodes and the blocks that are stored on them.

8 : D.Names of the files in HDFS.

8 : E.The directory structure of the files in HDFS.

8 : F.An edit log of changes that have been made since the last snapshot compaction by the Secondary NameNode.

--------------------------------------------------------------------------------------------------
9 : E.add the new node’s DNS name to the conf/slaves file on the master node.

Explanation:http://www.quora.com/How-to-add-a-node-in-Hadoop-cluster

--------------------------------------------------------------------------------------------------
10 : C. Restart the NameNode

--------------------------------------------------------------------------------------------------
11 : A. Yarn application –kill application_1374638600275_0109

--------------------------------------------------------------------------------------------------
12 : D. Restart the NameNode and ResourceManager deamons and resubmit any running jobs

--------------------------------------------------------------------------------------------------
13 : B. Run the ResourceManager on a different master from the NameNode in the order to load share HDFS metadata processing

--------------------------------------------------------------------------------------------------
14 : D. It only keeps track of which NameNode is Active at any given time

--------------------------------------------------------------------------------------------------
15 : B. The Mapper transfers the intermediate data immediately to the Reducers as it generated by the Map task

--------------------------------------------------------------------------------------------------
16 : A. NodeManager

--------------------------------------------------------------------------------------------------
17 : B. When your workload generates a large amount of intermediate data, on the order of the input data itself

--------------------------------------------------------------------------------------------------
18 : D. On the local disk of the slave node running the task

--------------------------------------------------------------------------------------------------
19 : A. This file will be immediately re-replicated and all other HDFS operations on the cluster will halt until the cluster’s replication values are restored

--------------------------------------------------------------------------------------------------
20 : C. TaskTracker

20 : E. ResourceManager

20 : F. JobTracker

--------------------------------------------------------------------------------------------------
21 : C. You get a warning that foo.txt is being overwritten

21 : D. You get an error message telling you that foo.txt already exists, and asking you if you would like to overwrite

--------------------------------------------------------------------------------------------------
22 : C. Install the impalad daemon and the impala shell on your gateway machine, and the statestored daemon and catalog daemon on one of the nodes in the cluster 

--------------------------------------------------------------------------------------------------
23 : A. In YARN, resource allocation is a function of virtual cores specified by the ApplicationMaster making requests to the NodeManager where a reduce task is handled by a single container (and this a single virtual core). Thus, the developer needs to specify the number of virtual cores to the NodeManager by executing –p yarn.nodemanager.cpuvcores= 2

--------------------------------------------------------------------------------------------------
24 : B. Set dfs.block.size to 134217728 on all the worker nodes and client machines, and set the parameter to final. You do need to set this value on the NameNode.

--------------------------------------------------------------------------------------------------
25 : B. Even for small clusters on a single rack, configuring rack awareness will improve performance.

25 : C. Rack location is considered in the HDFS block placement policy

--------------------------------------------------------------------------------------------------
26 : C. ResourceManager

--------------------------------------------------------------------------------------------------
27 : D. They must be formatted as either ext3 or ext4

--------------------------------------------------------------------------------------------------
28 : A. The client queries the NameNode which retrieves the block from the nearest DataNode to the client and then passes that block back to the client.

--------------------------------------------------------------------------------------------------
29 : D. They will see the file with its original name. if they attempt to view the file, they will get a ConcurrentFileAccessException until the entire file write is completed on the cluster

--------------------------------------------------------------------------------------------------
30 : A. 17.2 GB

--------------------------------------------------------------------------------------------------
31 : C. memcat 

--------------------------------------------------------------------------------------------------
32 : D. SampleJar.jar is sent to the ApplicationMaster which allocation a container for Sample.jar

--------------------------------------------------------------------------------------------------
33 : A. Configure the NodeManager hostname and enable services on YARN by setting the 

--------------------------------------------------------------------------------------------------
34 : C. Mapreduce.map.java.opts=-Xmx3072m

--------------------------------------------------------------------------------------------------
35 : A. Sample the web server logs web servers and copy them into HDFS using curl

35 : D. Ingest the server web logs into HDFS using Flume

--------------------------------------------------------------------------------------------------
36 : D. nn01 is fenced, and nn02 becomes the active NameNode

--------------------------------------------------------------------------------------------------
37 : A. Yes. The daemon will receive data from the NameNode to run Map tasks

--------------------------------------------------------------------------------------------------
38 : C. JobTracker

--------------------------------------------------------------------------------------------------
39 : C. When job B gets submitted, Job A has to finish first, before job B can scheduled

--------------------------------------------------------------------------------------------------
40 : D. Tune the io.sort.mb value until you observe that the number of spilled records equals (or is as close to equals) the number of map output records

--------------------------------------------------------------------------------------------------
41 : A. Set vm.swappiness to o in /etc/sysctl.conf

--------------------------------------------------------------------------------------------------
42 : A. Fair Scheduler

--------------------------------------------------------------------------------------------------
43 : B. Modify yarn-site.xml with the following property: &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;16&lt;/value&gt;

--------------------------------------------------------------------------------------------------
44 : B. The NameNode will update the dfs.hosts property to include machine running DataNode daemon on the next NameNode reboot or with the command dfsadmin -refreshNodes

--------------------------------------------------------------------------------------------------
45 : E. 3

--------------------------------------------------------------------------------------------------
46 : B. Oozie

--------------------------------------------------------------------------------------------------
47 : A. You must modify the configuration files on the NameNode only. DataNodes read their configuration from the master nodes.

47 : E. You must restart the NameNode daemon to apply the changes to the cluster

--------------------------------------------------------------------------------------------------
48 : D. Resource pressures on the JobTracker

48 : E. Ability to run frameworks other than MapReduce, such as MPI

--------------------------------------------------------------------------------------------------
49 : A. All hard drives may be used to store HDFS blocks as long as atleast 100 GB in total is available on the node

--------------------------------------------------------------------------------------------------
50 : C.Not until all mappers have finished processing all records.

Explanation:In a MapReduce job reducers do not start executing the reduce method until the allMap jobs have completed. Reducers start copying intermediate key-value pairs from the mappersas soon as they are available. The programmer defined reduce method is called only after all themappers have finished.Note: The reduce phase has 3 steps: shuffle, sort, reduce. Shuffle is where the data is collected bythe reducer from each mapper. This can happen while mappers are generating data since it is onlya data transfer. On the other hand, sort and reduce can only start once all the mappers are done.Why is starting the reducers early a good thing? Because it spreads out the data transfer from themappers to the reducers over time, which is a good thing if your network is the bottleneck.Why is starting the reducers early a bad thing? Because they “hog up” reduce slots while onlycopying data. Another job that starts later that will actually use the reduce slots now can’t usethem.You can customize when the reducers startup by changing the default value ofmapred.reduce.slowstart.completed.maps in mapred-site.xml. A value of 1.00 will wait for all themappers to finish before starting the reducers. A value of 0.0 will start the reducers right away. Avalue of 0.5 will start the reducers when half of the mappers are complete. You can also changemapred.reduce.slowstart.completed.maps on a job-by-job basis.Typically, keep mapred.reduce.slowstart.completed.maps above 0.9 if the system ever hasmultiple jobs running at once. This way the job doesn’t hog up reducers when they aren’t doinganything but copying data. If you only ever have one job running at a time, doing 0.1 wouldprobably be appropriate.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, When is thereducers are started in a MapReduce job?

--------------------------------------------------------------------------------------------------
51 : C.The client contacts the NameNode for the block location(s). The NameNode then queries the DataNodes for block locations. The DataNodes respond to the NameNode, and the NameNode redirects the client to the DataNode that holds the requested data block(s). The client then reads the data directly off the DataNode.

Explanation:The Client communication to HDFS happens using Hadoop HDFS API. Clientapplications talk to the NameNode whenever they wish to locate a file, or when they want toadd/copy/move/delete a file on HDFS. The NameNode responds the successful requests byreturning a list of relevant DataNode servers where the data lives. Client applications can talkdirectly to a DataNode, once the NameNode has provided the location of the data.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, How the Clientcommunicates with HDFS?

--------------------------------------------------------------------------------------------------
52 : D.Reducer &lt;Text, IntWritable, Text, IntWritable&gt; 

--------------------------------------------------------------------------------------------------
53 : D.Hadoop Streaming

Explanation:Hadoop streaming is a utility that comes with the Hadoop distribution. The utilityallows you to create and run Map/Reduce jobs with any executable or script as the mapper and/orthe reducer.Reference: http://hadoop.apache.org/common/docs/r0.20.1/streaming.html (Hadoop Streaming,second sentence)

--------------------------------------------------------------------------------------------------
54 : A.Keys are presented to reducer in sorted order; values for a given key are not sorted.

--------------------------------------------------------------------------------------------------
55 : D.The keys given to a reducer are in sorted order but the values associated with each key are in no predictable order

--------------------------------------------------------------------------------------------------
56 : E.You will have twenty failed task attempts 

--------------------------------------------------------------------------------------------------
57 : D.configure 

--------------------------------------------------------------------------------------------------
58 : F.Combiner

Explanation:Combiners are used to increase the efficiency of a MapReduce program. They areused to aggregate intermediate map output locally on individual mapper outputs. Combiners canhelp you reduce the amount of data that needs to be transferred across to the reducers. You canuse your reducer code as a combiner if the operation performed is commutative and associative.

--------------------------------------------------------------------------------------------------
59 : A.Yes.

--------------------------------------------------------------------------------------------------
60 : D.Into in-memory buffers that spill over to the local file system (outside HDFS) of the TaskTracker node running the Reducer

Explanation:The mapper output (intermediate data) is stored on the Local file system (NOTHDFS) of each individual mapper nodes. This is typically a temporary directory location which canbe setup in config by the hadoop administrator. The intermediate data is cleaned up after theHadoop Job completes.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, Where is theMapper Output (intermediate kay-value data) stored ?

--------------------------------------------------------------------------------------------------
61 : B.Write a MapReduce job, with the web servers for mappers, and the Hadoop cluster nodes for reduces.

--------------------------------------------------------------------------------------------------
62 : B.Resource pressure on the JobTracker.

62 : D.Ability to run frameworks other than MapReduce, such as MPI.

--------------------------------------------------------------------------------------------------
63 : C.hadoop MyDrive –D mapred.job.name=Example input output

Explanation:Configure the property using the -D key=value notation:-D mapred.job.name=’My Job’You can list a whole bunch of options by calling the streaming jar with just the -info argumentReference: Python hadoop streaming : Setting a job name

--------------------------------------------------------------------------------------------------
64 : D.The InputFormat used by the job determines the mapper’s input key and value types.

Explanation:The input types fed to the mapper are controlled by the InputFormat used. Thedefault input format, “TextInputFormat,” will load data in as (LongWritable, Text) pairs. The longvalue is the byte offset of the line in the file. The Text object holds the string contents of the line ofthe file.Note: The data types emitted by the reducer are identified by setOutputKeyClass()andsetOutputValueClass(). The data types emitted by the reducer are identified bysetOutputKeyClass() and setOutputValueClass().By default, it is assumed that these are the output types of the mapper as well. If this is not thecase, the methods setMapOutputKeyClass() and setMapOutputValueClass() methods of theJobConf class will override these.Reference: Yahoo! Hadoop Tutorial, THE DRIVER METHOD

--------------------------------------------------------------------------------------------------
65 : C.ApplicationMaster

Explanation:The fundamental idea of MRv2 (YARN) is to split up the two major functionalities ofthe JobTracker, resource management and job scheduling/monitoring, into separate daemons.The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM).An application is either a single job in the classical sense of Map-Reduce jobs or a DAG of jobs.Note: Let’s walk through an application execution sequence :Reference: Apache Hadoop YARN – Concepts &amp; Applications

--------------------------------------------------------------------------------------------------
66 : E.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReader of the split that contains the end of the broken line.

Explanation:As the Map operation is parallelized the input file set is first split to several piecescalled FileSplits. If an individual file is so large that it will affect seek time it will be split to severalSplits. The splitting does not know anything about the input file’s internal logical structure, forexample line-oriented text files are split on arbitrary byte boundaries. Then a new map task iscreated per FileSplit.When an individual map task starts it will open a new output writer per configured reduce task. Itwill then proceed to read its FileSplit using the RecordReader it gets from the specifiedInputFormat. InputFormat parses the input and generates key-value pairs. InputFormat must alsohandle records that may be split on the FileSplit boundary. For example TextInputFormat will readthe last line of the FileSplit past the split boundary and, when reading other than the first FileSplit,TextInputFormat ignores the content up to the first newline.Reference: How Map and Reduce operations are actually carried out

--------------------------------------------------------------------------------------------------
67 : E.As many intermediate key-value pairs as designed, as long as all the keys have the same types and all the values have the same type.

Explanation:Mapper maps input key/value pairs to a set of intermediate key/value pairs.Maps are the individual tasks that transform input records into intermediate records. Thetransformed intermediate records do not need to be of the same type as the input records. A giveninput pair may map to zero or many output pairs.Reference: Hadoop Map-Reduce Tutorial

--------------------------------------------------------------------------------------------------
68 : A.Six 

Explanation:Only one key value pair will be passed from the two (The, 1) key value pairs.

--------------------------------------------------------------------------------------------------
69 : B.Pig LOAD command

--------------------------------------------------------------------------------------------------
70 : C.By using multiple reducers with the default HashPartitioner, output files may not be in globally sorted order.

Explanation:Multiple reducers and total orderingIf your sort job runs with multiple reducers (either because mapreduce.job.reduces in mapredsite.xml has been set to a number larger than 1, or because you’ve used the -r option to specifythe number of reducers on the command-line), then by default Hadoop will use the HashPartitionerto distribute records across the reducers. Use of the HashPartitioner means that you can’tconcatenate your output files to create a single sorted output file. To do this you’ll need totalordering,Reference: Sorting text files with MapReduce

--------------------------------------------------------------------------------------------------
71 : B.SequenceFileInputFormat

--------------------------------------------------------------------------------------------------
72 : C.When submitting the job on the command line, specify the –libjars option followed by the JAR file path. 

Explanation:The usage of the jar command is like this,Usage: hadoop jar &lt;jar&gt; [mainClass] args… If you want the commons-math3.jar to be available for all the tasks you can do any one of these 1. Copy the jar file in $HADOOP_HOME/lib dir or 2. Use the generic option -libjars.

--------------------------------------------------------------------------------------------------
73 : E.Speculative Execution

Explanation:Speculative execution: One problem with the Hadoop system is that by dividing thetasks across many nodes, it is possible for a few slow nodes to rate-limit the rest of the program.For example if one node has a slow disk controller, then it may be reading its input at only 10% thespeed of all the other nodes. So when 99 map tasks are already complete, the system is stillwaiting for the final map task to check in, which takes much longer than all the other nodes.By forcing tasks to run in isolation from one another, individual tasks do not know where theirinputs come from. Tasks trust the Hadoop platform to just deliver the appropriate input. Therefore,the same input can be processed multiple times in parallel, to exploit differences in machinecapabilities. As most of the tasks in a job are coming to a close, the Hadoop platform will scheduleredundant copies of the remaining tasks across several nodes which do not have other work toperform. This process is known as speculative execution. When tasks complete, they announcethis fact to the JobTracker. Whichever copy of a task finishes first becomes the definitive copy. Ifother copies were executing speculatively, Hadoop tells the TaskTrackers to abandon the tasksand discard their outputs. The Reducers then receive their inputs from whichever Mappercompleted successfully, first.Reference: Apache Hadoop, Module 4: MapReduceNote:* Hadoop uses “speculative execution.” The same task may be started on multiple boxes. The firstone to finish wins, and the other copies are killed.Failed tasks are tasks that error out.* There are a few reasons Hadoop can kill tasks by his own decisions: a) Task does not report progress during timeout (default is 10 minutes) b) FairScheduler or CapacityScheduler needs the slot for some other pool (FairScheduler) orqueue (CapacityScheduler). c) Speculative execution causes results of task not to be needed since it has completed on otherplace.Reference: Difference failed tasks vs killed tasks

--------------------------------------------------------------------------------------------------
74 : E.One final key-value pair per key; no restrictions on the type.

Explanation:Reducer reduces a set of intermediate values which share a key to a smaller set ofvalues.Reducing lets you aggregate values together. A reducer function receives an iterator of inputvalues from an input list. It then combines these values together, returning a single output value.Reference: Hadoop Map-Reduce Tutorial; Yahoo! Hadoop Tutorial, Module 4: MapReduce

--------------------------------------------------------------------------------------------------
75 : C.All data for a given key, regardless of which mapper(s) produced it.

Explanation:Reducing lets you aggregate values together. A reducer function receives an iteratorof input values from an input list. It then combines these values together, returning a single outputvalue.All values with the same key are presented to a single reduce task.Reference: Yahoo! Hadoop Tutorial, Module 4: MapReduce

--------------------------------------------------------------------------------------------------
76 : C.Implement WritableComparable.

Explanation:The MapReduce framework operates exclusively on &lt;key, value&gt; pairs, that is, theframework views the input to the job as a set of &lt;key, value&gt; pairs and produces a set of &lt;key,value&gt; pairs as the output of the job, conceivably of different types.The key and value classes have to be serializable by the framework and hence need to implementthe Writable interface. Additionally, the key classes have to implement the WritableComparableinterface to facilitate sorting by the framework.Reference: MapReduce Tutorial

--------------------------------------------------------------------------------------------------
77 : E.The location of the InsputSplit to be processed in relation to the location of the node.

--------------------------------------------------------------------------------------------------
78 : D.A SequenceFile contains a binary encoding of an arbitrary number key-value pairs. Each key must be the same type. Each value must be the same type. 

Explanation:SequenceFile is a flat file consisting of binary key/value pairs.There are 3 different SequenceFile formats:Uncompressed key/value records.Record compressed key/value records – only ‘values’ are compressed here.Block compressed key/value records – both keys and values are collected in ‘blocks’ separatelyand compressed. The size of the ‘block’ is configurable.Reference: http://wiki.apache.org/hadoop/SequenceFile

--------------------------------------------------------------------------------------------------
79 : D.The file can be accessed if at least one of the data nodes storing the file is available.

Explanation:HDFS keeps three copies of a block on three different datanodes to protect againsttrue data corruption. HDFS also tries to distribute these three replicas on more than one rack toprotect against data availability issues. The fact that HDFS actively monitors any faileddatanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed)implies that three copies of data on three different nodes is sufficient to avoid corrupted files.Note: HDFS is designed to reliably store very large files across machines in a large cluster. It storeseach file as a sequence of blocks; all blocks in a file except the last block are the same size. Theblocks of a file are replicated for fault tolerance. The block size and replication factor areconfigurable per file. An application can specify the number of replicas of a file. The replicationfactor can be specified at file creation time and can be changed later. Files in HDFS are write-onceand have strictly one writer at any time. The NameNode makes all decisions regarding replicationof blocks. HDFS uses rack-aware replica placement policy. In default configuration there are total3 copies of a datablock on HDFS, 2 copies are stored on datanodes on same rack and 3rd copyon a different rack.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , How theHDFS Blocks are replicated?

--------------------------------------------------------------------------------------------------
80 : D.Write a custom FileInputFormat and override the method isSplitable to always return false.

--------------------------------------------------------------------------------------------------
81 : C.The TaskTracker spawns a new Mapper to process each key-value pair.

--------------------------------------------------------------------------------------------------
82 : D.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The reduce method is called as soon as the intermediate key-value pairs start to arrive.

--------------------------------------------------------------------------------------------------
83 : B.Place the data file in the DistributedCache and read the data into memory in the map method of the mapper.

--------------------------------------------------------------------------------------------------
84 : B.3

Explanation:reduce() gets called once for each [key, (list of values)] pair. To explain, let’s sayyou called:out.collect(new Text(“Car”),new Text(“Subaru”);out.collect(new Text(“Car”),new Text(“Honda”);out.collect(new Text(“Car”),new Text(“Ford”);out.collect(new Text(“Truck”),new Text(“Dodge”);out.collect(new Text(“Truck”),new Text(“Chevy”);Then reduce() would be called twice with the pairsreduce(Car, &lt;Subaru, Honda, Ford&gt;)reduce(Truck, &lt;Dodge, Chevy&gt;)Reference: Mapper output.collect()?

--------------------------------------------------------------------------------------------------
85 : B.The values are arbitrarily ordered, and the ordering may vary from run to run of the same MapReduce job.

--------------------------------------------------------------------------------------------------
86 : B.Disk I/O and network I/O

--------------------------------------------------------------------------------------------------
87 : D.JobTracker

--------------------------------------------------------------------------------------------------
88 : A.Yes, because the sum operation is both associative and commutative and the input and output types to the reduce method match.

Explanation:Combiners are used to increase the efficiency of a MapReduce program. They areused to aggregate intermediate map output locally on individual mapper outputs. Combiners canhelp you reduce the amount of data that needs to be transferred across to the reducers. You canuse your reducer code as a combiner if the operation performed is commutative and associative.The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, ifrequired it may execute it more then 1 times. Therefore your MapReduce jobs should not dependon the combiners execution.

--------------------------------------------------------------------------------------------------
89 : A.HBase

--------------------------------------------------------------------------------------------------
90 : D.They would see no content until the whole file written and closed.

Explanation:Note:* putUsage: hadoop fs -put &lt;localsrc&gt; … &lt;dst&gt;Copy single src, or multiple srcs from local file system to the destination filesystem. Also readsinput from stdin and writes to destination filesystem.

--------------------------------------------------------------------------------------------------
91 : C.Pig

91 : E.Hive

Explanation:Sqoop (“SQL-to-Hadoop”) is a straightforward command-line tool with the following capabilities:Imports individual tables or entire databases to files in HDFSGenerates Java classes to allow you to interact with your imported dataProvides the ability to import from SQL databases straight into your Hive data warehouseNote: Data Movement Between Hadoop and Relational DatabasesData can be moved between Hadoop and a relational database as a bulk data transfer, orrelational tables can be accessed from within a MapReduce map function.Note: * Cloudera’s Distribution for Hadoop provides a bulk data transfer tool (i.e., Sqoop) that importsindividual tables or entire databases into HDFS files. The tool also generates Java classes thatsupport interaction with the imported data. Sqoop supports all relational databases over JDBC,and Quest Software provides a connector (i.e., OraOop) that has been optimized for access todata residing in Oracle databases.Reference: http://log.medcl.net/item/2011/08/hadoop-and-mapreduce-big-data-analytics-gartner/(Data Movement between hadoop and relational databases, second paragraph)

--------------------------------------------------------------------------------------------------
92 : C.Two, file names with a leading period or underscore are ignored

Explanation:Files starting with ‘_’ are considered ‘hidden’ like unix files starting with ‘.’.# characters are allowed in HDFS file names.

--------------------------------------------------------------------------------------------------
93 : D.With zero reducers, instances of matching patterns are stored in multiple files on HDFS. With one reducer, all instances of matching patterns are gathered together in one file on HDFS.

--------------------------------------------------------------------------------------------------
94 : B.The amount of intermediate data that must be transferred between the mapper and reducer.

Explanation:Combiners are used to increase the efficiency of a MapReduce program. They areused to aggregate intermediate map output locally on individual mapper outputs. Combiners canhelp you reduce the amount of data that needs to be transferred across to the reducers. You canuse your reducer code as a combiner if the operation performed is commutative and associative.The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, ifrequired it may execute it more then 1 times. Therefore your MapReduce jobs should not dependon the combiners execution.

--------------------------------------------------------------------------------------------------
95 : D.At least 500.

Explanation:From Cloudera Training Course:Task attempt is a particular instance of an attempt to execute a task– There will be at least as many task attempts as there are tasks– If a task attempt fails, another will be started by the JobTracker– Speculative execution can also result in more task attempts than completed tasks

--------------------------------------------------------------------------------------------------
96 : B.Resource management

96 : D.Job coordination between the ResourceManager and NodeManager

--------------------------------------------------------------------------------------------------
97 : C.Algorithms that require global, sharing states.

--------------------------------------------------------------------------------------------------
98 : C.It returns a reference to the same Writable object each time, but populated with different data.

Explanation:Calling Iterator.next() will always return the SAME EXACT instance of IntWritable,with the contents of that instance replaced with the next value.Reference: manupulating iterator in mapreduce

--------------------------------------------------------------------------------------------------
99 : C.Stored in the Metastore.

--------------------------------------------------------------------------------------------------
100 : D.The default partitioner computers the hash of the key and divides that valule modulo the number of reducers. The result determines the reducer assigned to process the key-value pair.

--------------------------------------------------------------------------------------------------
101 : C.Decrease the block size on your remaining files.

Explanation:Note:* -put localSrc destCopies the file or directory from the local file system identified by localSrc todest within the DFS.* What is HDFS Block size? How is it different from traditional file system block size?In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each block istypically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicateeach block three times. Replicas are stored on different nodes. HDFS utilizes the local file systemto store each HDFS block as a separate file. HDFS Block size can not be compared with thetraditional file system block size.

--------------------------------------------------------------------------------------------------
102 : A.mXn (i.e., m multiplied by n)

Explanation:A MapReduce job with m mappers and r reducers involves up to m * r distinct copyoperations, since each mapper may have intermediate output going to every reducer.

--------------------------------------------------------------------------------------------------
103 : A.Sequences of MapReduce and Pig. These sequences can be combined with other actions including forks, decision points, and path joins.

Explanation:Oozie workflow is a collection of actions (i.e. Hadoop Map/Reduce jobs, Pig jobs)arranged in a control dependency DAG (Direct Acyclic Graph), specifying a sequence of actionsexecution. This graph is specified in hPDL (a XML Process Definition Language).hPDL is a fairly compact language, using a limited amount of flow control and action nodes.Control nodes define the flow of execution and include beginning and end of a workflow (start, endand fail nodes) and mechanisms to control the workflow execution path ( decision, fork and joinnodes).Workflow definitionsCurrently running workflow instances, including instance states and variablesReference: Introduction to OozieNote: Oozie is a Java Web-Application that runs in a Java servlet-container – Tomcat and uses adatabase to store:

--------------------------------------------------------------------------------------------------
104 : D.It accepts a single key-value pairs as input and can emit any number of key-value pair as output, including zero.

Explanation:public class Mapper&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt;extends ObjectMaps input key/value pairs to a set of intermediate key/value pairs.Maps are the individual tasks which transform input records into a intermediate records. Thetransformed intermediate records need not be of the same type as the input records. A given inputpair may map to zero or many output pairs.Reference: org.apache.hadoop.mapreduce Class Mapper&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt;

--------------------------------------------------------------------------------------------------
105 : A.When the types of the reduce operation’s input key and input value match the types of the reducer’s output key and output value and when the reduce operation is both communicative and associative.

Explanation:You can use your reducer code as a combiner if the operation performed iscommutative and associative.

--------------------------------------------------------------------------------------------------
106 : A.SequenceFiles

Explanation:Using Hadoop Sequence FilesSo what should we do in order to deal with huge amount of images? Use hadoop sequence files!Those are map files that inherently can be read by map reduce applications – there is an inputformat especially for sequence files – and are splitable by map reduce, so we can have one hugefile that will be the input of many map tasks. By using those sequence files we are letting hadoopuse its advantages. It can split the work into chunks so the processing is parallel, but the chunksare big enough that the process stays efficient.Since the sequence file are map file the desired format will be that the key will be text and hold theHDFS filename and the value will be BytesWritable and will contain the image content of the file.Reference: Hadoop binary files processing introduced by image duplicates finder

--------------------------------------------------------------------------------------------------
107 : A.Run all the nodes in your production cluster as virtual machines on your development workstation.

Explanation:Hosting on local VMsAs well as large-scale cloud infrastructures, there is another deployment pattern: local VMs ondesktop systems or other development machines. This is a good tactic if your physical machinesrun windows and you need to bring up a Linux system running Hadoop, and/or you want tosimulate the complexity of a small Hadoop cluster.Have enough RAM for the VM to not swap.Don’t try and run more than one VM per physical host, it will only make things slower.use file: URLs to access persistent input and output data.consider making the default filesystem a file: URL so that all storage is really on the physical host.It’s often faster and preserves data better.

--------------------------------------------------------------------------------------------------
108 : C.200

--------------------------------------------------------------------------------------------------
109 : D.Input file splits may cross line breaks. A line that crosses file splits is read by the RecordReader of the split that contains the end of the broken line.

Explanation:As the Map operation is parallelized the input file set is first split to several piecescalled FileSplits. If an individual file is so large that it will affect seek time it will be split to severalSplits. The splitting does not know anything about the input file’s internal logical structure, forexample line-oriented text files are split on arbitrary byte boundaries. Then a new map task iscreated per FileSplit.When an individual map task starts it will open a new output writer per configured reduce task. Itwill then proceed to read its FileSplit using the RecordReader it gets from the specifiedInputFormat. InputFormat parses the input and generates key-value pairs. InputFormat must alsohandle records that may be split on the FileSplit boundary. For example TextInputFormat will readthe last line of the FileSplit past the split boundary and, when reading other than the first FileSplit,TextInputFormat ignores the content up to the first newline.Reference: How Map and Reduce operations are actually carried outhttp://wiki.apache.org/hadoop/HadoopMapReduce (Map, second paragraph)

--------------------------------------------------------------------------------------------------
110 : D.Pig provides the additional capability of allowing you to control the flow of multiple MapReduce jobs.

Explanation:In addition to providing many relational and data flow operators Pig Latin providesways for you to control how your jobs execute on MapReduce. It allows you to set values thatcontrol your environment and to control details of MapReduce such as how your data ispartitioned.Reference: http://ofps.oreilly.com/titles/9781449302641/advanced_pig_latin.html (topic: controllingexecution)

--------------------------------------------------------------------------------------------------
111 : E.Sqoop

--------------------------------------------------------------------------------------------------
112 : C.Hive

--------------------------------------------------------------------------------------------------
113 : A.As key-value pairs in the jobconf object.

Explanation:In Hadoop, it is sometimes difficult to pass arguments to mappers and reducers. Ifthe number of arguments is huge (e.g., big arrays), DistributedCache might be a good choice.However, here, we’re discussing small arguments, usually a hand of configuration parameters.In fact, the way to configure these parameters is simple. When you initialize “JobConf” object tolaunch a mapreduce job, you can set the parameter by using “set” method like:1 JobConf job = (JobConf)getConf();2 job.set(“NumberOfDocuments”, args[0]);Here, “NumberOfDocuments” is the name of parameter and its value is read from “args[0]“, acommand line argument.Reference: Passing Parameters and Arguments to Mapper and Reducer in Hadoop

--------------------------------------------------------------------------------------------------
114 : C.hadoop jar MyJar.jar MyDriverClass inputdir outputdir

Explanation:Example:Run the application:$ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input/usr/joe/wordcount/output

--------------------------------------------------------------------------------------------------
115 : C.A failed task attempt is a task attempt that completed, but with an unexpected status value. A killed task attempt is a duplicate copy of a task attempt that was started as part of speculative execution.

Explanation:Note:* Hadoop uses “speculative execution.” The same task may be started on multiple boxes. The firstone to finish wins, and the other copies are killed.Failed tasks are tasks that error out.* There are a few reasons Hadoop can kill tasks by his own decisions: a) Task does not report progress during timeout (default is 10 minutes) b) FairScheduler or CapacityScheduler needs the slot for some other pool (FairScheduler) orqueue (CapacityScheduler). c) Speculative execution causes results of task not to be needed since it has completed on otherplace.Reference: Difference failed tasks vs killed tasks

--------------------------------------------------------------------------------------------------
116 : A.Lightweight devices for bookkeeping within MapReduce programs.

Explanation:Counters are a useful channel for gathering statistics about the job; for qualitycontrol, or for application-level statistics. They are also useful for problem diagnosis. Hadoopmaintains some built-in counters for every job, which reports various metrics for your job.Hadoop MapReduce also allows the user to define a set of user-defined counters that can beincremented (or decremented by specifying a negative value as the parameter), by the driver,mapper or the reducer.Reference: Iterative MapReduce and Counters, Introduction to Iterative MapReduce and Countershttp://hadooptutorial.wikispaces.com/Iterative+MapReduce+and+Counters (counters, secondparagraph)

--------------------------------------------------------------------------------------------------
117 : C.Online transaction processing (OLTP) for an e-commerce Website.

Explanation:Hadoop Map/Reduce is designed for batch-oriented work load.MapReduce is well suited for data warehousing (OLAP), but not for OLTP.

--------------------------------------------------------------------------------------------------
118 : C.200

Explanation:Each file would be split into two as the block size (64 MB) is less than the file size(100 MB), so 200 mappers would be running.Note:If you’re not compressing the files then hadoop will process your large files (say 10G), with anumber of mappers related to the block size of the file.Say your block size is 64M, then you will have ~160 mappers processing this 10G file (160*64 ~=10G). Depending on how CPU intensive your mapper logic is, this might be an acceptable blockssize, but if you find that your mappers are executing in sub minute times, then you might want toincrease the work done by each mapper (by increasing the block size to 128, 256, 512m – theactual size depends on how you intend to process the data).Reference: http://stackoverflow.com/questions/11014493/hadoop-mapreduce-appropriate-inputfiles-size (first answer, second paragraph)

--------------------------------------------------------------------------------------------------
119 : D.No, each reducer runs independently and in isolation.

Explanation:MapReduce programming model does not allow reducers to communicate with eachother. Reducers run in isolation.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developershttp://www.fromdev.com/2010/12/interview-questions-hadoop-mapreduce.html (See question no.9)

--------------------------------------------------------------------------------------------------
120 : D.It accepts a single key-value pair as input and can emit any number of key-value pairs as output, including zero.

Explanation:public class Mapper&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt;extends ObjectMaps input key/value pairs to a set of intermediate key/value pairs.Maps are the individual tasks which transform input records into a intermediate records. Thetransformed intermediate records need not be of the same type as the input records. A given inputpair may map to zero or many output pairs.Reference: org.apache.hadoop.mapreduce Class Mapper&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt;

--------------------------------------------------------------------------------------------------
121 : C.JobTracker

Explanation:JobTracker is the daemon service for submitting and tracking MapReduce jobs inHadoop. There is only One Job Tracker process run on any hadoop cluster. Job Tracker runs onits own JVM process. In a typical production cluster its run on a separate machine. Each slavenode is configured with job tracker node location. The JobTracker is single point of failure for theHadoop MapReduce service. If it goes down, all running jobs are halted. JobTracker in Hadoopperforms following actions(from Hadoop Wiki:)Client applications submit jobs to the Job tracker.The JobTracker talks to the NameNode to determine the location of the dataThe JobTracker locates TaskTracker nodes with available slots at or near the dataThe JobTracker submits the work to the chosen TaskTracker nodes.The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, theyare deemed to have failed and the work is scheduled on a different TaskTracker.A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to dothen: it may resubmit the job elsewhere, it may mark that specific record as something to avoid,and it may may even blacklist the TaskTracker as unreliable.When the work is completed, the JobTracker updates its status.Client applications can poll the JobTracker for information.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What is aJobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?

--------------------------------------------------------------------------------------------------
122 : A.TaskTracker

Explanation:Single instance of a Task Tracker is run on each Slave node. Task tracker is run asa separate JVM process.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What isconfiguration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?http://www.fromdev.com/2010/12/interview-questions-hadoop-mapreduce.html (See answer toquestion no. 5)

--------------------------------------------------------------------------------------------------
123 : B.Each slave node runs a TaskTracker and a DataNode daemon.

Explanation:Single instance of a Task Tracker is run on each Slave node. Task tracker is run asa separate JVM process.Single instance of a DataNode daemon is run on each Slave node. DataNode daemon is run as aseparate JVM process.One or Multiple instances of Task Instance is run on each slave node. Each task instance is run asa separate JVM process. The number of Task instances can be controlled by configuration.Typically a high end machine is configured to run more task instances.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What isconfiguration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?

--------------------------------------------------------------------------------------------------
124 : A.HDFS becomes unavailable until the NameNode is restored.

Explanation:The NameNode is a Single Point of Failure for the HDFS Cluster. When theNameNode goes down, the file system goes offline.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What is aNameNode? How many instances of NameNode run on a Hadoop Cluster?

--------------------------------------------------------------------------------------------------
125 : C.The node on which this InputSplit is stored

Explanation:The TaskTrackers send out heartbeat messages to the JobTracker, usually everyfew minutes, to reassure the JobTracker that it is still alive. These message also inform theJobTracker of the number of available slots, so the JobTracker can stay up to date with where inthe cluster work can be delegated. When the JobTracker tries to find somewhere to schedule atask within the MapReduce operations, it first looks for an empty slot on the same server thathosts the DataNode containing the data, and if not, it looks for an empty slot on a machine in thesame rack.

--------------------------------------------------------------------------------------------------
126 : B.When the NameNode fails to receive periodic heartbeats from the DataNode, it considers the DataNode as failed.

Explanation:NameNode periodically receives a Heartbeat and a Blockreport from each of theDataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly.A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has notrecieved a hearbeat message from a data node after a certain amount of time, the data node ismarked as dead. Since blocks will be under replicated the system begins replicating the blocksthat were stored on the dead datanode. The NameNode Orchestrates the replication of datablocks from one datanode to another. The replication data transfer happens directly betweendatanodes and the data never passes through the namenode.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, HowNameNode Handles data node failures?

--------------------------------------------------------------------------------------------------
127 : B.To store filenames, list of blocks and other meta information.

--------------------------------------------------------------------------------------------------
128 : A.Writable is an interface that all keys and values in MapReduce must implement. Classes implementing this interface must implement methods for serializing and deserializing themselves.

Explanation:public interface WritableA serializable object which implements a simple, efficient, serialization protocol, based onDataInput and DataOutput.Any key or value type in the Hadoop Map-Reduce framework implements this interface.Implementations typically implement a static read(DataInput) method which constructs a newinstance, calls readFields(DataInput) and returns the instance.Reference: org.apache.hadoop.io, Interface Writable

--------------------------------------------------------------------------------------------------
129 : D.Keys are presented to a reducer in random order; values for a given key are sorted in ascending order.

--------------------------------------------------------------------------------------------------
130 : D.The default partitioner computes the hash of the key and divides that value modulo the number of reducers. The result determines the reducer assigned to process the key-value pair.

Explanation:The default partitioner computes a hash value for the key and assigns the partitionbased on this result.The default Partitioner implementation is called HashPartitioner. It uses the hashCode() method ofthe key objects modulo the number of partitions total to determine which partition to send a given(key, value) pair to.In Hadoop, the default partitioner is HashPartitioner, which hashes a record’s key to determinewhich partition (and thus which reducer) the record belongs in. The number of partition is thenequal to the number of reduce tasks for the job.Reference: Getting Started With (Customized) Partitioning

--------------------------------------------------------------------------------------------------
131 : C.Intermediate key-value pairs are written to the local disks of the machines running the map tasks, and then copied to the machine running the reduce tasks.

Explanation:The mapper output (intermediate data) is stored on the Local file system (NOTHDFS) of each individual mapper nodes. This is typically a temporary directory location which canbe setup in config by the hadoop administrator. The intermediate data is cleaned up after theHadoop Job completes.Note: * Reducers start copying intermediate key-value pairs from the mappers as soon as they areavailable. The progress calculation also takes in account the processing of data transfer which isdone by reduce process, therefore the reduce progress starts showing up as soon as anyintermediate key-value pair for a mapper is available to be transferred to reducer. Though thereducer progress is updated still the programmer defined reduce method is called only after all themappers have finished.* Reducer is input the grouped output of a Mapper. In the phase the framework, for each Reducer,fetches the relevant partition of the output of all the Mappers, via HTTP.* Mapper maps input key/value pairs to a set of intermediate key/value pairs.Maps are the individual tasks that transform input records into intermediate records. Thetransformed intermediate records do not need to be of the same type as the input records. A giveninput pair may map to zero or many output pairs.* All intermediate values associated with a given output key are subsequently grouped by theframework, and passed to the Reducer(s) to determine the final output.Reference: Questions &amp; Answers for Hadoop MapReduce developers, Where is the MapperOutput (intermediate kay-value data) stored ?

--------------------------------------------------------------------------------------------------
132 : A.There will be r files, each with exactly k/r key-value pairs.

Explanation:Note: * A MapReduce job with m mappers and r reducers involves up to m * r distinct copy operations,since each mapper may have intermediate output going to every reducer.* In the canonical example of word counting, a key-value pair is emitted for every word found. Forexample, if we had 1,000 words, then 1,000 key-value pairs will be emitted from the mappers tothe reducer(s).

--------------------------------------------------------------------------------------------------
133 : B.Yes.

--------------------------------------------------------------------------------------------------
134 : B.No reducer executes, and the output of each mapper is written to a separate file in HDFS.

Explanation:* It is legal to set the number of reduce-tasks to zero if no reduction is desired.In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set bysetOutputPath(Path). The framework does not sort the map-outputs before writing them out to theFileSystem.* Often, you may want to process input data using a map function only. To do this, simply setmapreduce.job.reduces to zero. The MapReduce framework will not create any reducer tasks.Rather, the outputs of the mapper tasks will be the final output of the job.

--------------------------------------------------------------------------------------------------
135 : C.They aggregate intermediate map output locally on each individual machine and therefore reduce the amount of data that needs to be shuffled across the network to the reducers.

--------------------------------------------------------------------------------------------------
136 : D.mxr (i.e., m multiplied by r)

Explanation:A MapReduce job with m mappers and r reducers involves up to m * r distinct copyoperations, since each mapper may have intermediate output going to every reducer.

--------------------------------------------------------------------------------------------------
137 : B.A single reducer gathers and processes all the output from all the mappers. The output is written to a single file in HDFS.

--------------------------------------------------------------------------------------------------
138 : A.Because combiners perform local aggregation of word counts, thereby allowing the mappers to process input data faster.

Explanation:* Simply speaking a combiner can be considered as a “mini reducer” that will beapplied potentially several times still during the map phase before to send the new (hopefullyreduced) set of key/value pairs to the reducer(s). This is why a combiner must implement theReducer interface (or extend the Reducer class as of hadoop 0.20).* Combiners are used to increase the efficiency of a MapReduce program. They are used toaggregate intermediate map output locally on individual mapper outputs. Combiners can help youreduce the amount of data that needs to be transferred across to the reducers. You can use yourreducer code as a combiner if the operation performed is commutative and associative. Theexecution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, ifrequired it may execute it more then 1 times. Therefore your MapReduce jobs should not dependon the combiners execution.

--------------------------------------------------------------------------------------------------
139 : B.HDFS has the Characteristic of supporting a “write once, read many” data access model.

139 : D.HDFS is a distributed file system that runs on top of native OS filesystems and is well suited to storage of very large data sets.

--------------------------------------------------------------------------------------------------
140 : D.8.2 GB

--------------------------------------------------------------------------------------------------
141 : B.One active NameNode and one Standby NameNode

--------------------------------------------------------------------------------------------------
142 : B.Stored along with the data in HDFS

--------------------------------------------------------------------------------------------------
143 : D.On the local disk of the slave mode running the task

--------------------------------------------------------------------------------------------------
144 : B.When Job B gets submitted, Job A has to finish first, before job B can gets scheduled.

--------------------------------------------------------------------------------------------------
145 : C.Modify yarn-site.xml with the following property: &lt;name&gt;yarn.nodemanager.resource.cpu-vccores&lt;/name&gt;

--------------------------------------------------------------------------------------------------
146 : D.Set vm.swapfile file on the node

--------------------------------------------------------------------------------------------------
147 : D.JobTracker

147 : E.NameNode

--------------------------------------------------------------------------------------------------
148 : D.Tune the io.sort.mb value until you observe that the number of spilled records equals (or is as close to equals) the number of map output records.

--------------------------------------------------------------------------------------------------
149 : B.Execute hdfs dfsadmin –saveNamespace on the command line which returns to you the last checkpoint value in fstime file

Explanation:https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-10/hdfs

--------------------------------------------------------------------------------------------------
150 : E.10

--------------------------------------------------------------------------------------------------
151 : C.Set dfs.block.size to 128 M on all the worker nodes and client machines, and set the parameter to final. You do not need to set this value on the NameNode

Explanation:—

--------------------------------------------------------------------------------------------------
152 : B.The client queries the NameNode for the locations of the block, and reads from the first location in the list it receives.

--------------------------------------------------------------------------------------------------
153 : A.Yes. The daemon will receive data from the NameNode to run Map tasks

--------------------------------------------------------------------------------------------------
154 : D.Run the ResourceManager on a different master from the NameNode in order to load-share HDFS metadata processing

--------------------------------------------------------------------------------------------------
155 : B.The NameNode goes down

--------------------------------------------------------------------------------------------------
156 : A.It only keeps track of which NameNode is Active at any given time

Explanation:http://www.cloudera.com/content/cloudera-content/clouderadocs/CDH4/latest/PDF/CDH4-High-Availability-Guide.pdf(page 15)

--------------------------------------------------------------------------------------------------
157 : D.To ensure that there is consistent disk utilization across the DataNodes

Explanation:NOTE: There is only one correct answer in the options for this question. Pleasecheck the following reference:http://www.quora.com/Apache-Hadoop/It-is-recommended-that-you-run-the-HDFS-balancerperiodically-Why-Choose-3

--------------------------------------------------------------------------------------------------
158 : B.nn01 is fenced, and nn02 becomes the active NameNode

--------------------------------------------------------------------------------------------------
159 : E.Install the impalad daemon, statestored daemon, and catalogd daemon on each machine in the cluster and on the gateway node

--------------------------------------------------------------------------------------------------
160 : A.When your workload generates a large amount of output data, significantly larger than the amount of intermediate data

--------------------------------------------------------------------------------------------------
161 : A.You can specify new queue name when user submits a job and new queue can be created dynamically if the property yarn.scheduler.fair.allow-undecleared-pools = true

--------------------------------------------------------------------------------------------------
162 : D.A maximum if 100 GB on each hard drive may be used to store HDFS blocks

--------------------------------------------------------------------------------------------------
163 : B.You must modify the configuration files on each of the six SataNodes machines

163 : D.You must restart the NameNode daemon to apply the changes to the cluster

--------------------------------------------------------------------------------------------------
164 : A.Without creating a dfs.hosts file or making any entries, run the commands hadoop.dfsadminrefreshModes on the NameNode

--------------------------------------------------------------------------------------------------
165 : B.The cluster will re-replicate the file the next time the system administrator reboots the NameNode daemon (as long as the file’s replication factor doesn’t fall below)

--------------------------------------------------------------------------------------------------
166 : B.Yarn application –kill application_1374638600275_0109

Explanation:http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1-latest/bk_using-apachehadoop/content/common_mrv2_commands.html

--------------------------------------------------------------------------------------------------
167 : C.You get a warning that foo.txt is being overwritten

167 : E.You get a error message telling you that foo.txt already exists. The file is not written to HDFS

--------------------------------------------------------------------------------------------------
168 : B.They must be formatted as either ext3 or ext4

--------------------------------------------------------------------------------------------------
169 : D.Configure yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpuvcores to match the capacity you require under YARN for each NodeManager

--------------------------------------------------------------------------------------------------
170 : A.Oozie

--------------------------------------------------------------------------------------------------
171 : D.JobTracker

--------------------------------------------------------------------------------------------------
172 : B.Because your under-replicated blocks count matches the Live Nodes, one node is dead, and your DFS Used % equals 0%, you can’t be certain that your cluster has all the data you’ve written it.

--------------------------------------------------------------------------------------------------
173 : B.Encryption for data during transfer between the Mappers and Reducers

173 : D.Authentication for user access to the cluster against a central server

--------------------------------------------------------------------------------------------------
174 : B.Restart the NameNode and ResourceManager daemons and resubmit any running jobs.

--------------------------------------------------------------------------------------------------
175 : D.ResourceManager

Explanation:http://www.devx.com/opensource/intro-to-apache-mapreduce-2-yarn.html(See resourcemanager)

--------------------------------------------------------------------------------------------------
176 : C.The Mapper transfers the intermediate data immediately to the reducers as it is generated by the Map Task

--------------------------------------------------------------------------------------------------
177 : A.free

177 : D.top

177 : F.vmstat

Explanation:http://www.cyberciti.biz/faq/linux-check-swap-usage-command/

--------------------------------------------------------------------------------------------------
178 : D.They will see the file with its original name. If they attempt to view the file, they will get a ConcurrentFileAccessException until the entire file write is completed on the cluster

--------------------------------------------------------------------------------------------------
179 : B.Hdfs fsck

Explanation:https://twiki.grid.iu.edu/bin/view/Storage/HadoopRecovery

--------------------------------------------------------------------------------------------------
180 : B.Creates users for hdfs and mapreduce to facilitate role assignment

--------------------------------------------------------------------------------------------------
181 : A.Sample the web server logs web servers and copy them into HDFS using curl

181 : B.Ingest the server web logs into HDFS using Flume

--------------------------------------------------------------------------------------------------
182 : A.Configure the NodeManager to enable MapReduce services on YARN by setting the following property in yarn-site.xml: &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt; &lt;value&gt;your_nodeManager_shuffle&lt;/value&gt;

182 : B.Configure the NodeManager hostname and enable node services on YARN by setting the following property in yarn-site.xml: &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt; &lt;value&gt;your_nodeManager_hostname&lt;/value&gt;

182 : D.Configure the number of map tasks per jon YARN by setting the following property in mapred: &lt;name&gt;mapreduce.job.maps&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;

--------------------------------------------------------------------------------------------------
183 : A.CSV

183 : B.XML

--------------------------------------------------------------------------------------------------
184 : B.Single point of failure in the NameNode

184 : D.Resource pressure on the JobTracker

Explanation:http://www.revelytix.com/?q=content/hadoop-ecosystem(YARN, first para)

--------------------------------------------------------------------------------------------------
185 : A.ApplicationMaster

--------------------------------------------------------------------------------------------------
186 : B.Capacity Scheduler

Explanation:http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarnsite/CapacityScheduler.html

--------------------------------------------------------------------------------------------------
187 : C.ApplicationMaster

--------------------------------------------------------------------------------------------------
188 : C.Fair Scheduler

Explanation:http://hadoop.apache.org/docs/r1.2.1/fair_scheduler.html

--------------------------------------------------------------------------------------------------
189 : A.SampleJar.Jar is sent to the ApplicationMaster which allocates a container for SampleJar.Jar

--------------------------------------------------------------------------------------------------
190 : A.The client will query ZooKeeper to find the location of the new HMaster and complete the metadata change.

Explanation: the HBase master publishes its location to clients via Zookeeper. This is done tosupport multimaster operation (failover). So if the HBase master self-discovers its location as alocalhost address, then it will publish that. Region servers or clients which go to Zookeeper for themaster location will get back an address in that case only useful if they happen to be co-locatedwith the master.Note:* HMaster is the implementation of the Master Server. The Master server is responsible formonitoring all RegionServer instances in the cluster, and is the interface for all metadata changes.

--------------------------------------------------------------------------------------------------
191 : D.scan ‘Employees’, {STARTROW =&gt; ‘user_100’, STOPROW =&gt; ‘user_110’}

Explanation: public Scan(byte[] startRow,byte[] stopRow)Create a Scan operation for the range of rows specified.Parameters:startRow – row to start scanner at or after (inclusive)stopRow – row to stop scanner before (exclusive)Reference:o rg.apache.hadoop.hbase.client, Class Scan

--------------------------------------------------------------------------------------------------
192 : C.The entire row is deleted.

Explanation: When performing a delete operation in HBase, there are two ways to specify theversions to be deletedDelete all versions older than a certain timestampDelete the version at a specific timestampA delete can apply to a complete row, a complete column family, or to just one column. It is only inthe last case that you can delete explicit versions. For the deletion of a row or all the columnswithin a family, it always works by deleting all cells older than a certain version.Deletes work by creating tombstone markers. For example, let’s suppose we want to delete a row.For this you can specify a version, or else by default the currentTimeMillis is used. What thismeans is “delete all cells where the version is less than or equal to this version”. HBase nevermodifies data in place, so for example a delete will not immediately delete (or mark as deleted) theentries in the storage file that correspond to the delete condition. Rather, a so-called tombstone iswritten, which will mask the deleted values[17]. If the version you specified when deleting a row islarger than the version of any value in the row, then you can consider the complete row to bedeleted.Reference: Apache HBase, Deletehttp://archive.cloudera.com/cdh4/cdh/4/hbase/book.html#delete(scroll below and see 5.8.1.5.Delete topic, read the last paragraph)

--------------------------------------------------------------------------------------------------
193 : C.Decrease the block size

Explanation:  Larger block size is preferred if files are primarily for sequential access. Smaller blocks are good for random access, but require more memory to hold the block index, andmay be slower to createReference: Could I improve HBase performance by reducing the hdfs block size?

--------------------------------------------------------------------------------------------------
194 : D.r35, r30, r25, r20, r15, r10, r3, r2, r1

Explanation: If you can have the table receiving rows always in decreasing order of the row keys,you then have easy access to the first and last rows. This is possible because HBase tables arealways sorted by row key.

--------------------------------------------------------------------------------------------------
195 : D.Descending first by region and second by row key

--------------------------------------------------------------------------------------------------
196 : C.Whichever HMaster creates the znode first

Explanation: * The Hbase master server creates the zookeeper znode /hbase . This is then usedfor hbase daemons to coordinate. Even the name of the active Hbase master is stored here. If thehbase master dies, the backup hbase master overwrites the contents of the znode so clients andregion servers know about the new master. Apart from this, region info is maintained in zookeeperznodes as well.* Multi-master feature introduced in 0.20.0 does not add cooperating Masters; there is still just oneworking Master while the other backups wait. For example, if you start 200 Masters only 1 will beactive while the others wait for it to die. The switch usually takes zookeeper.session.timeout plus acouple of seconds to occur.

--------------------------------------------------------------------------------------------------
197 : A.All HBase activity is written to the WAL, which is stored in HDFS

--------------------------------------------------------------------------------------------------
198 : C.create ‘WebLogs’, ‘Errors:IP’, ‘Errors:URL’

--------------------------------------------------------------------------------------------------
199 : A.Two

--------------------------------------------------------------------------------------------------
200 : D.Increasing block size means fewer block indexes that need to be read from disk, thereby increasing scan performance.

Explanation: Change HFile block size to something bigger to improve scan (at cost of randomread).Reference:Testing HBase Scan performance

--------------------------------------------------------------------------------------------------
201 : B.The last write to cell

Explanation: If multiple writes to a cell have the same version, are all versions maintained or justthe last?Currently, only the last written is fetchable.Reference:The Apache HBaseReference Guide,5.8. Versions

--------------------------------------------------------------------------------------------------
202 : A.The client looks up the location of ROOT, in which it looks up the location of META, in which it looks up the location of the correct Users region.

--------------------------------------------------------------------------------------------------
203 : B.The scan class supports ranges via the stop and start rows.

Explanation: Rather than specifying a single row, an optional startRow and stopRow may bedefined. If rows are not specified, the Scanner will iterate over all rows.Reference: org.apache.hadoop.hbase.client,Class Scan

--------------------------------------------------------------------------------------------------
204 : D.HColumnDescriptor

Explanation: maxVersions – Maximum number of versions to keepNote:*public HColumnDescriptor(byte[] familyName,int maxVersions,String compression,boolean inMemory,boolean blockCacheEnabled,int timeToLive,String bloomFilter)*An HColumnDescriptor contains information about a column family such as the number ofversions, compression settings, etc. It is used as input when creating a table or adding a column.Once set, the parameters that specify a column cannot be changed without deleting the columnand recreating it. If there is data stored in the column, it will be deleted when the column isdeleted.Reference:org.apache.hadoop.hbase,Class HColumnDescriptor

--------------------------------------------------------------------------------------------------
205 : C.Create a single table with two column families

Explanation: Physically, all column family members are stored together on the filesystem.Because tunings and storage specifications are done at the column family level, it is advised thatall column family members have the same general access pattern and size characteristics.Reference:The Apache HBaseReference Guide,Column Family

--------------------------------------------------------------------------------------------------
206 : A.One of the daughter regions

--------------------------------------------------------------------------------------------------
207 : C.ZooKeeper -&gt; ROOT -&gt; .META. -&gt; RegionServer -&gt; Region

--------------------------------------------------------------------------------------------------
208 : D.hbase.hregion.max.filesize

Explanation: Consider going to larger regions to cut down on the total number of regions on yourcluster. Generally less Regions to manage makes for a smoother running cluster (You can alwayslatermanually split the big Regions should one prove hot and you want to spread the request loadover the cluster). A lower number of regions is preferred, generally in the range of 20 to lowhundreds per RegionServer. Adjust the regionsize as appropriate to achieve this number.For the 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of 256Mb. For0.92.x codebase, due to the HFile v2 change much larger regionsizes can be supported (e.g.,20Gb).You may need to experiment with this setting based on your hardware configuration andapplication needs.Adjust hbase.hregion.max.filesize in your hbase-site.xml. RegionSize can also be set on a pertable basis via HTableDescriptor.Reference:The Apache HBaseReference Guide,Bigger Regionshttp://hbase.apache.org/book/important_configurations.html(2.5.2.6. Bigger regions, see the codein the last sentence)

--------------------------------------------------------------------------------------------------
209 : A.scan throughput increases and random-access latency decreases

Explanation:  Larger block size is preferred if files are primarily for sequential access. Smaller blocks are good for random access, but require more memory to hold the block index, andmay be slower to createReference: Could I improve HBase performance by reducing the hdfs block size?

--------------------------------------------------------------------------------------------------
210 : B.Create two tables each with a single column family

Explanation:

--------------------------------------------------------------------------------------------------
211 : A.Have no effect on performance.

Explanation: You can add multiple HBase master nodes; however, only one HBase master nodeis active at a time. The active HBase master node changes only when the current active HBasemaster node is shut down or fails.

--------------------------------------------------------------------------------------------------
212 : A.From the moment the RegionServer wrote to the WAL (write-ahead log)

--------------------------------------------------------------------------------------------------
213 : A.Four

Explanation:

--------------------------------------------------------------------------------------------------
214 : C.TTL = 432000, MIN_VERSIONS = 1

--------------------------------------------------------------------------------------------------
215 : C.Decrease your number of RegionServers to 5

--------------------------------------------------------------------------------------------------
216 : A.The reverse domain name (e.g., com.example.beta)

Explanation: Consider a table whose keys are domain names. It makes the most sense to listthem in reverse notation (so “com.jimbojw.www” rather than “www.jimbojw.com”) so that rowsabout a subdomain will be near the parent domain row.Continuing the domain example, the row for the domain “mail.jimbojw.com” would be right next tothe row for “www.jimbojw.com” rather than say “mail.xyz.com” which would happen if the keyswere regular domain notation.Reference: Understanding HBase and BigTable

--------------------------------------------------------------------------------------------------
217 : B.admin.createTable(t);

Explanation: See line 10 below.Creating a table in HBase01public void createTable (String tablename, String familyname) throws IOException {0203Configuration conf = HBaseConfiguration.create();04HBaseAdmin admin = new HBaseAdmin(conf);0506HTableDescriptor tabledescriptor = new HTableDescriptor(Bytes.toBytes(tablename));0708tabledescriptor.addFamily(new HColumnDescriptor (familyname));0910admin.createTable(tabledescriptor);1112}Reference:HBASE ADMINISTRATION USING THE JAVA API, USING CODE EXAMPLEShttp://linuxjunkies.wordpress.com/2011/12/03/hbase-administration-using-the-java-api-using-codeexamples/(creating a table in Hbase, see the code)

--------------------------------------------------------------------------------------------------
218 : C.4, 6, 3, 1, 5, 2

Explanation:

--------------------------------------------------------------------------------------------------
219 : A.The location of a Region

--------------------------------------------------------------------------------------------------
220 : C.Data is stored sorted on row keys

Explanation: HBase tables are always sorted by row key.

--------------------------------------------------------------------------------------------------
221 : A.&lt;hashCode (centralServerGeneratedSequenceID) &gt;&lt;timestamp&gt;

--------------------------------------------------------------------------------------------------
222 : B.Confirmation that 104 is contained in the set

--------------------------------------------------------------------------------------------------
223 : C.No. If you disable block caching, HBase must read each block index from disk for each scan, thereby decreasing scan performance.

Explanation: Disabling BlockcacheDo not turn off block cache (You’d do it by setting hbase.block.cache.size to zero). Currently wedo not do well if you do this because the regionserver will spend all its time loading hfileindicesover and over again. If your working set it such that block cache does you no good, at leastsize the block cache such that hfile indices will stay up in the cache (you can get a rough idea onthe size you need by surveying regionserver UIs; you’ll see index block size accounted near thetop of the webpage).Reference: Apache HBase (TM) Configuration

--------------------------------------------------------------------------------------------------
224 : C.MemStore

Explanation: HBase data updates are stored in a place in memory called memstore for fast write.In the event of a region server failure, the contents of the memstore are lost because they havenot been saved to disk yet.Reference:HBase data updates are stored in a place in memory called memstore for fast write. Inthe event of a region server failure, the contents of the memstore are lost because they have notbeen saved to disk yet.http://www.cloudera.com/blog/2012/07/hbase-log-splitting/(Log splitting, first paragraph)

--------------------------------------------------------------------------------------------------
225 : D.HBase has not run a major compaction

Explanation: The actual deletion of the excess versions is done upon major compaction.Note: HBase basically never overwrites data but only appends. The data files are rewritten once inwhile by a compaction process. A data file is basically a list of key-value pairs, where the key isthe composite {row key, column key, time}. Each time you do a put that writes a new value for anexisting cell, a new key-value pair gets appended to the store. Even if you would specify anexisting timestamp. Doing lots of updates to the same row in a short time span will lead to a lot ofkey-value pairs being present in the store. Depending on the garbage collection settings (seenext), these will be removed during the next compaction.

--------------------------------------------------------------------------------------------------
226 : A.Random writes

Explanation: HBase adds random read/write access to HDFS.Note:Hadoop is scalable, but…* MapReduce is slow and difcult* Does not support random writes* Poor support for random readsReference:http://borthakur.com/ftp/SIGMODRealtimeHadoopPresentation.pdf(11th slide)

--------------------------------------------------------------------------------------------------
227 : B.Row Key, colFam_A:a, colFam__A:b, colFam_B:2, colFam_B:10

Explanation: All is sorted in hbase, first by row (row key), then by column familyfollowed by column qualifier, type and finally timestamp (ts is sortedin reverse .. so you see newest records first).

--------------------------------------------------------------------------------------------------
228 : C.Two

--------------------------------------------------------------------------------------------------
229 : B.Exactly one

Explanation:

--------------------------------------------------------------------------------------------------
230 : B.To prevent overfitting

--------------------------------------------------------------------------------------------------
231 : A.^A (Control-A)

Explanation:http://blog.spryinc.com/2013/10/four-useful-tricks-for-working-with-hive.html(change thedelimiter when exporting hive table)

--------------------------------------------------------------------------------------------------
232 : B.Linear Regression

--------------------------------------------------------------------------------------------------
233 : A.Convex

--------------------------------------------------------------------------------------------------
234 : A.A

--------------------------------------------------------------------------------------------------
235 : C.C

--------------------------------------------------------------------------------------------------
236 : B.B

--------------------------------------------------------------------------------------------------
237 : A.When the volume of input data is so large and diverse that a 2nd-order optimization technique can be fit to a sample of the data

237 : B.When the model’s estimates must be updated in real-time in order to account for newobservations.

--------------------------------------------------------------------------------------------------
238 : C.sqoop lists the available tables from the database

Explanation:https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-15/gettingsqoop

--------------------------------------------------------------------------------------------------
239 : C.Non-normal distribution of the input data

--------------------------------------------------------------------------------------------------
240 : D.X4

--------------------------------------------------------------------------------------------------
241 : E.X2 and X9

--------------------------------------------------------------------------------------------------
242 : C.Write a MapReduce job with the web servers for mappers and the Hadoop cluster nodes for reducers

--------------------------------------------------------------------------------------------------
243 : A.Method A

--------------------------------------------------------------------------------------------------
244 : B.Method B

--------------------------------------------------------------------------------------------------
245 : C.Method C

--------------------------------------------------------------------------------------------------
246 : B.Method B

--------------------------------------------------------------------------------------------------
247 : A.$125,000

--------------------------------------------------------------------------------------------------
248 : B.1/2

--------------------------------------------------------------------------------------------------
249 : C.1/8

--------------------------------------------------------------------------------------------------
250 : D.The number of bugs that had been found in each sub-component of the project

--------------------------------------------------------------------------------------------------
251 : B.Distributing the updates of the cluster centroids

--------------------------------------------------------------------------------------------------
252 : B.def mapper1 (line): key1, key2, message = line.split (‘ , ’) emit ( (key1, key2) , 1) emit ( (key1, key2) , 1) def reducer1(key, values): emit (key, sum(values)) def mapper2(key, value): key1, key2 = key / / unpack both friends name into separate keys emit (key1, value) def reducer2(key, values): emit (key, mean (values) )

--------------------------------------------------------------------------------------------------
253 : B.Hadoop fs –get westUsers WestUsers.txt

--------------------------------------------------------------------------------------------------
254 : A.X1/2

--------------------------------------------------------------------------------------------------
255 : B.XML

255 : F.JSON

--------------------------------------------------------------------------------------------------
256 : B.Ingest with Apache Flume

256 : D.Ingest using Sqoop

Explanation:https://thinkbiganalytics.com/leading_big_data_technologies/ingestion-and-streamingwith-storm-kafka-flume/

--------------------------------------------------------------------------------------------------
257 : A.Database of employees that Includes only the employee ID, start date, and department

257 : D.Database of user sessions that includes only session ID, corresponding user ID, and the corresponding IP address

257 : F.Database of items that includes only the item name, item ID, and warehouse location

--------------------------------------------------------------------------------------------------
258 : A.It does not require you to make strong assumptions about the data because it is a nonparametric

--------------------------------------------------------------------------------------------------
259 : B.It is the mean value of recommendations of the K-equal partitions in the input data

259 : D.It is appropriate for numeric data

--------------------------------------------------------------------------------------------------
260 : B.k-NN compotation does not coverage in high dimensions

--------------------------------------------------------------------------------------------------
261 : D.Flume provides a query languages for Hadoop similar to SQL

--------------------------------------------------------------------------------------------------
262 : B.Find . –name ‘name * .CSV’ | cat | awk ‘BEGIN {FS = “,” OFS = “\t”} {print $1, $2, $3}’ &gt; all.tsv

--------------------------------------------------------------------------------------------------
263 : D.Speeds up the model fitting process

263 : E.Develops an understanding of the importance of different features

263 : F.Improves the predictive performance of the model

--------------------------------------------------------------------------------------------------
264 : A.Once after every pass through the data set

264 : C.For each observation with a probability that you choose ahead of time

--------------------------------------------------------------------------------------------------
265 : A.XML files that you need to convert to JSON

265 : B.Text files that require parsing into useful fields

--------------------------------------------------------------------------------------------------
266 : C.User-based collaborative filtering

Explanation:http://www.cs.cmu.edu/~srosenth/papers/Rosenthal_RecSys09.pdf

--------------------------------------------------------------------------------------------------
267 : E.1000

--------------------------------------------------------------------------------------------------
268 : C.Write a script that receives records on stdin, corrects them, and then writes them to stdout. Then, invoke this script in a map-only Hadoop Streaming Job

--------------------------------------------------------------------------------------------------
269 : A.A bar chart of engineers vs. number of bugs fixed

--------------------------------------------------------------------------------------------------
270 : D.Perform matched sampling across other provided variables

--------------------------------------------------------------------------------------------------
271 : D.Filtering

--------------------------------------------------------------------------------------------------
272 : C.A scatter plot of two largest principal components

--------------------------------------------------------------------------------------------------
273 : C.Markov chain Monte Carlo

273 : D.Hidden Markov

273 : F.Mutual Information

--------------------------------------------------------------------------------------------------
274 : B.~ 400 MB

--------------------------------------------------------------------------------------------------
275 : C.The standard deviation of the data set

--------------------------------------------------------------------------------------------------
276 : A.M = U S V

--------------------------------------------------------------------------------------------------
277 : A.Blue

--------------------------------------------------------------------------------------------------
278 : B.The derivative of convex function is always defined

--------------------------------------------------------------------------------------------------
279 : A.Include a small number “noise” features that are not through to be correlated with the dependent variable.

279 : E.Preprocess the data to exclude a typical observation from the model input

--------------------------------------------------------------------------------------------------
280 : C. Hadoop fs –getemerge westUsers westUsers.txt

--------------------------------------------------------------------------------------------------
281 : C. Fsimage_N (where N reflects transactions up to transaction ID N)

--------------------------------------------------------------------------------------------------
282 : B. Connect to http://mynamenode:50070/dfshealth.jsp and locate the DFS remaining value

282 : C. Run hdfs dfs / and subtract NDFS Used from configured Capacity

--------------------------------------------------------------------------------------------------
283 : D. Developers specify reduce tasks in the exact same way for both MapReduce version 1 

--------------------------------------------------------------------------------------------------
284 : C. The NameNode will update the dfs.hosts property to include machines running the DataNode daemon on the next NameNode reboot or with the command dfsadmin –refreshNodes

--------------------------------------------------------------------------------------------------
285 : C. mapreduce.map.java.opts=-Xms3072m

--------------------------------------------------------------------------------------------------
286 : D. Because there is a more than a single job on the cluster, the FIFO Scheduler will enforce a limit on the percentage of resources allocated to a particular job at any given time

286 : F. The FIFO Scheduler will give, on average, and equal share of the cluster resources over the job lifecycle

--------------------------------------------------------------------------------------------------
287 : A. The user is not authorized to run the job on the cluster

--------------------------------------------------------------------------------------------------
288 : C. Ingest with Pig’s LOAD command

288 : E. Ingest with sqoop import

--------------------------------------------------------------------------------------------------
289 : A. Even for small clusters on a single rack, configuring rack awareness will improve performance

289 : D. Rack location is considered in the HDFS block placement policy

--------------------------------------------------------------------------------------------------
290 : C.Hive

--------------------------------------------------------------------------------------------------
291 : D.Pig provides the additional capability of allowing you to control the flow of multiple MapReduce jobs.

Explanation:In addition to providing many relational and data flow operators Pig Latin providesways for you to control how your jobs execute on MapReduce. It allows you to set values thatcontrol your environment and to control details of MapReduce such as how your data ispartitioned.Reference:http://ofps.oreilly.com/titles/9781449302641/advanced_pig_latin.html(topic: controllingexecution)

--------------------------------------------------------------------------------------------------
292 : E.Sqoop

--------------------------------------------------------------------------------------------------
293 : D.Sequences of MapReduce and Pig. These sequences can be combined with other actions including forks, decision points, and path joins.

Explanation:Reference:http://incubator.apache.org/oozie/docs/3.1.3/docs/WorkflowFunctionalSpec.html(workflow definition, first sentence)

--------------------------------------------------------------------------------------------------
294 : E.HBase

--------------------------------------------------------------------------------------------------
295 : A.As key-value pairs in the jobconf object.

Explanation:In Hadoop, it is sometimes difficult to pass arguments to mappers and reducers. Ifthe number of arguments is huge (e.g., big arrays), DistributedCache might be a good choice.However, here, we’re discussing small arguments, usually a hand of configuration parameters.In fact, the way to configure these parameters is simple. When you initialize“JobConf”object tolaunch a mapreduce job, you can set the parameter by using“set”method like:1JobConf job = (JobConf)getConf();2job.set(“NumberOfDocuments”, args[0]);Here,“NumberOfDocuments”is the name of parameter and its value is read from“args[0]“, acommand line argument.Reference:Passing Parameters and Arguments to Mapper and Reducer in Hadoop

--------------------------------------------------------------------------------------------------
296 : C.hadoop jar MyJar.jar MyDriverClass inputdir outputdir

--------------------------------------------------------------------------------------------------
297 : D.Hadoop Streaming

Explanation:Hadoop streaming is a utility that comes with the Hadoop distribution. The utilityallows you to create and run Map/Reduce jobs with any executable or script as the mapper and/orthe reducer.Reference:http://hadoop.apache.org/common/docs/r0.20.1/streaming.html(HadoopStreaming,second sentence)

--------------------------------------------------------------------------------------------------
298 : C.A failed task attempt is a task attempt that completed, but with an unexpected status value. A killed task attempt is a duplicate copy of a task attempt that was started as part of speculative execution.

Explanation:Note:*Hadoop uses “speculative execution.” The same task may be started on multiple boxes. The firstone to finish wins, and the other copies are killed.Failed tasks are tasks that error out.*There are a few reasons Hadoop can kill tasks by his own decisions:a) Task does not report progress during timeout (default is 10 minutes)b) FairScheduler or CapacityScheduler needs the slot for some other pool (FairScheduler) orqueue (CapacityScheduler).c) Speculative execution causes results of task not to be needed since it has completed on otherplace.Reference:Difference failed tasks vs killed tasks

--------------------------------------------------------------------------------------------------
299 : A.Lightweight devices for bookkeeping within MapReduce programs.

Explanation:Countersare a useful channel for gathering statistics about the job; for qualitycontrol, or for application-level statistics. They are also useful for problem diagnosis. Hadoopmaintains somebuilt-in counters for every job, which reports various metrics for your job.Hadoop MapReduce also allows the user to define a set of user-defined counters that can beincremented (or decremented by specifying a negative value as the parameter), by the driver,mapper or the reducer.Reference:Iterative MapReduce and Counters,Introduction to Iterative MapReduce and Countershttp://hadooptutorial.wikispaces.com/Iterative+MapReduce+and+Counters(counters, secondparagraph)

--------------------------------------------------------------------------------------------------
300 : D.On the local disk of the slave node running the task.

Explanation: Apache Hadoop Log Files: Where to find them in CDH, and what info they contain

--------------------------------------------------------------------------------------------------
301 : D.The NameNode requires more memory but less disk capacity.

Explanation:Note:* The NameNode is the centerpiece of an HDFS file system. It keeps the directory tree of all filesin the file system, and tracks where across the cluster the file data is kept. It does not store thedata of these files itself. There is only One NameNode process run on any hadoop cluster.NameNode runs on its own JVM process. In a typical production cluster its run on a separatemachine. The NameNode is a Single Point of Failure for the HDFS Cluster. When the NameNodegoes down, the file system goes offline. Client applications talk to the NameNode whenever theywish to locate a file, or when they want to add/copy/move/delete a file. The NameNode respondsthe successful requests by returning a list of relevant DataNode servers where the data lives.* A DataNode stores data in the Hadoop File System HDFS. There is only One DataNode processrun on any hadoop slave node. DataNode runs on its own JVM process. On startup, a DataNodeconnects to the NameNode. DataNode instances can talk to each other, this is mostly duringreplicating data. 24 Interview Questions &amp; Answers for Hadoop MapReduce developers

--------------------------------------------------------------------------------------------------
302 : B.Connect to http://mynamemode:50070/ and locate the DFS Remaining value.

302 : C.Run hadoop dfsadmin –report and locate the DFS Remaining value.

--------------------------------------------------------------------------------------------------
303 : D.50 slave nodes

Explanation:Total number available space required: 52 (weeks) * 1 (disk space per week) * 3(default replication factor) = 156 TBMinimum number of slave nodes required: 156 /4 = 39

--------------------------------------------------------------------------------------------------
304 : B.Fair Scheduler 

--------------------------------------------------------------------------------------------------
305 : C.The slave node on which the Reducer runs gets the first copy of every block written. Other block replicas will be placed on other nodes.

--------------------------------------------------------------------------------------------------
306 : D.When job B gets submitted, it will get assigned tasks, while job A continues to run with fewer tasks.

Explanation:Fair scheduling is a method of assigning resources to jobs such that all jobs get, onaverage, an equal share of resources over time. When there is a single job running, that job usesthe entire cluster. When other jobs are submitted, tasks slots that free up are assigned to the newjobs, so that each job gets roughly the same amount of CPU time. Unlike the default Hadoopscheduler, which forms a queue of jobs, this lets short jobs finish in reasonable time while notstarving long jobs. It is also a reasonable way to share a cluster between a number of users.Finally, fair sharing can also work with job priorities – the priorities are used as weights todetermine the fraction of total compute time that each job should get. Hadoop, Fair Scheduler Guide

--------------------------------------------------------------------------------------------------
307 : A.Isolating a failed NameNode from write access to the fsimage and edits files so that is cannot resume write operations if it recovers.

--------------------------------------------------------------------------------------------------
308 : A.When the typical workloads generates a large amount of intermediate data, on the order of the input data itself.

--------------------------------------------------------------------------------------------------
309 : B.It is set by the developer.

Explanation:Number of ReducesThe right number of reduces seems to be 0.95 or 1.75 * (nodes *mapred.tasktracker.tasks.maximum). At 0.95 all of the reduces can launch immediately and starttransfering map outputs as the maps finish. At 1.75 the faster nodes will finish their first round ofreduces and launch a second round of reduces doing a much better job of load balancing.Currently the number of reduces is limited to roughly 1000 by the buffer size for the output files(io.buffer.size * 2 * numReduces &lt;&lt; heapSize). This will be fixed at some point, but until it is itprovides a pretty firm upper bound.The number of reduces also controls the number of output files in the output directory, but usuallythat is not important because the next map/reduce step will split them into even smaller splits forthe maps.The number of reduce tasks can also be increased in the same way as the map tasks, viaJobConf’s conf.setNumReduceTasks(int num). org.apache.hadoop.mapred Class JobConf

--------------------------------------------------------------------------------------------------
310 : D.A SequenceFile contains a binary encoding of an arbitrary number key-value pairs. Each key must be the same type. Each value must be the same type. 

Explanation:SequenceFile is a flat file consisting of binary key/value pairs.There are 3 different SequenceFile formats:Uncompressed key/value records.Record compressed key/value records – only ‘values’ are compressed here.Block compressed key/value records – both keys and values are collected in ‘blocks’ separatelyand compressed. The size of the ‘block’ is configurable.Reference: http://wiki.apache.org/hadoop/SequenceFile

--------------------------------------------------------------------------------------------------
311 : D.The file can be accessed if at least one of the data nodes storing the file is available.

Explanation:HDFS keeps three copies of a block on three different datanodes to protect againsttrue data corruption. HDFS also tries to distribute these three replicas on more than one rack toprotect against data availability issues. The fact that HDFS actively monitors any faileddatanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed)implies that three copies of data on three different nodes is sufficient to avoid corrupted files.Note: HDFS is designed to reliably store very large files across machines in a large cluster. It storeseach file as a sequence of blocks; all blocks in a file except the last block are the same size. Theblocks of a file are replicated for fault tolerance. The block size and replication factor areconfigurable per file. An application can specify the number of replicas of a file. The replicationfactor can be specified at file creation time and can be changed later. Files in HDFS are write-onceand have strictly one writer at any time. The NameNode makes all decisions regarding replicationof blocks. HDFS uses rack-aware replica placement policy. In default configuration there are total3 copies of a datablock on HDFS, 2 copies are stored on datanodes on same rack and 3rd copyon a different rack.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , How theHDFS Blocks are replicated?

--------------------------------------------------------------------------------------------------
312 : D.Write a custom FileInputFormat and override the method isSplitable to always return false.

Explanation:FileInputFormat is the base class for all file-based InputFormats. This provides ageneric implementation of getSplits(JobContext). Subclasses of FileInputFormat can also overridethe isSplitable(JobContext, Path) method to ensure input-files are not split-up and are processedas a whole by Mappers.Reference: org.apache.hadoop.mapreduce.lib.input, Class FileInputFormat&lt;K,V&gt;

--------------------------------------------------------------------------------------------------
313 : C.The TaskTracker spawns a new Mapper to process each key-value pair.

--------------------------------------------------------------------------------------------------
314 : D.Reducers start copying intermediate key-value pairs from each Mapper as soon as it has completed. The reduce method is called as soon as the intermediate key-value pairs start to arrive.

--------------------------------------------------------------------------------------------------
315 : B.3

--------------------------------------------------------------------------------------------------
316 : B.Place the data file in the DistributedCache and read the data into memory in the map method of the mapper.

--------------------------------------------------------------------------------------------------
317 : B.The values are arbitrarily ordered, and the ordering may vary from run to run of the same MapReduce job.

Explanation:Note: * Input to the Reducer is the sorted output of the mappers. * The framework calls the application’s Reduce function once for each unique key in the sortedorder.* Example:For the given sample input the first map emits:&lt; Hello, 1&gt; &lt; World, 1&gt; &lt; Bye, 1&gt; &lt; World, 1&gt; The second map emits:&lt; Hello, 1&gt; &lt; Hadoop, 1&gt; &lt; Goodbye, 1&gt; &lt; Hadoop, 1&gt;

--------------------------------------------------------------------------------------------------
318 : B.Disk I/O and network I/O

--------------------------------------------------------------------------------------------------
319 : D.JobTracker

Explanation:JobTracker is the daemon service for submitting and tracking MapReduce jobs inHadoop. There is only One Job Tracker process run on any hadoop cluster. Job Tracker runs onits own JVM process. In a typical production cluster its run on a separate machine. Each slavenode is configured with job tracker node location. The JobTracker is single point of failure for theHadoop MapReduce service. If it goes down, all running jobs are halted. JobTracker in Hadoopperforms following actions(from Hadoop Wiki:)Client applications submit jobs to the Job tracker.The JobTracker talks to the NameNode to determine the location of the dataThe JobTracker locates TaskTracker nodes with available slots at or near the dataThe JobTracker submits the work to the chosen TaskTracker nodes.The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, theyare deemed to have failed and the work is scheduled on a different TaskTracker.A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to dothen: it may resubmit the job elsewhere, it may mark that specific record as something to avoid,and it may may even blacklist the TaskTracker as unreliable.When the work is completed, the JobTracker updates its status.Client applications can poll the JobTracker for information.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, What is aJobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?

--------------------------------------------------------------------------------------------------
320 : D.When the typical workload generates a large amount of output data, significantly larger than the amount of intermediate data.

--------------------------------------------------------------------------------------------------
321 : B.To ensure that there is capacity in HDTS for additional data.

321 : C.To help HDFS deliver consistent performance under heavy loads.

321 : E.To ensure that there is consistent disk utilization across the DataNodes.

Explanation:http://hadoop.apache.org/docs/hdfs/r0.21.0/api/org/apache/hadoop/hdfs/server/balancer/Balance.html

--------------------------------------------------------------------------------------------------
322 : B.A process that performs a checkpoint operation on the files produced by the NameNode.

Explanation:http://wiki.apache.org/hadoop/FAQ#What_is_the_purpose_of_the_secondary_namenode.3F(3.2)

--------------------------------------------------------------------------------------------------
323 : B.After identifying the outage, the NameNode will naturally re-replicate the data and there will be no data loss. The administrator can re-add the DataNode at any time. The client can disregard warnings concerned with this event. Data will be under-replicated but will become properly replicated over time.

--------------------------------------------------------------------------------------------------
324 : A.Storing multiple replicas of data blocks on different DataNodes.

Explanation:http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-thenetwork/(writing files to HDFS)

--------------------------------------------------------------------------------------------------
325 : B.Pools get a dynamically-allocated share of the available task slots (subject to additional constraints).

325 : D.Pools are assigned priorities. Pools with higher priorities are executed before pools with lower priorities.

--------------------------------------------------------------------------------------------------
326 : B.Hadoop fs –getemerge westUsers westUsers.txt

--------------------------------------------------------------------------------------------------
327 : C.Fsimage_N (where N reflects transactions up to transaction ID N)

Explanation:http://mikepluta.com/tag/namenode/

--------------------------------------------------------------------------------------------------
328 : C.Run hdfs dfs / and subtract NDFS Used from configured Capacity

328 : D.Connect to http://mynamenode:50070/dfshealth.jsp and locate the DFS remaining value

--------------------------------------------------------------------------------------------------
329 : D.Developers specify reduce tasks in the exact same way for both MapReduce version 1 (MRv1) and MapReduce version 2 (MRv2) on YARN. Thus, executing –D mapreduce.job.reduces-2 will specify reduce tasks.

--------------------------------------------------------------------------------------------------
330 : A.The NameNode will update the dfs.hosts property to include machines running the DataNode daemon on the next NameNode reboot or with the command dfsadmin –refreshNodes

--------------------------------------------------------------------------------------------------
331 : C.mapreduce.map.java.opts=-Xms3072m

Explanation:http://hortonworks.com/blog/how-to-plan-and-configure-yarn-in-hdp-2-0/

--------------------------------------------------------------------------------------------------
332 : A.Because there is a more than a single job on the cluster, the FIFO Scheduler will enforce a limit on the percentage of resources allocated to a particular job at any given time

332 : E.The FIFO Scheduler will give, on average, and equal share of the cluster resources over the job lifecycle

--------------------------------------------------------------------------------------------------
333 : A.The user is not authorized to run the job on the cluster

--------------------------------------------------------------------------------------------------
334 : C.Ingest with sqoop import

334 : D.Ingest with Pig’s LOAD command

--------------------------------------------------------------------------------------------------
335 : C.Rack location is considered in the HDFS block placement policy

335 : E.Even for small clusters on a single rack, configuring rack awareness will improve performance

--------------------------------------------------------------------------------------------------
336 : A.Choose words for your sample that are most correlated with the Spam label

--------------------------------------------------------------------------------------------------
337 : A.1, 3, 8, 34, 89

--------------------------------------------------------------------------------------------------
338 : D.You can calculate it quickly using a relational database like MySQL, even when we have a large sample

--------------------------------------------------------------------------------------------------
339 : C.They use pivots to assign each observations to the reducer that calculate each percentile

--------------------------------------------------------------------------------------------------
340 : C.The learning rate should be the value that optimizes the value of the objective function over the first N samples in the dataset

--------------------------------------------------------------------------------------------------
341 : A.Support vector machine

341 : B.Naïve Bayes

Explanation:http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2656082/

--------------------------------------------------------------------------------------------------
342 : B.Markov Chain Monte Carlo

--------------------------------------------------------------------------------------------------
343 : A.Consumers do not have stable ratings for the same product over time

--------------------------------------------------------------------------------------------------
344 : C.It assumes Independence between all features

Explanation:http://www.mathworks.com/help/stats/naive-bayes-classification.html

--------------------------------------------------------------------------------------------------
345 : C.Tanimoto coefficient

345 : D.Pearson correlation

345 : E.Precision

Explanation:https://lirias.kuleuven.be/bitstream/123456789/289803/3/datasets-cameraready.pdf

--------------------------------------------------------------------------------------------------
346 : D.put ‘Users’, ‘Meta:UserID’, ‘jsmith70’, ‘Meta:Email, ‘jane@example.com’

Explanation: Need to include the column family name: Meta for both columns using the : syntax.Note:Columns in Apache HBase are grouped into column families. All column members of a columnfamily have the same prefix. For example, the columns courses:history and courses:math are bothmembers of the courses column family. The colon character (:) delimits the column family from thecolumn qualifier . The column family prefix must be composed of printable characters. Thequalifying tail, the column family qualifier, can be made of any arbitrary bytes. Column familiesmust be declared up front at schema definition time whereas columns do not need to be defined atschema time but can be conjured on the fly while the table is up an running.

--------------------------------------------------------------------------------------------------
347 : A.Modify the client application to write to both the old table and a new table while migrating the old data separately

Explanation: Rowkeys cannot be changed. The only way they can be “changed” in a table is ifthe row is deleted and then re-inserted. This is a fairly common question on the HBase dist-list soit pays to get the rowkeys right the first time (and/or before you’ve inserted a lot of data).Reference:Rowkey Design

--------------------------------------------------------------------------------------------------
348 : D.Data for your client application in the MemStores for region1 is lost

Explanation: What role does ‘setWriteToWAL(false)’ play?HBase uses a write ahead log, if you don’t write to it you will loseall the data that’s only in the memstores when a region server fails.This setting is useful for importing a lot of data.

--------------------------------------------------------------------------------------------------
349 : B.The Put class allows setting a cell specific timestamp

Explanation: Doing a put always creates a new version of a cell, at a certain timestamp. Bydefault the system uses the server’s currentTimeMillis, but you can specify the version (= the longinteger) yourself, on a per-column level. This means you could assign a time in the past or thefuture, or use the long value for non-time purposes.To overwrite an existing value, do a put at exactly the same row, column, and version as that ofthe cell you would overshadow.Reference:Versions, Put

--------------------------------------------------------------------------------------------------
350 : C.The client contacts the NameNode for the block location(s). The NameNode then queries the DataNodes for block locations. The DataNodes respond to the NameNode, and the NameNode redirects the client to the DataNode that holds the requested data block(s). The client then reads the data directly off the DataNode.

Explanation:The Client communication to HDFS happens using Hadoop HDFS API. Clientapplications talk to the NameNode whenever they wish to locate a file, or when they want toadd/copy/move/delete a file on HDFS. The NameNode responds the successful requests byreturning a list of relevant DataNode servers where the data lives. Client applications can talkdirectly to a DataNode, once the NameNode has provided the location of the data.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers, How the Clientcommunicates with HDFS?

--------------------------------------------------------------------------------------------------
351 : E.The file is divided into fixed-size blocks, which are stored on multiple datanodes. Each block is replicated three times by default. HDFS guarantees that different blocks from the same file are never on the same datanode.

--------------------------------------------------------------------------------------------------
352 : A.about 3 TB

Explanation:In default configuration there are total 3 copies of a datablock on HDFS, 2 copiesare stored on datanodes on same rack and 3rd copy on a different rack.Note: HDFS is designed to reliably store very large files across machines in a large cluster. Itstores each file as a sequence of blocks; all blocks in a file except the last block are the samesize. The blocks of a file are replicated for fault tolerance. The block size and replication factor areconfigurable per file. An application can specify the number of replicas of a file. The replicationfactor can be specified at file creation time and can be changed later. Files in HDFS are write-onceand have strictly one writer at any time. The NameNode makes all decisions regarding replicationof blocks. HDFS uses rack-aware replica placement policy.Reference: 24 Interview Questions &amp; Answers for Hadoop MapReduce developers , How theHDFS Blocks are replicated?

--------------------------------------------------------------------------------------------------
353 : A. The Hadoop configuration files on the client do not point to the cluster

--------------------------------------------------------------------------------------------------
354 : B. Fsimage_N (Where N reflects all transactions up to transaction ID N)

--------------------------------------------------------------------------------------------------
355 : E. JobTracker

--------------------------------------------------------------------------------------------------
356 : D. Configure yarn.nodemanager.resource.memory-mb and 

--------------------------------------------------------------------------------------------------
357 : A.The Hadoop configuration files on the client do not point to the cluster

--------------------------------------------------------------------------------------------------
358 : C.Fsimage_N (Where N reflects all transactions up to transaction ID N)

Explanation:http://mikepluta.com/tag/namenode/

--------------------------------------------------------------------------------------------------
359 : E.JobTracker

--------------------------------------------------------------------------------------------------
360 : C.Configure yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpuvcores to match the capacity you require under YARN for each NodeManager

--------------------------------------------------------------------------------------------------
361 : A.HBase

Explanation:Use Apache HBase when you need random, realtime read/write access to your BigData.Note: This project’s goal is the hosting of very large tables — billions of rows X millions of columns— atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned,column-oriented store modeled after Google’s Bigtable: A Distributed Storage System forStructured Data by Chang et al. Just as Bigtable leverages the distributed data storage providedby the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoopand HDFS.FeaturesLinear and modular scalability.Strictly consistent reads and writes.Automatic and configurable sharding of tablesAutomatic failover support between RegionServers.Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.Easy to use Java API for client access.Block cache and Bloom Filters for real-time queries.Query predicate push down via server side FiltersThrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary dataencoding optionsExtensible jruby-based (JIRB) shellSupport for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMXReference: http://hbase.apache.org/ (when would I use HBase? First sentence)

--------------------------------------------------------------------------------------------------
